{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"NOMAD Documentation","text":"<p>NOMAD is a free, and open-source data management platform for materials science, whose goal is to make scientific research data FAIR (findable, accessible, interoperable and reusable).</p> <p>NOMAD provides tools for data management, sharing, and publishing. The platform lets you structure, explore, and analyze your data and the data of others.</p> <p>NOMAD solves the challenge of using heterogeneous and unfindable data.</p> <p>NOMAD is useful for scientists that work with data, for research groups that need to collaborate on data, and for communities that need to build an archive of FAIR research data.</p> Project and community <p>NOMAD is an open source project that warmly welcomes community projects, contributions, suggestions, fixes and constructive feedback. NOMAD is developed by FAIRmat, an open NFDI consortium of over 30 partners building a shared data structure of for materials science together.</p> <ul> <li>Get support</li> <li>Join our online forum</li> <li>Contribute</li> <li>View our roadmap</li> <li>Code guidelines</li> </ul> <p>Thinking about using NOMAD for your next project? Get in touch!</p>"},{"location":"index.html#tutorial","title":"Tutorial","text":"<p>A series of tutorials will guide you through the main functionality of NOMAD.</p> <ul> <li>Upload and publish your own data</li> <li>Use the search interface to identify interesting data</li> <li>Use the API to search and access processed data for analysis</li> <li>Create and use custom schemas in NOMAD</li> <li> <p>Developing a NOMAD plugin</p> </li> <li> <p>Example data and exercises</p> </li> <li>More videos and tutorials on YouTube</li> </ul>"},{"location":"index.html#how-to-guides","title":"How-to guides","text":"<p>How-to guides provide step-by-step instructions for a wide range of tasks, with the overarching topics:</p> <ul> <li>Manage and find data</li> <li>Programmatic data access</li> <li>Oasis</li> <li>Customization</li> <li>Development</li> </ul> <p>Open the how-to guides</p>"},{"location":"index.html#explanation","title":"Explanation","text":"<p>The explanation section provides background knowledge on what are schemas and structured data, how does processing work, the NOMAD architecture, and more.</p>"},{"location":"index.html#reference","title":"Reference","text":"<p>The reference includes all CLI commands and arguments, all configuration options, the possible schema annotations and their arguments, and a glossary of used terms.</p>"},{"location":"aitoolkit.html","title":"Aitoolkit","text":"<p>go here ..</p>"},{"location":"todo.html","title":"Coming soon ...","text":"<p>We still have to write this.</p>"},{"location":"writing_guide.html","title":"Writing Guide","text":"<p>This is a guide for best practices when contributing to the NOMAD documentation.</p>"},{"location":"writing_guide.html#images-and-data","title":"Images and Data","text":"<p>All assets specific to an individual markdown file should be stored within an immediate sub-directory of the file, labeled accordingly. Please use <code>images/</code> and <code>data/</code> for the image and data files, respectively.</p>"},{"location":"writing_guide.html#sections-hierarchy","title":"Sections Hierarchy","text":"<p>single \"#\" sections should only be used at the beginning of the md file</p>"},{"location":"writing_guide.html#external-links","title":"External Links","text":"<p>Use  for external links to open a new browser window.</p>"},{"location":"writing_guide.html#admonitions","title":"Admonitions","text":"<p>Here is a list of currently used admonitions within the docs:</p> <ul> <li> <p>Attention</p> </li> <li> <p>Note</p> </li> <li> <p>Tip</p> </li> <li> <p>Important</p> </li> </ul>"},{"location":"examples/overview.html","title":"NOMAD Domain-specific Examples","text":"<p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> One last thing <p>If you can't find what you're looking for in our guides, contact our friendly team for personalized help and assistance. Don't worry, we're here to help and learn what we're doing wrong!</p>"},{"location":"examples/overview.html#computational-data","title":"Computational data","text":"<p>Historically a repository for Density Functional Theory calculations, NOMAD now supports a wide range of computational methodologies including advanced many-body calculations and classical molecular dynamics simulations, as well as complex simulation workflows.</p> <ul> <li>Quick Start: Uploading computational data</li> <li>Standard and custom computational workflows</li> <li>Guide to computational MetaInfo</li> <li>Guide to computational schema plugins</li> <li>Guide to computational parser plugins</li> <li>H5MD: Uploading custom molecular dynamics data<ul> <li>How-to</li> <li>Explanation</li> <li>Reference</li> </ul> </li> </ul>"},{"location":"examples/overview.html#experimental-data","title":"Experimental data","text":"<p>Thanks to key activities of the FAIRmat project, NOMAD also supports a set of parsing capabilities for standardizing data from materials characterization experiments.</p> <ul> <li>NeXus</li> <li>Guide to the pynxtools parser library</li> <li>Electron microscopy</li> <li>Photoemission spectroscopy</li> <li>X-ray photoemission spectroscopy</li> <li>Optical spectroscopy</li> <li>Atom probe tomography</li> <li>Scanning tunneling spectroscopy</li> </ul>"},{"location":"examples/computational_data/h5md_expl.html","title":"H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD","text":""},{"location":"examples/computational_data/h5md_expl.html#overview","title":"Overview","text":"<p>Most computational data in NOMAD is harvested with code-specific parsers that recognize the output files from a particular software and retrieve the appropriate (meta)data accordingly. However, this approach is not possible for many modern molecular simulation engines that use fully-flexible scriptable input and non-fixed output files. \"HDF5 for molecular data\" (H5MD) is a data schema for storage of molecular simulation data, based on the HDF5 file format. This page describes an extension of the H5MD schema, denoted H5MD-NOMAD, which adds specificity to several of the H5MD guidelines while also retaining reasonable flexibility. This enables simulation data stored according to the H5MD-NOMAD schema to be stored in the NOMAD.</p> <p>Due to the nature of extending upon the original H5MD schema, portions of this doc page was duplicated, extended, or summarized from the H5MD webpage.</p>"},{"location":"examples/computational_data/h5md_expl.html#introduction-to-the-h5md-storage-format","title":"Introduction to the H5MD storage format","text":"<p>H5MD was originally proposed by P. de Buyl, P. H. Colberg and F. H\u00f6fling in H5MD: A structured, efficient, and portable file format for molecular data, Comp. Phys. Comm. 185, 1546\u20131553 (2014) [arXiv:1308.6382]. The schema is maintained, along with associated tools, in a GitHub repository: H5MD GitHub.</p> <p>This section provides the basic nomenclature of the H5MD schema relevant for understanding H5MD-NOMAD, and was duplicated or summarized from the H5MD webpage.</p>"},{"location":"examples/computational_data/h5md_expl.html#file-format","title":"File format","text":"<p>H5MD structures are stored in the HDF5 file format version 0 or later. It is recommended to use the HDF5 file format version 2, which includes the implicit tracking of the creation and modification times of the file and of each of its objects.</p>"},{"location":"examples/computational_data/h5md_expl.html#notation-and-naming","title":"Notation and naming","text":"<p>HDF5 files are organized into groups and datasets, summarized as objects, which form a tree structure with the datasets as leaves. Attributes can be attached to each object. The H5MD specification adopts this naming and uses the following notation to depict the tree or its subtrees:</p> <code>\\-- item</code> An object within a group, that is either a dataset or a group. If it is a group itself, the objects within the group are indented by five spaces with respect to the group name. <code>+-- attribute</code> An attribute, that relates either to a group or a dataset. <code>\\-- data: &lt;type&gt;[dim1][dim2]</code> A dataset with array dimensions <code>dim1</code> by <code>dim2</code> and of type <code>&lt;type&gt;</code>. The type is taken from <code>Enumeration</code>, <code>Integer</code>, <code>Float</code> or <code>String</code> and follows the HDF5 Datatype classes. If the type is not mandated by H5MD, <code>&lt;type&gt;</code> is indicated. A scalar dataspace is indicated by <code>[]</code>. <code>(identifier)</code> An optional item. <code>&lt;identifier&gt;</code> An optional item with unspecified name. <p>H5MD defines a structure called H5MD element (or element whenever there is no confusion). An element is either a time-dependent group or a single dataset (see time-dependent data below), depending on the situation.</p>"},{"location":"examples/computational_data/h5md_expl.html#time-dependent-data","title":"Time-dependent data","text":"<p>Time-dependent data consists of a series of samples (or frames) referring to multiple time steps. Such data are found inside a single dataset and are accessed via dataset slicing. In order to link the samples to the time axis of the simulation, H5MD defines a time-dependent H5MD element as a group that contains, in addition to the actual data, information on the corresponding integer time step and on the physical time. The structure of such a group is:</p> <pre><code>&lt;element&gt;\n \\-- step\n \\-- (time)\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <code>value</code> A dataset that holds the data of the time series. It uses a simple dataspace whose rank is given by 1 plus the tensor rank of the data stored. Its shape is the shape of a single data item prepended by a <code>[variable]</code> dimension that allows the accumulation of samples during the course of time. For instance, the data shape of scalars has the form <code>[variable]</code>, <code>D</code>-dimensional vectors use <code>[variable][D]</code>, etc. The first dimension of <code>value</code> must match the unique dimension of <code>step</code> and <code>time</code>. <p>If several H5MD elements are sampled at equal times, <code>step</code> and <code>time</code> of one element may be hard links to the <code>step</code> and <code>time</code> datasets of a different element. If two elements are sampled at different times (for instance, if one needs the positions more frequently than the velocities), <code>step</code> and <code>time</code> are unique to each of them.</p> <p>The storage of step and time information follows one of the two modes below, depending on the dataset layout of <code>step</code>.</p>"},{"location":"examples/computational_data/h5md_expl.html#explicit-step-and-time-storage","title":"Explicit step and time storage","text":"<pre><code>&lt;element&gt;\n \\-- step: Integer[variable]\n \\-- (time: type[variable])\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <code>step</code> A dataset with dimensions <code>[variable]</code> that contains the time steps at which the corresponding data were sampled. It is of <code>Integer</code> type to allow exact temporal matching of data from one H5MD element to another. The values of the dataset are in monotonically increasing order. <code>time</code> An optional dataset that is the same as the <code>step</code> dataset, except it is <code>Float</code> or <code>Integer</code>-valued and contains the simulation time in physical units. The values of the dataset are in monotonically increasing order."},{"location":"examples/computational_data/h5md_expl.html#fixed-step-and-time-storage-currently-not-supported-in-h5md-nomad","title":"Fixed step and time storage (currently not supported in H5MD-NOMAD)","text":"<pre><code>&lt;element&gt;\n \\-- step: Integer[]\n     +-- (offset: type[])\n \\-- (time: type[])\n     +-- (offset: type[])\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <code>step</code> <p>A scalar dataset of <code>Integer</code> type that contains the increment of the time step between two successive rows of data in <code>value</code>.</p> <code>offset</code> A scalar attribute of type <code>Integer</code> corresponding to the first sampled value of <code>step</code>. <code>time</code> An optional scalar dataset that is the same as the <code>step</code> dataset, except that it is <code>Float</code> or <code>Integer</code>-valued and contains the increment in simulation time, in physical units. <p><code>offset</code>     : A scalar attribute of the same type as <code>time</code> corresponding to the first     sampled value of <code>time</code>.</p> <p>For this storage mode, the explicit value \\(s(i)\\) of the step corresponding to the \\(i\\)-th row of the dataset <code>value</code> is \\(s(i) = i\\times\\mathrm{step} + \\mathrm{offset}\\) where \\(\\mathrm{offset}\\) is set to zero if absent. The corresponding formula for the time \\(t(i)\\) is identical: \\(t(i) = i\\times\\mathrm{time} + \\mathrm{offset}\\). The index \\(i\\) is zero-based.</p>"},{"location":"examples/computational_data/h5md_expl.html#time-independent-data","title":"Time-independent data","text":"<p>H5MD defines a time-independent H5MD element as a dataset. As for the <code>value</code> dataset in the case of time-dependent data, data type and array shape are implied by the stored data, where the <code>[variable]</code> dimension is omitted.</p>"},{"location":"examples/computational_data/h5md_expl.html#storage-order-of-arrays","title":"Storage order of arrays","text":"<p>All arrays are stored in C-order as enforced by the HDF5 file format. A C or C++ program may thus declare <code>r[N][D]</code> for the array of particle coordinates while the Fortran program will declare a <code>r(D,N)</code> array (appropriate index ordering for a system of <code>N</code> particles in <code>D</code> spatial dimensions), and the HDF5 file will be the same.</p>"},{"location":"examples/computational_data/h5md_expl.html#storage-of-particles-and-tuples-lists","title":"Storage of particles and tuples lists","text":""},{"location":"examples/computational_data/h5md_expl.html#storage-of-a-list-of-particles","title":"Storage of a list of particles","text":"<p>A list of particles is an H5MD element:</p> <pre><code>&lt;list_name&gt;: Integer[N]\n +-- particles_group: Object reference\n</code></pre> <p>where <code>list_name</code> is a dataset of <code>Integer</code> type and dimensions <code>[N]</code>, N being the number of particle indices stored in the list. <code>particles_group</code> is an attribute containing an HDF5 Object Reference as defined by the HDF5 file format. <code>particles_group</code> must refer to one of the groups in <code>/particles</code>.</p> <p>If a fill value is defined for <code>list_name</code>, the particles indices in <code>list_name</code> set to this value are ignored.</p> <p>If the corresponding <code>particles_group</code> does not possess the <code>id</code> element, the values in <code>list_name</code> correspond to the indexing of the elements in <code>particles_group</code>. Else, the values in <code>list_name</code> must be put in correspondence with the equal values in the <code>id</code> element.</p>"},{"location":"examples/computational_data/h5md_expl.html#storage-of-tuples","title":"Storage of tuples","text":"<p>A list of tuples is an H5MD element:</p> <pre><code>&lt;tuples_list_name&gt;: Integer[N,T]\n +-- particles_group: Object reference\n</code></pre> <p>where <code>N</code> is the length of the list and <code>T</code> is the size of the tuples.  Both <code>N</code> and <code>T</code> may indicate variable dimensions. <code>particles_group</code> is an attribute containing an HDF5 Object Reference, obeying the same rules as for the lists of particles.</p> <p>The interpretation of the values stored within the tuples is done as for a list of particles.</p> <p>If a fill value is defined, tuples with at least one entry set to this value are ignored.</p>"},{"location":"examples/computational_data/h5md_expl.html#time-dependence-time-dependent-particle-lists-currently-not-supported-in-h5md-nomad","title":"Time-dependence (time-dependent particle lists currently not supported in H5MD-NOMAD)","text":"<p>As the lists of particles and tuples above are H5MD elements, they can be stored either as time-dependent groups or time-independent datasets.</p> <p>As an example, a time-dependent list of pairs is stored as:</p> <pre><code>&lt;pair_list_name&gt;\n   +-- particles_group: Object reference\n   \\-- value: Integer[variable,N,2]\n   \\-- step: Integer[variable]\n</code></pre> <p>The dimension denoted by <code>N</code> may be variable.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-root-level","title":"The root level","text":"<p>The root level of H5MD-NOMAD structure is organized as follows (identical to the original H5MD specification):</p> <pre><code>H5MD-NOMAD root\n \\-- h5md\n \\-- (particles)\n \\-- (observables)\n \\-- (connectivity)\n \\-- (parameters)\n</code></pre> <code>h5md</code> A group that contains metadata and information on the H5MD structure itself. It is the only mandatory group at the root level of H5MD. <code>particles</code> An optional group that contains information on each particle in the system, e.g., a snapshot of the positions or the full trajectory in phase space. <code>observables</code> An optional group that contains other quantities of interest, e.g., physical observables that are derived from the system state at given points in time. <code>connectivity</code> An optional group that contains information about the connectivity between particles. <code>parameters</code> An optional group that contains application-specific (meta)data such as control parameters or simulation scripts."},{"location":"examples/computational_data/h5md_expl.html#the-h5md-group","title":"The H5MD Group","text":"<p>A set of global metadata describing the H5MD structure is stored in the <code>h5md</code> group as attributes. The contents of the group are:</p> <pre><code>h5md\n +-- version: Integer[2]\n \\-- author\n |    +-- name: String[]\n |    +-- (email: String[])\n \\-- creator\n |    +-- name: String[]\n |    +-- version: String[]\n \\-- program\n      +-- name: String[]\n      +-- version: String[]\n</code></pre> <code>version</code> An attribute, of <code>Integer</code> datatype and of simple dataspace of rank 1 and size 2, that contains the major version number and the minor version number of the H5MD specification the H5MD structure conforms to. <p>The version x.y.z of the H5MD specification follows semantic versioning: A change of the major version number x indicates backward-incompatible changes to the file structure. A change of the minor version number y indicates backwards-compatible changes to the file structure. A change of the patch version number z indicates changes that have no effect on the file structure and serves to allow for clarifications or minor text editing of the specification.</p> <p>As the z component has no impact on the content of an H5MD file, the <code>version</code> attribute contains only x and y.</p> <code>author</code> A group that contains metadata on the person responsible for the simulation (or the experiment) as follows: <ul> <li> <code>name</code> An attribute, of fixed-length string datatype and of scalar dataspace, that holds the author's real name. </li> <li> <code>email</code> An optional attribute, of fixed-length string datatype and of scalar dataspace, that holds the author's email address of the form <code>email@domain.tld</code>. </li> </ul> <code>creator</code> A group that contains metadata on the program that created the H5MD structure as follows: <ul> <li> <code>name</code> An attribute, of fixed-length string datatype and of scalar dataspace, that stores the name of the program. </li> <li> <code>version</code> An attribute, of fixed-length string datatype and of scalar dataspace, that yields the version of the program. </li> </ul> <code>program</code> A group that contains metadata on the code/package that created the simulation data contained within this H5MD structure: <ul> <li> <code>name</code> An attribute, of fixed-length string datatype and of scalar dataspace, that stores the name of the program. </li> <li> <code>version</code> An attribute, of fixed-length string datatype and of scalar dataspace, that yields the version of the program. </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html#modules-currently-unused-in-h5md-nomad","title":"Modules (currently unused in H5MD-NOMAD)","text":"<p>The original H5MD specification allowed the definition of modules under the h5md group. Such modules are currently ignored when uploading to NOMAD, although they of course will remain present in the raw uploaded hdf5 file.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-particles-group","title":"The particles group","text":"<p>Particle attributes, i.e., information about each particle in the system, are stored within the <code>particles</code> group. According to the original H5MD schema, the <code>particles</code> group is a container for subgroups that represent different subsets of the system under consideration. For simplicity of parsing, H5MD-NOMAD currently requires one such group, labeled <code>all</code>, to contain all the particles and corresponding attributes to be stored in the NOMAD archive. Additional particle groups will be ignored.</p> <p>For each dataset, the ordering of indices (whenever relevant) is as follows: frame index, particle index, dimension index. Thus, the contents of the <code>particles</code> group for a trajectory with <code>N_frames</code> frames and <code>N_part</code> particles in a <code>D</code>-dimensional space can be represented:</p> <pre><code>particles\n \\-- all\n |    \\-- box\n |    \\-- (&lt;time-dependent_vector_attribute&gt;)\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][N_part][D]\n |    \\-- (&lt;time-dependent_scalar_attribute&gt;)\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][N_part]\n |    \\-- (&lt;time-independent_vector_attribute&gt;): &lt;type&gt;[N_part][D]\n |    \\-- (&lt;time-independent_scalar_attribute&gt;): &lt;type&gt;[N_part]\n |    \\-- ...\n \\-- &lt;group2&gt;\n      \\-- ...\n</code></pre>"},{"location":"examples/computational_data/h5md_expl.html#standardized-h5md-elements-for-particles-group","title":"Standardized H5MD elements for particles group","text":"<code>position</code> (required for parsing other particle attributes) An element that describes the particle positions as coordinate vectors of <code>Float</code> or <code>Integer</code> type. <code>velocity</code> An element that contains the velocities for each particle as a vector of <code>Float</code> or <code>Integer</code> type. <code>force</code> An element that contains the total forces (i.e., the accelerations multiplied by the particle mass) for each particle as a vector of <code>Float</code> or <code>Integer</code> type. <code>mass</code> An element that holds the mass for each particle as a scalar of <code>Float</code> type. <ul> <li> <code>image</code> (currently unused in H5MD-NOMAD) </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html#todo-can-we-make-these-admonitions-indented-somehow-or-more-obviously-connected-with-the-members-of-this-list","title":"TODO can we make these admonitions indented somehow or more obviously connected with the members of this list?","text":"Details <p>An element that represents periodic images of the box as coordinate vectors of <code>Float</code> or <code>Integer</code> type and allows one to compute for each particle its absolute position in space. If <code>image</code> is present, <code>position</code> must be present as well. For time-dependent data, the <code>step</code> and <code>time</code> datasets of <code>image</code> must equal those of <code>position</code>, which must be accomplished by hard-linking the respective datasets.</p> <code>species</code> (currently unused in H5MD-NOMAD) Details <p>An element that describes the species for each particle, i.e., its atomic or chemical identity, as a scalar of <code>Enumeration</code> or <code>Integer</code> data type. Particles of the same species are assumed to be identical with respect to their properties and unbonded interactions.</p> <code>id</code> (currently unused in H5MD-NOMAD) Details <p>An element that holds a scalar identifier for each particle of <code>Integer</code> type, which is unique within the given particle subgroup. The <code>id</code> serves to identify particles over the course of the simulation in the case when the order of the particles changes, or when new particles are inserted and removed. If <code>id</code> is absent, the identity of the particles is given by their index in the <code>value</code> datasets of the elements within the same subgroup.</p> <code>charge</code> An element that contains the charge associated to each particle as a scalar, of <code>Integer</code> or <code>Float</code> type."},{"location":"examples/computational_data/h5md_expl.html#standardized-h5md-nomad-elements-for-particles-group","title":"Standardized H5MD-NOMAD elements for particles group","text":"<code>species_label</code>  An element that holds a label (fixed-length string datatype) for each particle. This label denotes the fundamental species type of the particle (e.g., the chemical element label for atoms), regardless of its given interactions within the model. Both time-independent and time-dependent <code>species_label</code> elements are supported. <code>model_label</code> An element that holds a label (fixed-length string datatype) for each particle. This label denotes the type of particle with respect to the given interactions within the model (e.g., force field) Currently only time-independent species labels are supported."},{"location":"examples/computational_data/h5md_expl.html#non-standard-elements-in-particles-group","title":"Non-standard elements in particles group","text":"<p>All non-standard elements within the particles group are currently ignored by the NOMAD H5MD parser. In principle, one can store additional custom attributes as configuration-specific observables (see The observables group).</p>"},{"location":"examples/computational_data/h5md_expl.html#the-simulation-box-subgroup","title":"The simulation box subgroup","text":"<p>Information about the simulation box is stored in a subgroup named <code>box</code>, within the relevant particles group (<code>all</code> in our case). Both time-independent and time-dependent box information are supported (i.e. via the <code>edges</code> element). Because the <code>box</code> group is specific to a particle group of particles, time-dependent boxes must contain <code>step</code> and <code>time</code> datasets that exactly match those of the corresponding <code>position</code> group. In principal, this should be accomplished by hard-linking the respective datasets. In practice, H5MD-NOMAD currently assumes that this is the case (i.e., the box group <code>step</code> and <code>time</code> information is unused), and simply checks that <code>edges.value</code> has the same leading dimension as <code>position</code>.</p> <p>The structure of the <code>box</code> group is as follows:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- (edges)\n</code></pre> <code>dimension</code> An attribute that stores the spatial dimension <code>D</code> of the simulation box and is of <code>Integer</code> datatype and scalar dataspace. <code>boundary</code>  An attribute, of boolean datatype (changed from string to boolean in H5MD-NOMAD) and of simple dataspace of rank 1 and size <code>D</code>, that specifies the boundary condition of the box along each dimension, i.e., <code>True</code> implies periodic boundaries are applied in the corresponding dimension. If all values in <code>boundary</code> are <code>False</code>, <code>edges</code> may be omitted. <code>edges</code> A <code>D</code>-dimensional vector or a <code>D</code> \u00d7 <code>D</code> matrix, depending on the geometry of the box, of <code>Float</code> or <code>Integer</code> type. Only cuboid and triclinic boxes are allowed. If <code>edges</code> is a vector, it specifies the space diagonal of a cuboid-shaped box. If <code>edges</code> is a matrix, the box is of triclinic shape with the edge vectors given by the rows of the matrix. For a time-dependent box, a cuboid geometry is encoded by a dataset <code>value</code> (within the H5MD element) of rank 2 (1 dimension for the time and 1 for the vector) and a triclinic geometry by a dataset <code>value</code> of rank 3 (1 dimension for the time and 2 for the matrix). For a time-independent box, a cuboid geometry is encoded by a dataset <code>edges</code> of rank 1 and a triclinic geometry by a dataset of rank 2. <p>For instance, a cuboid box that changes in time would appear as:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges\n                \\-- step: Integer[variable]\n                \\-- time: Float[variable]\n                \\-- value: &lt;type&gt;[variable][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>. A triclinic box that is fixed in time would appear as:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges: &lt;type&gt;[D][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-observables-group","title":"The observables group","text":"<p>The initial H5MD proposed a simple and flexible schema for the general storage of observable info, defined roughly as \"macroscopic observables\" or \"averages of a property over many particles\", as H5MD elements:</p> <pre><code>observables\n \\-- &lt;observable1&gt;\n |    \\-- step: Integer[N_frames]\n |    \\-- time: Float[N_frames]\n |    \\-- value: &lt;type&gt;[N_frames]\n \\-- &lt;observable2&gt;\n |    \\-- step: Integer[N_frames]\n |    \\-- time: Float[N_frames]\n |    \\-- value: &lt;type&gt;[N_frames][D]\n \\-- &lt;group1&gt;\n |    \\-- &lt;observable3&gt;\n |         \\-- step: Integer[N_frames]\n |         \\-- time: Float[N_frames]\n |         \\-- value: &lt;type&gt;[N_frames][D][D]\n \\-- &lt;observable4&gt;: &lt;type&gt;[]\n \\-- ...\n</code></pre> <p></p> <p>As depicted above, observables representing only a subset of the particles may be stored in appropriate subgroups similar to the <code>particles</code> tree. H5MD-NOMAD does support the organization of observables into subgroups (as discussed in more detail below). However, grouping by particle groups is not fully supported in the sense that there is currently no metadata storing the corresponding indices of the relevant particles subgroup. Additionally, since only the <code>all</code> particles group is parsed, information about the named subgroup will not be stored anywhere in the archive. Thus, we recommend for now that only observables relevant to the <code>all</code> particles subgroup are stored within this section.</p>"},{"location":"examples/computational_data/h5md_expl.html#h5md-nomad-observables","title":"H5MD-NOMAD observables","text":"<p>H5MD-NOMAD extends H5MD observable storage by 1. specifying standard observable types with associated metadata and 2. providing standardized specifications for some common observables. In contrast to the schema above, a more restrictive structure is required:</p> <pre><code>observables\n \\-- &lt;observable_type_1&gt;\n |    \\-- &lt;observable_1_label_1&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n \\-- &lt;observable_type_2&gt;\n |    \\-- &lt;observable_2_label_1&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n |    \\-- &lt;observable_2_label_2&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n |    \\-- ...\n \\-- ...\n</code></pre> <p>Here, each <code>observable_type</code> corresponds to a particular group of observables, e.g., to be plotted together in a single plot. The given name for this group could be generic, e.g., <code>radial distribution function</code>, or more specific, e.g., <code>molecular radial distribution function for solvents</code>. The latter may be useful in case multiple groupings of a single type of observable are needed. Each <code>observable_label</code> then corresponds to a specific name for an individual instance of this observable type. For example, for a radial distribution function between particles of type <code>A</code> and <code>B</code>, <code>observable_label</code> might be set to <code>A-B</code>.</p> <p>Finally, H5MD-NOMAD has added the observable <code>type</code> as an attribute of each observable: The following observable types are supported:</p> <p></p> <code>configurational</code> <p>An observable that is computed for each individual configuration, with the following general structure:</p> <p>observables  --   |    --   |    |    +-- type: \"configurational\"  |    |    -- step: Integer[N_frames]  |    |    -- time: Float[N_frames]  |    |    -- value: [N_frames][M]  |    -- ...  -- ...  where <code>M</code> is the dimension of the observable. This section may also be used to store per-particle quantities/attributes that are not currently supported as standardized H5MD-NOMAD elements for particles group, in which case <code>value</code> will have dimensions <code>[N_frames][N_part][M]</code>. <p></p> <code>ensemble_average</code> <p>An observable that is computed by averaging over multiple configurations, with the following generic structure:</p> <p>observables  --   |    --   |    |    +-- type: \"ensemble_average\"  |    |    -- (n_variables): Integer  |    |    -- (variables_name): String[n_variables][]  |    |    -- (n_bins): Integer[]  |    |    -- bins: Float[n_bins][]  |    |    -- value: [n_bins][]  |    |    -- (frame_start): Integer  |    |    -- (frame_end): Integer  |    |    -- (n_smooth): Integer  |    |    -- (type): String[]  |    |    -- (error_type): String[]  |    |    -- (errors): Float[n_bins]  |    |    -- (error_labels): String[]  |    |    -- (frame_end): Integer  |    |    -- (): []  |    -- ...  -- ... <ul> <li> <code>n_variables</code> dimensionality of the observable. Can also be inferred from leading dimension of <code>bins</code>. </li> <li> <code>variables_name</code> name/description of the independent variables along which the observable is defined. </li> <li> <code>n_bins</code> number of bins along each dimension of the observable. Either single Integer for 1-D observables, or a list of Integers for multi-dimensional observable. Can also be inferred from dimensions of <code>bins</code>. </li> <li> <code>bins</code> value of the bins used for calculating the observable along each dimension of the observable. </li> <li> <code>value</code> value of the calculated ensemble average at each bin. </li> <li> <code>frame_start</code> trajectory frame index at which the averaging begins. This index must correspond to the list of steps and times in <code>particles.all.position</code>. </li> <li> <code>frame_end</code> trajectory frame index at which the averaging ends. This index must correspond to the list of steps and times in <code>particles.all.position</code>. </li> <li> <code>n_smooth</code> number of bins over which the running average was computed for <code>value</code>. </li> <li> <code>type</code> Allowed values of <code>molecular</code> or <code>atomic</code>. Categorizes if the observable is calculated at the molecular or atomic level. </li> </ul> <ul> <li> <code>error_type</code> describes the type of error reported for this observable. Examples: <code>Pearson correlation coefficient</code>, <code>mean squared error</code>. </li> <li> <code>errors</code> value of the error at each bin. Can be multidimensional with corresponding label stored in <code>error_labels</code>. </li> <li> <code>error_labels</code> describes the error along individual dimensions for multi-D errors. </li> <li> <code>&lt;custom_dataset&gt;</code> additional metadata may be given as necessary. </li> </ul> <p></p> <code>time_correlation</code> <p>An observable that is computed by calculating correlations between configurations in time, with the following general structure:</p> <p>observables  --   |    --   |    |    +-- type: \"time_correlation\"  |    |    -- (direction): String[]  |    |    -- (n_times): Integer[]  |    |    -- times: Float[n_times][]  |    |    -- value: [n_bins][]  |    |    -- (type): String[]  |    |    -- (error_type): String[]  |    |    -- (errors): Float[n_bins]  |    |    -- (error_labels): String[]  |    |    -- (): []  |    -- ...  -- ... <ul> <li> <code>label</code> describes the particles involved in determining the property. For example, for a radial distribution function between particles of type <code>A</code> and <code>B</code>, <code>label</code> might be set to <code>A-B</code> </li> <li> <code>direction</code> allowed values of <code>x</code>, <code>y</code>, <code>z</code>, <code>xy</code>, <code>yz</code>, <code>xz</code>, <code>xyz</code>. The direction/s used for calculating the correlation function. </li> <li> <code>n_times</code> number of times windows for the calculation of the correlation function. Can also be inferred from dimensions of <code>times</code>. </li> <li> <code>times</code> time values used for calculating the correlation function (i.e., \u0394t values). </li> <li> <code>value</code> value of the calculated correlation function at each time. </li> <li> <code>type</code> Allowed values of <code>molecular</code> or <code>atomic</code>. Categorizes if the observable is calculated at the molecular or atomic level. </li> </ul> <ul> <li> <code>error_type</code> describes the type of error reported for this observable. Examples: <code>Pearson correlation coefficient</code>, <code>mean squared error</code>. </li> <li> <code>errors</code> value of the error at each bin. Can be multidimensional with corresponding label stored in <code>error_labels</code>. </li> <li> <code>error_labels</code> describes the error along individual dimensions for multi-D errors. </li> <li> <code>&lt;custom_dataset&gt;</code> additional metadata may be given as necessary. </li> </ul> <p>A list of standardized observables can be found in Reference - H5MD-NOMAD &gt; Standardized observables in H5MD-NOMAD.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-connectivity-group","title":"The connectivity group","text":"<p>The initial H5MD proposed a simple and flexible schema for the storage of \"connectivity\" information, e.g., to be used in conjunction with a molecular mechanics force field. The connectivity information is stored as tuples in the group <code>/connectivity</code>. The tuples are pairs, triples, etc. as needed and may be either time-independent or time-dependent. As with other elements, connectivity elements can be defined for particular particle groups. However, H5MD-NOMAD focuses on the storage of connectivity elements for the entire system (i.e., the <code>all</code> particles group).</p>"},{"location":"examples/computational_data/h5md_expl.html#standardized-h5md-nomad-connectivity","title":"Standardized H5MD-NOMAD connectivity","text":"<p>The general structure of the <code>connectivity</code> group is as follows:</p> <pre><code>connectivity\n \\-- (bonds): Integer[N_part][2]\n \\-- (angles): Integer[N_part][3]\n \\-- (dihedrals): Integer[N_part][4]\n \\-- (impropers): Integer[N_part][4]\n \\-- (&lt;custom_interaction&gt;): Integer[N_part][m]\n \\-- (particles_group)\n      \\-- ...\n</code></pre> <p><code>N_part</code> corresponds to the number of particles stored in the <code>particles/all</code> group.</p> <ul> <li> <code>bonds</code> a list of 2-tuples specifying the indices of particles containing a \"bond interaction\". </li> <li> <code>angles</code> a list of 3-tuples specifying the indices of particles containing an \"angle interaction\". </li> <li> <code>dihedrals</code> a list of 4-tuples specifying the indices of particles containing a \"dihedral interaction\". </li> <li> <code>impropers</code> a list of 4-tuples specifying the indices of particles containing an \"improper dihedral interaction\". </li> <li> <code>&lt;custom_interaction&gt;</code> a list of m-tuples specifying the indices of particles containing an arbitrary interaction. <code>m</code> denotes the number of particles involved in the interaction. </li> <li> <code>particles_group</code> See below. </li> </ul> <p> Currently only time-independent connectivity elements are supported.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-particles_group-subgroup","title":"The particles_group subgroup","text":"<p>Despite not fully utilizing the organization of arbitrary groups of particles within the <code>particles</code> group, H5MD-NOMAD allows for the user to provide an arbitrary hierarchy of particle groupings, also referred to as a \"topology\", within the <code>connectivity</code> subgroup called <code>particles_group</code>. This information will be used by NOMAD to facilitate visualizations of the system, through the \"topology bar\" in the overview page. The general structure of the topology group is as follows:</p> <pre><code>connectivity\n \\-- particles_group\n      \\-- &lt;group_1&gt;\n      |    \\-- (type): String[]\n      |    \\-- (formula): String[]\n      |    \\-- indices: Integer[]\n      |    \\-- (is_molecule): Bool\n |    |    \\-- (&lt;custom_dataset&gt;): &lt;type&gt;[]\n      |    \\-- (particles_group):\n      |        \\-- ...\n      \\-- &lt;group_2&gt;\n          \\-- ...\n</code></pre> <p>The initial <code>particles_group</code> subgroup, directly under <code>connectivity</code>, is a container for the entire topology. <code>particles_group</code> contains a series of subgroups with arbitrary names, which denote the first level of organization within the topology. The name of each subgroup will become the group label within the NOMAD metadata. Each of these subgroups then contain a series of datasets:</p> <ul> <li> <code>type</code> describes the type of particle group. There exists a list of standardized types: <code>molecule_group</code>, <code>molecule</code>, <code>monomer_group</code>, <code>monomer</code>. However, arbitrary types can be given. We suggest that you 1. use the standardized types when appropriate (note that protein residues should be generically typed as <code>monomer</code>) and 2. use the general format <code>&lt;type&gt;_group</code> for groups of a distinct type (see further description of suggested hierarchy below). </li> <li> <code>formula</code> a \"chemical-like\" formula that describes the particle group with respect to its underlying components. The format for the formula is <code>&lt;child_1&gt;(n_child_1)&lt;child_2&gt;(n_child_2)...</code>, where <code>&lt;child_x&gt;</code> is the name/label of the underlying component, and <code>n_child_x</code> is the number of such components found within this particle group. Example: A particles group containing 100 water molecules named <code>water</code> has the formula <code>water(100)</code>, whereas each underlying water molecule has the standard chemical formula <code>H2O</code>. </li> <li> <code>indices</code> a list of integer indices corresponding to all particles belonging to this group. Indices should correspond to the list of particles stored in the <code>particles/all</code> group. </li> <li> <code>is_molecule</code> indicator of individual molecules (typically with respect to the bond connections defined by a force field). </li> <li> <code>custom_dataset</code> arbitrary additional metadata for this particle group may be given. </li> </ul> <p>Each subgroup may also contain a (nested) <code>particles_group</code> subgroup, in order to subdivide the group of particles into an organizational hierarchy. As with the overall <code>particles_group</code> container, the groups contained within <code>particles_group</code> must not partition the particles within this group (i.e., overlapping or non-complete groupings are allowed). However, particle groups must contain particles already contained within the parent <code>particles_group</code> (i.e., subgroups must be a subset of the grouping at the previous level of the hierarchy).</p> <p>Note that typically the <code>particles_group</code> hierarchy ends at the level of individual particles (i.e., individual particles are not stored, since this information is already contained within the <code>particles</code> group).</p>"},{"location":"examples/computational_data/h5md_expl.html#the-parameters-group","title":"The parameters group","text":"<p>The initial H5MD proposed a simple and flexible schema for the storage of general \"parameter\" information within the <code>parameters</code> group, with the following structure:</p> <pre><code>parameters\n +-- &lt;user_attribute1&gt;\n \\-- &lt;user_data1&gt;\n \\-- &lt;user_group1&gt;\n |    \\-- &lt;user_data2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre> <p>In contrast, the H5MD-NOMAD schema calls for very specific structures to be used when storing parameter information. While the previous groups have attempted to stay away from enforcing NOMAD-specific data structures on the user, instead opting for more intuitive and generally-convenient structures, the <code>parameters</code> group utilizes already-existing metadata and structures within NOMAD to efficiently import simulation parameters in a way that is searchable and comparable to simulations performed by other users.</p> <p>In this way, the H5MD-NOMAD <code>parameters</code> group has the following structure:</p> <pre><code>parameters\n \\-- &lt;parameter_subgroup_1&gt;\n |    \\-- ...\n \\-- &lt;parameter_subgroup_2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre> <p>The subgroups <code>force_calculations</code> and <code>workflow</code> are supported. The following describes the detailed data structures for these subgroups, using the NOMAD MetaInfo definitions for each underlying <code>Quantity</code>. Please note that:</p> <ol> <li> <p>Quantities with <code>type=MEnum()</code> are restricted to the provided allowed values.</p> </li> <li> <p>The unit given in the MetaInfo definition does not have to be used within the H5MD-NOMAD file, however, the dimensionality of the unit should match.</p> </li> </ol>"},{"location":"examples/computational_data/h5md_expl.html#force-calculations","title":"Force calculations","text":"<p>The <code>force_calculations</code> group contains the parameters for force calculations according to the force field during a molecular dynamics run.</p> <p></p> <p>The following json template illustrates the structure of the <code>force_calculations</code> group, with example values for clarity:</p> <pre><code>{\n    \"vdw_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"},\n    \"coulomb_type\": \"particle_mesh_ewald\",\n    \"coulomb_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"},\n    \"neighbor_searching\": {\n        \"neighbor_update_frequency\": 1,\n        \"neighbor_update_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"}\n        }\n    }\n</code></pre> <p>In the following, we provide the NOMAD definitions for each of these quantities:</p> <ul> <li> <p><code>vdw_cutoff</code> :</p> <pre><code>Quantity(\n        type=np.float64,\n        shape=[],\n        unit='m',\n        description='''\n        Cutoff for calculating VDW forces.\n        ''')\n</code></pre> </li> <li> <p><code>coulomb_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('cutoff', 'ewald', 'multilevel_summation', 'particle_mesh_ewald',\n            'particle_particle_particle_mesh', 'reaction_field'),\n    shape=[],\n    description='''\n    Method used for calculating long-ranged Coulomb forces.\n\n    Allowed values are:\n\n    | Barostat Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"Cutoff\"`          | Simple cutoff scheme. |\n\n    | `\"Ewald\"` | Standard Ewald summation as described in any solid-state physics text. |\n\n    | `\"Multi-Level Summation\"` |  D. Hardy, J.E. Stone, and K. Schulten,\n    [Parallel. Comput. **35**, 164](https://doi.org/10.1016/j.parco.2008.12.005)|\n\n    | `\"Particle-Mesh-Ewald\"`        | T. Darden, D. York, and L. Pedersen,\n    [J. Chem. Phys. **98**, 10089 (1993)](https://doi.org/10.1063/1.464397) |\n\n    | `\"Particle-Particle Particle-Mesh\"` | See e.g. Hockney and Eastwood, Computer Simulation Using Particles,\n    Adam Hilger, NY (1989). |\n\n    | `\"Reaction-Field\"` | J.A. Barker and R.O. Watts,\n    [Mol. Phys. **26**, 789 (1973)](https://doi.org/10.1080/00268977300102101)|\n    ''')\n</code></pre> </li> <li> <p><code>coulomb_cutoff</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='m',\n    description='''\n    Cutoff for calculating short-ranged Coulomb forces.\n    ''')\n</code></pre> </li> <li> <p><code>neighbor_searching</code> : Section containing the parameters for neighbor searching/lists during a molecular dynamics run.</p> </li> <li> <p><code>neighbor_update_frequency</code> :</p> <pre><code>Quantity(\n    type=int,\n    shape=[],\n    description='''\n    Number of timesteps between updating the neighbor list.\n    ''')\n</code></pre> </li> <li> <p><code>neighbor_update_cutoff</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='m',\n    description='''\n    The distance cutoff for determining the neighbor list.\n    ''')\n</code></pre> </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html#the-molecular-dynamics-workflow","title":"The molecular dynamics workflow","text":"<p>The <code>workflow</code> group contains the parameters for any type of workflow. Here we describe the specific case of the well-defined <code>molecular_dynamics</code> workflow. Custom workflows are described in detail in Workflows in NOMAD.</p> <p></p> <p>The following json template illustrates the structure of the <code>molecular_dynamics</code> subsection of the <code>workflow</code> group, with example values for clarity:</p> <pre><code>{\n    \"molecular_dynamics\": {\n        \"thermodynamic_ensemble\": \"NPT\",\n        \"integrator_type\": \"langevin_leap_frog\",\n        \"integration_timestep\": {\"value\": 2e-15, \"unit\": \"ps\"},\n        \"n_steps\": 20000000,\n        \"coordinate_save_frequency\": 10000,\n        \"velocity_save_frequency\": null,\n        \"force_save_frequency\": null,\n        \"thermodynamics_save_frequency\": null,\n        \"thermostat_parameters\": {\n            \"thermostat_type\": \"langevin_leap_frog\",\n            \"reference_temperature\": {\"value\": 300.0, \"unit\": \"kelvin\"},\n            \"coupling_constant\": {\"value\": 1.0, \"unit\": \"ps\"}},\n        \"barostat_parameters\": {\n            \"barostat_type\": \"berendsen\",\n            \"coupling_type\": \"isotropic\",\n            \"reference_pressure\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]], \"unit\": \"bar\"},\n            \"coupling_constant\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]},\n            \"compressibility\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]}\n            }\n    }\n}\n</code></pre> <p>In the following, we provide the NOMAD definitions for each of these quantities:</p> <ul> <li> <p><code>thermodynamic_ensemble</code> :</p> <pre><code>Quantity(\n    type=MEnum('NVE', 'NVT', 'NPT', 'NPH'),\n    shape=[],\n    description='''\n    The type of thermodynamic ensemble that was simulated.\n\n    Allowed values are:\n\n    | Thermodynamic Ensemble          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"NVE\"`           | Constant number of particles, volume, and energy |\n\n    | `\"NVT\"`           | Constant number of particles, volume, and temperature |\n\n    | `\"NPT\"`           | Constant number of particles, pressure, and temperature |\n\n    | `\"NPH\"`           | Constant number of particles, pressure, and enthalpy |\n    ''')\n</code></pre> </li> <li> <p><code>integrator_type</code> :         Quantity(             type=MEnum(                 'brownian', 'conjugant_gradient', 'langevin_goga',                 'langevin_schneider', 'leap_frog', 'rRESPA_multitimescale', 'velocity_verlet'             ),             shape=[],             description='''             Name of the integrator.</p> <pre><code>    Allowed values are:\n\n    | Integrator Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"langevin_goga\"`           | N. Goga, A. J. Rzepiela, A. H. de Vries,\n    S. J. Marrink, and H. J. C. Berendsen, [J. Chem. Theory Comput. **8**, 3637 (2012)]\n    (https://doi.org/10.1021/ct3000876) |\n\n    | `\"langevin_schneider\"`           | T. Schneider and E. Stoll,\n    [Phys. Rev. B **17**, 1302](https://doi.org/10.1103/PhysRevB.17.1302) |\n\n    | `\"leap_frog\"`          | R.W. Hockney, S.P. Goel, and J. Eastwood,\n    [J. Comp. Phys. **14**, 148 (1974)](https://doi.org/10.1016/0021-9991(74)90010-2) |\n\n    | `\"velocity_verlet\"` | W.C. Swope, H.C. Andersen, P.H. Berens, and K.R. Wilson,\n    [J. Chem. Phys. **76**, 637 (1982)](https://doi.org/10.1063/1.442716) |\n\n    | `\"rRESPA_multitimescale\"` | M. Tuckerman, B. J. Berne, and G. J. Martyna\n    [J. Chem. Phys. **97**, 1990 (1992)](https://doi.org/10.1063/1.463137) |\n    ''')\n</code></pre> </li> <li> <p><code>integration_timestep</code> :         Quantity(             type=np.float64,             shape=[],             unit='s',             description='''             The timestep at which the numerical integration is performed.             ''')</p> </li> <li> <p><code>n_steps</code> :         Quantity(             type=int,             shape=[],             description='''             Number of timesteps performed.             ''')</p> </li> <li> <p><code>coordinate_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the coordinates.             ''')</p> </li> <li> <p><code>velocity_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the velocities.             ''')</p> </li> <li> <p><code>force_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the forces.             ''')</p> </li> <li> <p><code>thermodynamics_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the thermodynamic quantities.             ''')</p> </li> <li> <code>thermostat_parameters</code> Section containing the parameters pertaining to the thermostat for a molecular dynamics run. </li> <li> <p><code>thermostat_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('andersen', 'berendsen', 'brownian', 'langevin_goga', 'langevin_schneider', 'nose_hoover', 'velocity_rescaling',\n            'velocity_rescaling_langevin'),\n    shape=[],\n    description='''\n    The name of the thermostat used for temperature control. If skipped or an empty string is used, it\n    means no thermostat was applied.\n\n    Allowed values are:\n\n    | Thermostat Name        | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"andersen\"`           | H.C. Andersen, [J. Chem. Phys.\n    **72**, 2384 (1980)](https://doi.org/10.1063/1.439486) |\n\n    | `\"berendsen\"`          | H. J. C. Berendsen, J. P. M. Postma,\n    W. F. van Gunsteren, A. DiNola, and J. R. Haak, [J. Chem. Phys.\n    **81**, 3684 (1984)](https://doi.org/10.1063/1.448118) |\n\n    | `\"brownian\"`           | Brownian Dynamics |\n\n    | `\"langevin_goga\"`           | N. Goga, A. J. Rzepiela, A. H. de Vries,\n    S. J. Marrink, and H. J. C. Berendsen, [J. Chem. Theory Comput. **8**, 3637 (2012)]\n    (https://doi.org/10.1021/ct3000876) |\n\n    | `\"langevin_schneider\"`           | T. Schneider and E. Stoll,\n    [Phys. Rev. B **17**, 1302](https://doi.org/10.1103/PhysRevB.17.1302) |\n\n    | `\"nose_hoover\"`        | S. Nos\u00e9, [Mol. Phys. **52**, 255 (1984)]\n    (https://doi.org/10.1080/00268978400101201); W.G. Hoover, [Phys. Rev. A\n    **31**, 1695 (1985) |\n\n    | `\"velocity_rescaling\"` | G. Bussi, D. Donadio, and M. Parrinello,\n    [J. Chem. Phys. **126**, 014101 (2007)](https://doi.org/10.1063/1.2408420) |\n\n    | `\"velocity_rescaling_langevin\"` | G. Bussi and M. Parrinello,\n    [Phys. Rev. E **75**, 056707 (2007)](https://doi.org/10.1103/PhysRevE.75.056707) |\n    ''')\n</code></pre> </li> <li> <p><code>reference_temperature</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='kelvin',\n    description='''\n    The target temperature for the simulation.\n    ''')\n</code></pre> </li> <li> <p><code>coupling_constant</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='s',\n    description='''\n    The time constant for temperature coupling. Need to describe what this means for the various\n    thermostat options...\n    ''')\n</code></pre> </li> <li> <p><code>effective_mass</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='kilogram',\n    description='''\n    The effective or fictitious mass of the temperature resevoir.\n    ''')\n</code></pre> </li> <li> <code>barostat_parameters</code> Section containing the parameters pertaining to the barostat for a molecular dynamics run. </li> <li> <p><code>barostat_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('berendsen', 'martyna_tuckerman_tobias_klein', 'nose_hoover', 'parrinello_rahman', 'stochastic_cell_rescaling'),\n    shape=[],\n    description='''\n    The name of the barostat used for temperature control. If skipped or an empty string is used, it\n    means no barostat was applied.\n\n    Allowed values are:\n\n    | Barostat Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"berendsen\"`          | H. J. C. Berendsen, J. P. M. Postma,\n    W. F. van Gunsteren, A. DiNola, and J. R. Haak, [J. Chem. Phys.\n    **81**, 3684 (1984)](https://doi.org/10.1063/1.448118) |\n\n    | `\"martyna_tuckerman_tobias_klein\"` | G.J. Martyna, M.E. Tuckerman, D.J. Tobias, and M.L. Klein,\n    [Mol. Phys. **87**, 1117 (1996)](https://doi.org/10.1080/00268979600100761);\n    M.E. Tuckerman, J. Alejandre, R. L\u00f3pez-Rend\u00f3n, A.L. Jochim, and G.J. Martyna,\n    [J. Phys. A. **59**, 5629 (2006)](https://doi.org/10.1088/0305-4470/39/19/S18)|\n\n    | `\"nose_hoover\"`        | S. Nos\u00e9, [Mol. Phys. **52**, 255 (1984)]\n    (https://doi.org/10.1080/00268978400101201); W.G. Hoover, [Phys. Rev. A\n    **31**, 1695 (1985) |\n\n    | `\"parrinello_rahman\"`        | M. Parrinello and A. Rahman,\n    [J. Appl. Phys. **52**, 7182 (1981)](https://doi.org/10.1063/1.328693);\n    S. Nos\u00e9 and M.L. Klein, [Mol. Phys. **50**, 1055 (1983) |\n\n    | `\"stochastic_cell_rescaling\"` | M. Bernetti and G. Bussi,\n    [J. Chem. Phys. **153**, 114107 (2020)](https://doi.org/10.1063/1.2408420) |\n    ''')\n</code></pre> </li> <li> <p><code>coupling_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('isotropic', 'semi_isotropic', 'anisotropic'),\n    shape=[],\n    description='''\n    Describes the symmetry of pressure coupling. Specifics can be inferred from the `coupling constant`\n\n    | Type          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `isotropic`          | Identical coupling in all directions. |\n\n    | `semi_isotropic` | Identical coupling in 2 directions. |\n\n    | `anisotropic`        | General case. |\n    ''')\n</code></pre> </li> <li> <p><code>reference_pressure</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='pascal',\n    description='''\n    The target pressure for the simulation, stored in a 3x3 matrix, indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal.\n    ''')\n</code></pre> </li> <li> <p><code>coupling_constant</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='s',\n    description='''\n    The time constants for pressure coupling, stored in a 3x3 matrix, indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal. 0 values along the off-diagonal\n    indicate no-coupling between these directions.\n    ''')\n</code></pre> </li> <li> <p><code>compressibility</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='1 / pascal',\n    description='''\n    An estimate of the system's compressibility, used for box rescaling, stored in a 3x3 matrix indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal. If None, it may indicate that these values\n    are incorporated into the coupling_constant, or simply that the software used uses a fixed value that is not available in\n    the input/output files.\n    ''')\n</code></pre> </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html#units","title":"Units","text":"<p>In the original H5MD schema, units were given as string attributes of datasets, e.g., <code>60 m s-2</code>. H5MD-NOMAD amends the treatment of units in 2 ways:</p> <ol> <li> <p>If needed, the leading prefactor is stored as a separate attribute of <code>float</code> datatype called <code>unit_factor</code>.</p> </li> <li> <p>The string that describes the unit should be compatible with the <code>UnitRegistry</code> class of the <code>pint</code> python module.</p> </li> </ol> <p>Generic representation of unit storage in H5MD-NOMAD:</p> <pre><code>&lt;group&gt;\n    \\-- &lt;dataset&gt;\n        +-- (unit: String[])\n        +-- (unit_factor: Float)\n</code></pre>"},{"location":"examples/computational_data/h5md_howto.html","title":"How to work with the H5MD-NOMAD schema","text":"<p>Attention</p> <p>The H5MD-NOMAD functionalities are still in the beta testing phase. Please contact us with any issues or questions.</p>"},{"location":"examples/computational_data/h5md_howto.html#writing-an-hdf5-file-according-to-h5md-nomad-with-python","title":"Writing an HDF5 file according to H5MD-NOMAD with python","text":"<p>You can write to an HDF5 file via a python interface, using the h5py package. This section walks you through the creation of each section of the H5MD-NOMAD schema, using practical examples to help you get started.</p>"},{"location":"examples/computational_data/h5md_howto.html#imports","title":"Imports","text":"<pre><code>import numpy as np\nimport json\n\nimport h5py\nimport parmed as chem\nimport MDAnalysis as mda\nfrom pint import UnitRegistry\nureg = UnitRegistry()\n</code></pre> h5py module for reading and writing HDF5 files. UnitRegistry object from the pint package that provides assistance for working with units. We suggest using this package for easiest compatibility with NOMAD. If you have <code>nomad-lab</code> installed, you can alternatively import <code>ureg</code> with <code>from nomad.units import ureg</code>. MDAnalysis a library to analyze trajectories from molecular dynamics simulations stored in various formats. ParmEd a tool for aiding in investigations of biomolecular systems using popular molecular simulation packages."},{"location":"examples/computational_data/h5md_howto.html#example-data","title":"Example Data","text":"<p>For concreteness, we consider a fictitious set of \"vanilla\" molecular dynamics simulations, run with the OpenMM software. The following definitions set the dimensionality, periodicity, and the units for this simulation.</p> <pre><code>dimension = 3\nperiodicity = [True, True, True]\n\ntime_unit = 1.0 * ureg.picosecond\nlength_unit = 1.0 * ureg.angstrom\nenergy_unit = 1000. * ureg.joule\nmass_unit = 1.0 * ureg.amu\ncharge_unit = 1.0 * ureg.e\ntemperature_unit = 1.0 * ureg.K\ncustom_unit = 1.0 * ureg.newton / length_unit**2\nacceleration_unit = 1.0 * length_unit / time_unit**2\n</code></pre> <p>In this example, we will assume that the relevant simulation data is compatible with MDAnalysis, such that a universe containing the trajectory and topology information can be created.</p> <p>Note</p> <p>Knowledge of the MDAnalysis package is not necessary for understanding this example. The dimensions of the supplied quantities will be made clear in each case.</p> <p>Create a universe by supplying a <code>pdb</code> structure file and corresponding <code>dcd</code> trajectory file (MDAnalysis supports many different file formats): <pre><code>universe = mda.Universe('initial_structure.pdb', 'trajectory.dcd')\n\nn_frames = len(universe.trajectory)\nn_atoms = universe.trajectory[0].n_atoms\n</code></pre> Some topologies can be loaded directly into MDAnalysis. However, for simulations from OpenMM, one can read the topology using <code>parmed</code> and then import it to MDanalysis: <pre><code>pdb = app.PDBFile('initial_structure.pdb')\nforcefield = app.ForceField('force_field.xml')\nsystem = forcefield.createSystem(pdb.topology)\nstruct = chem.openmm.load_topology(pdb.topology, system)\nuniverse_toponly = mda.Universe(struct)\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#h5md-group","title":"H5MD Group","text":"<p>Create an HDF5 file called <code>test_h5md-nomad.h5</code> and create the group <code>h5md</code> under <code>root</code>: <pre><code>h5_write = h5py.File('test_h5md-nomad.h5', 'w')\nh5md = h5_write.create_group('h5md')\n</code></pre></p> <p>Add the h5md version (1.0.x in this case) as an attribute of the <code>h5md</code> group: <pre><code>h5md.attrs['version'] = [1, 0]\n</code></pre></p> <p>Create the <code>author</code> group and add the associated metadata: <pre><code>author = h5md.create_group('author')\nauthor.attrs['name'] = 'author name'\nauthor.attrs['email'] = 'author-name@example-domain.edu'\n</code></pre></p> <p>Create the <code>program</code> group and add the associated metadata: <pre><code>program = h5md.create_group('program')\nprogram.attrs['name'] = 'OpenMM'\nprogram.attrs['version'] = '7.7.0'\n</code></pre></p> <p>Create the <code>creator</code> group and add the associated metadata: <pre><code>program = h5md.create_group('creator')\nprogram.attrs['name'] = h5py.__name__\nprogram.attrs['version'] = str(h5py.__version__)\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#particles-group","title":"Particles Group","text":"<p>Create the <code>particles</code> group and the underlying <code>all</code> group to hold the relevant particle data: <pre><code>particles = h5_write.create_group('particles')\nparticles_group_all = particles.create_group('all')\n</code></pre></p> <p>Get the steps, times, positions, and lattice vectors (i.e., box dimensions) from the MDA universe: <pre><code># quantities extracted from MDAnalysis\nsteps = []\ntimes = []\npositions = []\nlattice_vectors = []\nfor i_frame, frame in enumerate(universe.trajectory):\n    times.append(frame.time)\n    steps.append(frame.frame)\n    positions.append(frame.positions)\n    lattice_vectors.append(frame.triclinic_dimensions)\n</code></pre></p> <p>Set the positions and corresponding metadata: <pre><code>position_group_all = particles_group_all.create_group('position')\nposition_group_all['step'] = steps  # shape = (n_frames)\nposition_group_all['time'] = times  # shape = (n_frames)\nposition_group_all['time'].attrs['unit'] = str(time_unit.units)\nposition_group_all['time'].attrs['unit_factor'] = time_unit.magnitude\nposition_group_all['value'] = positions  # shape = (n_frames, n_atoms, dimension)\nposition_group_all['value'].attrs['unit'] = str(length_unit.units)\nposition_group_all['value'].attrs['unit_factor'] = length_unit.magnitude\n</code></pre></p> <p>Set the particle-specific metadata: <pre><code>particles_group_all['species_label'] = universe_toponly.atoms.types  # shape = (n_atoms)\nparticles_group_all['force_field_label'] = universe_toponly.atoms.names  # shape = (n_atoms)\nparticles_group_all['mass'] = universe_toponly.atoms.masses  # shape = (n_atoms)\nparticles_group_all['mass'].attrs['unit'] = str(mass_unit.units)\nparticles_group_all['mass'].attrs['unit_factor'] = mass_unit.magnitude\nparticles_group_all['charge'] = universe_toponly.atoms.charges  # shape = (n_atoms)\nparticles_group_all['charge'].attrs['unit'] = str(charge_unit.units)\nparticles_group_all['charge'].attrs['unit_factor'] = charge_unit.magnitude\n</code></pre></p> <p>Create the <code>box</code> group under <code>particles.all</code> and write corresponding data: <pre><code>box_group = particles_group_all.create_group('box')\nbox_group.attrs['dimension'] = dimension\nbox_group.attrs['boundary'] = periodicity\nedges = box_group.create_group('edges')\nedges['step'] = steps\nedges['time'] = times\nedges['time'].attrs['unit'] = str(time_unit.units)\nedges['time'].attrs['unit_factor'] = time_unit.magnitude\nedges['value'] = lattice_vectors\nedges['value'].attrs['unit'] = str(length_unit.units)\nedges['value'].attrs['unit_factor'] = length_unit.magnitude\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#connectivity-group","title":"Connectivity Group","text":"<p>Create the <code>connectivity</code> group under <code>root</code> and add the tuples of bonds, angles, and dihedrals: <pre><code>connectivity = h5_write.create_group('connectivity')\nconnectivity['bonds'] = universe_toponly.bonds._bix  # shape = (n_bonds, 2)\nconnectivity['angles'] = universe_toponly.angles._bix  # shape = (n_angles, 3)\nconnectivity['dihedrals'] = universe_toponly.dihedrals._bix  # shape = (n_dihedrals, 4)\nconnectivity['impropers'] = universe_toponly.impropers._bix  # shape = (n_impropers, 4)\n</code></pre> Here <code>n_bonds</code>, <code>n_angles</code>, <code>n_dihedrals</code>, and <code>n_impropers</code> represent the corresponding number of instances of each interaction within the force field.</p> <p>You can read more about the creation of the hierarchical <code>particles_group</code> in Creating a topology.</p>"},{"location":"examples/computational_data/h5md_howto.html#observables-group","title":"Observables Group","text":"<p>For this section, we will consider sets of fabricated observable data for clarity. First, create the <code>observables</code> group under root: <pre><code>observables = h5_write.create_group('observables')\n</code></pre></p> <p>There are 3 types of support observables: <pre><code>types = ['configurational', 'ensemble_average', 'correlation_function']\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#configurational-observables","title":"Configurational Observables","text":"<p>Fabricated data: <pre><code>temperatures = 300. * np.ones(n_frames)\npotential_energies = 1.0 * np.ones(n_frames)\nkinetic_energies = 2.0 * np.ones(n_frames)\n</code></pre></p> <p>Create a <code>temperature</code> group and populate the associated metadata:</p> <pre><code>temperature = observables.create_group('temperature')\ntemperature.attrs['type'] = types[0]\ntemperature['step'] = steps\ntemperature['time'] = times\ntemperature['time'].attrs['unit'] = str(time_unit.units)\ntemperature['time'].attrs['unit_factor'] = time_unit.magnitude\ntemperature['value'] = temperatures\ntemperature['value'].attrs['unit'] = str(temperature_unit.units)\ntemperature['value'].attrs['unit_factor'] = temperature_unit.magnitude\n</code></pre> <p>Create an <code>energy</code> group to hold various types of energies. Add : <pre><code>energies = observables.create_group('energy')\n\npotential_energy = energies.create_group('potential')\npotential_energy.attrs['type'] = types[0]\npotential_energy['step'] = steps\npotential_energy['time'] = times\npotential_energy['time'].attrs['unit'] = str(time_unit.units)\npotential_energy['time'].attrs['unit_factor'] = time_unit.magnitude\npotential_energy['value'] = potential_energies\npotential_energy['value'].attrs['unit'] = str(energy_unit.units)\npotential_energy['value'].attrs['unit_factor'] = energy_unit.magnitude\n\nkinetic_energy = energies.create_group('kinetic')\nkinetic_energy.attrs['type'] = types[0]\nkinetic_energy['step'] = steps\nkinetic_energy['time'] = times\nkinetic_energy['time'].attrs['unit'] = str(time_unit.units)\nkinetic_energy['time'].attrs['unit_factor'] = time_unit.magnitude\nkinetic_energy['value'] = kinetic_energies\nkinetic_energy['value'].attrs['unit'] = str(energy_unit.units)\nkinetic_energy['value'].attrs['unit_factor'] = energy_unit.magnitude\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#ensemble-average-observables","title":"Ensemble Average Observables","text":"<p>Fabricated data - the following represents radial distribution function (rdf) data calculated between molecule types <code>X</code> and <code>Y</code>, stored in <code>rdf_MOLX-MOLY.xvg</code>: <pre><code>      0.24 0.000152428\n     0.245 0.00457094\n      0.25  0.0573499\n     0.255   0.284764\n      0.26   0.842825\n     0.265    1.64705\n      0.27    2.37243\n     0.275    2.77916\n      0.28    2.80622\n     0.285    2.60082\n      0.29    2.27182\n      ...\n</code></pre></p> <p>Store the rdf data in a dictionary along with some relevant metadata:</p> <pre><code>rdf_XX = np.loadtxt('rdf_MOLX-MOLX.xvg')\nrdf_XY = np.loadtxt('rdf_MOLX-MOLY.xvg')\nrdf_YY = np.loadtxt('rdf_MOLY-MOLY.xvg')\nrdfs = {\n    'MOLX-MOLX': {\n        'n_bins': len(rdf_XX[:, 0]),\n        'bins': rdf_XX[:, 0],\n        'value': rdf_XX[:, 1],\n        'type': 'molecular',\n        'frame_start': 0,\n        'frame_end': n_frames-1\n    },\n    'MOLX-MOLY': {\n        'n_bins': len(rdf_XY[:, 0]),\n        'bins': rdf_XY[:, 0],\n        'value': rdf_XY[:, 1],\n        'type': 'molecular',\n        'frame_start': 0,\n        'frame_end': n_frames-1\n    },\n    'MOLY-MOLY': {\n        'n_bins': len(rdf_YY[:, 0]),\n        'bins': rdf_YY[:, 0],\n        'value': rdf_YY[:, 1],\n        'type': 'molecular',\n        'frame_start': 0,\n        'frame_end': n_frames-1\n    }\n}\n</code></pre> <p>Now create the <code>radial_distribution_functions</code> group under <code>observables</code> and store each imported rdf: <pre><code>radial_distribution_functions = observables.create_group('radial_distribution_functions')\nfor key in rdfs.keys():\n    rdf = radial_distribution_functions.create_group(key)\n    rdf.attrs['type'] = types[1]\n    rdf['type'] = rdfs[key]['type']\n    rdf['n_bins'] = rdfs[key]['n_bins']\n    rdf['bins'] = rdfs[key]['bins']\n    rdf['bins'].attrs['unit'] = str(length_unit.units)\n    rdf['bins'].attrs['unit_factor'] = length_unit.magnitude\n    rdf['value'] = rdfs[key]['value']\n    rdf['frame_start'] = rdfs[key]['frame_start']\n    rdf['frame_end'] = rdfs[key]['frame_end']\n</code></pre></p> <p>We can also store scalar ensemble average observables. Let's consider some fabricated diffusion constant data: <pre><code>Ds = {\n    'MOLX': {\n        'value': 1.0,\n        'error_type': 'Pearson_correlation_coefficient',\n        'errors': 0.98\n    },\n    'MOLY': {\n        'value': 2.0,\n        'error_type': 'Pearson_correlation_coefficient',\n        'errors': 0.95\n    }\n}\n</code></pre></p> <p>Create the <code>diffusion constants</code> group under <code>observables</code> and store the correspond (meta)data:</p> <pre><code>diffusion_constants = observables.create_group('diffusion_constants')\nfor key in Ds.keys():\n    diffusion_constant = diffusion_constants.create_group(key)\n    diffusion_constant.attrs['type'] = types[1]\n    diffusion_constant['value'] = Ds[key]['value']\n    diffusion_constant['value'].attrs['unit'] = str(diff_unit.units)\n    diffusion_constant['value'].attrs['unit_factor'] = diff_unit.magnitude\n    diffusion_constant['error_type'] = Ds[key]['error_type']\n    diffusion_constant['errors'] = Ds[key]['errors']\n</code></pre>"},{"location":"examples/computational_data/h5md_howto.html#time-correlation-observables","title":"Time Correlation Observables","text":"<p>Fabricated data - the following represents mean squared displacement (msd) data calculated for molecule type <code>X</code>, stored in <code>msd_MOLX.xvg</code>: <pre><code>         0           0\n         2   0.0688769\n         4    0.135904\n         6    0.203573\n         8    0.271162\n        10    0.339284\n        12    0.410115\n        14    0.477376\n        16    0.545184\n        18     0.61283\n        ...\n</code></pre></p> <p>Store the msd data in a dictionary along with some relevant metadata:</p> <pre><code>msd_X = np.loadtxt('msd_MOLX.xvg')\nmsd_Y = np.loadtxt('msd_MOLY.xvg')\nmsds = {\n    'MOLX': {\n        'n_times': len(msd_X[:, 0]),\n        'times': msd_X[:, 0],\n        'value': msd_X[:, 1],\n        'type': 'molecular',\n        'direction': 'xyz',\n        'error_type': 'standard_deviation',\n        'errors': np.zeros(len(msd_X[:, 0])),\n    },\n    'MOLY': {\n        'n_times': len(msd_Y[:, 0]),\n        'times': msd_Y[:, 0],\n        'value': msd_Y[:, 1],\n        'type': 'molecular',\n        'direction': 'xyz',\n        'error_type': 'standard_deviation',\n        'errors': np.zeros(len(msd_Y[:, 0])),\n    }\n}\n</code></pre> <p>Now create the <code>mean_squared_displacements</code> group under <code>observables</code> and store each imported rdf:</p> <pre><code>mean_squared_displacements = observables.create_group('mean_squared_displacements')\nmsd_unit = length_unit * length_unit\ndiff_unit = msd_unit / time_unit\nfor key in msds.keys():\n    msd = mean_squared_displacements.create_group(key)\n    msd.attrs['type'] = types[2]\n    msd['type'] = msds[key]['type']\n    msd['direction'] = msds[key]['direction']\n    msd['error_type'] = msds[key]['error_type']\n    msd['n_times'] = msds[key]['n_times']\n    msd['times'] = msds[key]['times']\n    msd['times'].attrs['unit'] = str(time_unit.units)\n    msd['times'].attrs['unit_factor'] = time_unit.magnitude\n    msd['value'] = msds[key]['value']\n    msd['value'].attrs['unit'] = str(msd_unit.units)\n    msd['value'].attrs['unit_factor'] = msd_unit.magnitude\n    msd['errors'] = msds[key]['errors']\n</code></pre>"},{"location":"examples/computational_data/h5md_howto.html#parameter-group","title":"Parameter Group","text":"<p>Using the json templates for force calculations and molecular dynamics workflows, the (meta)data can be written to the H5MD-NOMAD file using the following code:</p> <p>First, import the data extracted from the JSON templates: <pre><code>with open('force_calculations_metainfo.json') as json_file:\n    force_calculation_parameters = json.load(json_file)\n\nwith open('workflow_metainfo.json') as json_file:\n    workflow_parameters = json.load(json_file)\n</code></pre></p> <p>Then, create the appropriate container groups: <pre><code>parameters = h5_write.create_group('parameters')\nforce_calculations = parameters.create_group('force_calculations')\nworkflow = parameters.create_group('workflow')\n</code></pre></p> <p>Now, recursively write the (meta)data: <pre><code>def get_parameters_recursive(parameter_group, parameter_dict):\n    # Store the parameters from parameter dict into an hdf5 file\n    for key, val in parameter_dict.items():\n        if type(val) == dict:\n            param = val.get('value')\n            if param is not None:\n                parameter_group[key] = param\n                unit = val.get('unit')\n                if unit is not None:\n                    parameter_group[key].attrs['unit'] = unit\n            else:  # This is a subsection\n                subsection = parameter_group.create_group(key)\n                subsection = get_parameters_recursive(subsection, val)\n        else:\n            if val is not None:\n                parameter_group[key] = val\n\n    return parameter_group\n\n\nforce_calculations = get_parameters_recursive(force_calculations, force_calculation_parameters)\nworkflow = get_parameters_recursive(workflow, workflow_parameters)\n</code></pre></p> <p>It's as simple as that! Now, we can upload our H5MD-NOMAD file directly to NOMAD and all the written (meta)data will be stored according to the standard NOMAD schema.</p>"},{"location":"examples/computational_data/h5md_howto.html#accessing-an-h5md-nomad-file","title":"Accessing an H5MD-NOMAD file","text":"<p>The following functions are useful for accessing data from your H5MD-NOMAD file: <pre><code>def apply_unit(quantity, unit, unit_factor):\n    from pint import UnitRegistry\n    ureg = UnitRegistry()\n\n    if quantity is None:\n        return\n    if unit:\n        unit = ureg(unit)\n        unit *= unit_factor\n        quantity *= unit\n\n    return quantity\n\ndef decode_hdf5_bytes(dataset):\n    if dataset is None:\n        return\n    elif type(dataset).__name__ == 'ndarray':\n        if dataset == []:\n            return dataset\n        dataset = np.array([val.decode(\"utf-8\") for val in dataset]) if type(dataset[0]) == bytes else dataset\n    else:\n        dataset = dataset.decode(\"utf-8\") if type(dataset) == bytes else dataset\n    return dataset\n\ndef hdf5_attr_getter(source, path, attribute, default=None):\n    '''\n    Extracts attribute from object based on path, and returns default if not defined.\n    '''\n    section_segments = path.split('.')\n    for section in section_segments:\n        try:\n            value = source.get(section)\n            source = value[-1] if isinstance(value, list) else value\n        except Exception:\n            return\n    value = source.attrs.get(attribute)\n    source = value[-1] if isinstance(value, list) else value\n    source = decode_hdf5_bytes(source) if source is not None else default\n    return source\n\ndef hdf5_getter(source, path, default=None):\n    '''\n    Extracts attribute from object based on path, and returns default if not defined.\n    '''\n    section_segments = path.split('.')\n    for section in section_segments:\n        try:\n            value = source.get(section)\n            unit = hdf5_attr_getter(source, section, 'unit')\n            unit_factor = hdf5_attr_getter(source, section, 'unit_factor', default=1.0)\n            source = value[-1] if isinstance(value, list) else value\n        except Exception:\n            return\n\n    if source is None:\n        source = default\n    elif type(source) == h5py.Dataset:\n        source = source[()]\n        source = apply_unit(source, unit, unit_factor)\n\n    return decode_hdf5_bytes(source)\n</code></pre></p> <p>Open your H5MD-NOMAD file with <code>h5py</code>: <pre><code>import h5py\n\nh5_read = h5py.File('test_h5md-nomad.h5', 'r')\n</code></pre></p> <p>Access a particular data set: <pre><code>potential_energies = h5_read['observables']['energy']['potential']['value']\nprint(potential_energies[()])\n</code></pre> result: <pre><code>array([1., 1., 1., 1., 1.])\n</code></pre></p> <p>Get the unit information for this quantity: <pre><code>unit = potential_energies.attrs['unit']\nunit_factor = potential_energies.attrs['unit_factor']\n\nprint(unit)\nprint(unit_factor)\n</code></pre></p> <p>results: <pre><code>joule\n1000.0\n</code></pre></p> <p>Alternatively, the above functions will return the dataset as python arrays, i.e., already applying <code>[()]</code> to the HDF5 element, and also apply the appropriate units where applicable: <pre><code>potential_energies = hdf5_getter(h5_read, 'observables.energy.potential.value')\nprint(potential_energies)\n</code></pre></p> <p>result: <pre><code>Magnitude\n[1000.0 1000.0 1000.0 1000.0 1000.0]\nUnits   joule\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#creating-a-topology-particles_group","title":"Creating a topology (<code>particles_group</code>)","text":"<p>This page demonstrates how to create a \"standard\" topology in H5MD-NOMAD. The demonstrated organization of molecules and monomers is identical to what other NOMAD parsers do to create a topology from native simulation files (e.g., outputs from GROMACS or LAMMPS). However, the user is free to deviate from this standard to create arbitrary organizations of particles, as described in Connectivity.</p>"},{"location":"examples/computational_data/h5md_howto.html#standard-topology-structure-for-bonded-force-fields","title":"Standard topology structure for bonded force fields","text":"<p><pre><code>topology\n\u251c\u2500\u2500 molecule_group_1\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 monomer_group_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502       \u251c\u2500\u2500 monomer_1\n\u2502    \u2502      \u2502       \u2502       \u2514\u2500\u2500 metadata for monomer_1\n\u2502    \u2502      \u2502       \u251c\u2500\u2500 monomer_2\n\u2502    \u2502      \u2502       \u2502       \u2514\u2500\u2500 metadata for monomer_2\n\u2502    \u2502      \u2502       \u251c\u2500\u2500 ...\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 monomer_group_2\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_2\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 molecule_group_2\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2514\u2500\u2500 metadata for molecule_1\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_2\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2514\u2500\u2500 metadata for molecule_2\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre> Here, the first level of organization is the \"molecule group\". Molecule groups contain molecules of the same type. In other words, <code>molecule_group_1</code> and <code>molecule_group_2</code> represent distinct molecule types. At the next level of the hierarchy, each molecule within this group is stored (i.e., <code>molecule_1</code>, <code>molecule_2</code>, etc.). In the above example, <code>molecule_group_1</code> represents a polymer (or protein). Thus, below the molecule level, there is a \"monomer group level\". Similar to the molecule group, the monomer group organizes all monomers (of the parent molecule) that are of the same type. Thus, for <code>molecule_1</code> of <code>molecule_group_1</code>, <code>monomer_group_1</code> and <code>monomer_group_2</code> represent distinct types of monomers existing within the polymer. Then, below <code>monomer_group_1</code>, each monomer within this group is stored. Finally, beneath these individual monomers, only the metadata for that monomer is stored (i.e., no further organization levels). Note however, that metadata can be (and is) stored at each level of the hierarchy, but is left out of the illustration for clarity. Notice also that <code>molecule_group_2</code> is not a polymer. Thus, each molecule within this group stores only the corresponding metadata, and no further levels of organization.</p>"},{"location":"examples/computational_data/h5md_howto.html#creating-the-standard-hierarchy-from-an-mdanalysis-universe","title":"Creating the standard hierarchy from an MDAnalysis universe","text":"<p>We start from the perspective of the Writing an HDF5 file according to H5MD-NOMAD with python section, with identical imports and assuming that an MDAnalysis <code>universe</code> is already instantiated from the raw simulation files. As in the previous example, the <code>universe</code> containing the topology information is called <code>universe_topology</code>.</p> <p>The following functions will be useful for creating the topology:</p> <pre><code>def get_composition(children_names):\n    '''\n    Given a list of children, return a compositional formula as a function of\n    these children. The format is &lt;child_1&gt;(n_child_1)&lt;child_2&gt;(n_child_2)...\n    '''\n    children_count_tup = np.unique(children_names, return_counts=True)\n    formula = ''.join([f'{name}({count})' for name, count in zip(*children_count_tup)])\n    return formula\n\n\ndef get_molecules_from_bond_list(n_particles: int, bond_list: List[int], particle_types: List[str] = None, particles_typeid=None):\n    '''\n    Returns a dictionary with molecule info from the list of bonds\n    '''\n\n    import networkx\n\n    system_graph = networkx.empty_graph(n_particles)\n    system_graph.add_edges_from([(i[0], i[1]) for i in bond_list])\n    molecules = [system_graph.subgraph(c).copy() for c in networkx.connected_components(system_graph)]\n    mol_dict = []\n    for i_mol, mol in enumerate(molecules):\n        mol_dict.append({})\n        mol_dict[i_mol]['indices'] = np.array(mol.nodes())\n        mol_dict[i_mol]['bonds'] = np.array(mol.edges())\n        mol_dict[i_mol]['type'] = 'molecule'\n        mol_dict[i_mol]['is_molecule'] = True\n        if particles_typeid is None and len(particle_types) == n_particles:\n            mol_dict[i_mol]['names'] = [particle_types[int(x)] for x in sorted(np.array(mol_dict[i_mol]['indices']))]\n        if particle_types is not None and particles_typeid is not None:\n            mol_dict[i_mol]['names'] = [particle_types[particles_typeid[int(x)]] for x in sorted(np.array(mol_dict[i_mol]['indices']))]\n        mol_dict[i_mol]['formula'] = get_composition(mol_dict[i_mol]['names'])\n\n    return mol_dict\n\n\ndef is_same_molecule(mol_1: dict, mol_2: dict):\n    '''\n    Checks whether the 2 input molecule dictionaries represent the same\n    molecule type, i.e., same particle types and corresponding bond connections.\n    '''\n\n    if sorted(mol_1['names']) == sorted(mol_2['names']):\n        mol_1_shift = np.min(mol_1['indices'])\n        mol_2_shift = np.min(mol_2['indices'])\n        mol_1_bonds_shift = mol_1['bonds'] - mol_1_shift\n        mol_2_bonds_shift = mol_2['bonds'] - mol_2_shift\n\n        bond_list_1 = [sorted((mol_1['names'][i], mol_1['names'][j])) for i, j in mol_1_bonds_shift]\n        bond_list_2 = [sorted((mol_2['names'][i], mol_2['names'][j])) for i, j in mol_2_bonds_shift]\n\n        bond_list_names_1, bond_list_counts_1 = np.unique(bond_list_1, axis=0, return_counts=True)\n        bond_list_names_2, bond_list_counts_2 = np.unique(bond_list_2, axis=0, return_counts=True)\n\n        bond_list_dict_1 = {bond[0] + '-' + bond[1]: bond_list_counts_1[i_bond] for i_bond, bond in enumerate(bond_list_names_1)}\n        bond_list_dict_2 = {bond[0] + '-' + bond[1]: bond_list_counts_2[i_bond] for i_bond, bond in enumerate(bond_list_names_2)}\n        if bond_list_dict_1 == bond_list_dict_2:\n            return True\n\n        return False\n\n    return False\n</code></pre> <p>Then, we can create the topology structure from the MDAnalysis universe:</p> <pre><code>bond_list = universe_toponly.bonds._bix\nmolecules = get_molecules_from_bond_list(n_atoms, bond_list, particle_types=universe_toponly.atoms.types, particles_typeid=None)\n\n# create the topology\nmol_groups = []\nmol_groups.append({})\nmol_groups[0]['molecules'] = []\nmol_groups[0]['molecules'].append(molecules[0])\nmol_groups[0]['type'] = 'molecule_group'\nmol_groups[0]['is_molecule'] = False\nfor mol in molecules[1:]:\n    flag_mol_group_exists = False\n    for i_mol_group in range(len(mol_groups)):\n        if is_same_molecule(mol, mol_groups[i_mol_group]['molecules'][0]):\n            mol_groups[i_mol_group]['molecules'].append(mol)\n            flag_mol_group_exists = True\n            break\n    if not flag_mol_group_exists:\n        mol_groups.append({})\n        mol_groups[-1]['molecules'] = []\n        mol_groups[-1]['molecules'].append(mol)\n        mol_groups[-1]['type'] = 'molecule_group'\n        mol_groups[-1]['is_molecule'] = False\n\n\nfor i_mol_group, mol_group in enumerate(mol_groups):\n    mol_groups[i_mol_group]['formula'] = molecule_labels[i_mol_group] + '(' + str(len(mol_group['molecules'])) + ')'\n    mol_groups[i_mol_group]['label'] = 'group_' + str(molecule_labels[i_mol_group])\n    mol_group_indices = []\n    for i_molecule, molecule in enumerate(mol_group['molecules']):\n        molecule['label'] = molecule_labels[i_mol_group]\n        mol_indices = molecule['indices']\n        mol_group_indices.append(mol_indices)\n        mol_resids = np.unique(universe_toponly.atoms.resindices[mol_indices])\n        if mol_resids.shape[0] == 1:\n            continue\n\n        res_dict = []\n        for i_resid, resid in enumerate(mol_resids):\n            res_dict.append({})\n            res_dict[i_resid]['indices'] = np.where( universe_toponly.atoms.resindices[mol_indices] == resid)[0]\n            res_dict[i_resid]['label'] = universe_toponly.atoms.resnames[res_dict[i_resid]['indices'][0]]\n            res_dict[i_resid]['formula'] = get_composition(universe_toponly.atoms.names[res_dict[i_resid]['indices']])\n            res_dict[i_resid]['is_molecule'] = False\n            res_dict[i_resid]['type'] = 'monomer'\n\n        res_groups = []\n        res_groups.append({})\n        res_groups[0]['residues'] = []\n        res_groups[0]['residues'].append(res_dict[0])\n        res_groups[0]['label'] = 'group_' + res_dict[0]['label']\n        res_groups[0]['type'] = 'monomer_group'\n        res_groups[0]['is_molecule'] = False\n        for res in res_dict[1:]:\n            flag_res_group_exists = False\n            for i_res_group in range(len(res_groups)):\n                if res['label'] == res_groups[i_res_group]['label']:\n                    res_groups[i_res_group]['residues'].append(res)\n                    flag_res_group_exists = True\n                    break\n            if not flag_res_group_exists:\n                res_groups.append({})\n                res_groups[-1]['residues'] = []\n                res_groups[-1]['residues'].append(res)\n                res_groups[-1]['label'] = 'group_' + res['label']\n                res_groups[-1]['formula'] = get_composition(universe_toponly.atoms.names[res['indices']])\n                res_groups[-1]['type'] = 'monomer_group'\n                res_groups[-1]['is_molecule'] = False\n\n        molecule['formula'] = ''\n        for res_group in res_groups:\n            res_group['formula'] = res_group['residues'][0]['label'] + '(' + str(len(res_group['residues'])) + ')'\n            molecule['formula'] += res_group['formula']\n            res_group_indices = []\n            for res in res_group['residues']:\n                res_group_indices.append(res['indices'])\n            res_group['indices'] = np.concatenate(res_group_indices)\n\n        mol_group['indices'] = np.concatenate(mol_group_indices)\n\n        molecule['residue_groups'] = res_groups\n</code></pre>"},{"location":"examples/computational_data/h5md_howto.html#writing-the-topology-to-an-h5md-nomad-file","title":"Writing the topology to an H5MD-NOMAD file","text":"<p>Here we assume an H5MD-NOMAD file has already been created, as demonstrated on the Writing an HDF5 file according to H5MD-NOMAD with python section, and that the <code>connectivity</code> group was created under the root level.</p> <p>Now, create the <code>particles_group</code> group under <code>connectivity</code> within our HDF5-NOMAD file: <pre><code>topology_keys = ['type', 'formula', 'particles_group', 'label', 'is_molecule', 'indices']\ncustom_keys = ['molecules', 'residue_groups', 'residues']\n\ntopology = connectivity.create_group('particles_group')\n\nfor i_mol_group, mol_group in enumerate(mol_groups):\n    hdf5_mol_group = topology.create_group('group_' + molecule_labels[i_mol_group])\n    for mol_group_key in mol_group.keys():\n        if mol_group_key not in topology_keys + custom_keys:\n            continue\n        if mol_group_key != 'molecules':\n            hdf5_mol_group[mol_group_key] = mol_group[mol_group_key]\n        else:\n            hdf5_molecules = hdf5_mol_group.create_group('particles_group')\n            for i_molecule, molecule in enumerate(mol_group[mol_group_key]):\n                hdf5_mol = hdf5_molecules.create_group('molecule_' + str(i_molecule))\n                for mol_key in molecule.keys():\n                    if mol_key not in topology_keys + custom_keys:\n                        continue\n                    if mol_key != 'residue_groups':\n                        hdf5_mol[mol_key] = molecule[mol_key]\n                    else:\n                        hdf5_residue_groups = hdf5_mol.create_group('particles_group')\n                        for i_res_group, res_group in enumerate(molecule[mol_key]):\n                            hdf5_res_group = hdf5_residue_groups.create_group('residue_group_' + str(i_res_group))\n                            for res_group_key in res_group.keys():\n                                if res_group_key not in topology_keys + custom_keys:\n                                    continue\n                                if res_group_key != 'residues':\n                                    hdf5_res_group[res_group_key] = res_group[res_group_key]\n                                else:\n                                    hdf5_residues = hdf5_res_group.create_group('particles_group')\n                                    for i_res, res in enumerate(res_group[res_group_key]):\n                                        hdf5_res = hdf5_residues.create_group('residue_' + str(i_res))\n                                        for res_key in res.keys():\n                                            if res_key not in topology_keys:\n                                                continue\n                                            if res[res_key] is not None:\n                                                hdf5_res[res_key] = res[res_key]\n</code></pre></p>"},{"location":"examples/computational_data/h5md_ref.html","title":"H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD","text":""},{"location":"examples/computational_data/h5md_ref.html#notable-changes-from-h5md-to-h5md-nomad","title":"Notable changes from H5MD to H5MD-NOMAD","text":"<p>In order to effectively parse and normalize the molecular simulation data, the H5MD-NOMAD schema extends the original H5MD framework while also enforces various restrictions to the schema. This section contains a list of such additions and restrictions. Here we distinguish between \"unused\" features, i.e., metadata that will be ignored by NOMAD and \"unsupported\" features, i.e., structures that will likely cause an error if used within an H5MD-NOMAD file for upload to NOMAD.</p>"},{"location":"examples/computational_data/h5md_ref.html#new-or-amended-features","title":"New or amended features","text":"<ul> <li> <p>additional standardized particles group elements</p> </li> <li> <p>boundary attribute changed to boolean datatype</p> </li> <li> <p>treatment of units</p> </li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#unused-features","title":"Unused features","text":"<ul> <li> <p>modules in h5md metadata</p> </li> <li> <p>arbitrary particle groups not parsed, group labeled <code>all</code> required</p> </li> <li> <p>image, species, and id elements of particles group</p> </li> <li> <p>non-standard elements in particles group</p> </li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#unsupported-features","title":"Unsupported features","text":"<ul> <li> <p>fixed step and time storage</p> </li> <li> <p>time-dependent particle lists</p> </li> <li> <p>time-dependent model labels for particles</p> </li> <li> <p>only partial support for grouping of observables by particle subgroups</p> </li> <li> <p>time-dependent connectivity elements</p> </li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#standardized-observables-in-h5md-nomad","title":"Standardized observables in H5MD-NOMAD","text":""},{"location":"examples/computational_data/h5md_ref.html#configurational","title":"configurational","text":"<ul> <li> <p><code>energy quantities</code> :</p> </li> <li> <p><code>radius_of_gyration</code> :</p> </li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#ensemble-average","title":"ensemble average","text":"<ul> <li><code>radial_distribution_function</code> :</li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#time-correlation","title":"time correlation","text":"<ul> <li><code>mean_squared_displacement</code> :</li> </ul>"},{"location":"examples/computational_data/metainfo.html","title":"Guide to Computational MetaInfo","text":""},{"location":"examples/computational_data/metainfo.html#overview-of-metadata-organization-for-computation","title":"Overview of metadata organization for computation","text":"<p>NOMAD stores all processed data in a well defined, structured, and machine readable format, known as the <code>archive</code>. The schema that defines the organization of (meta)data within the archive is known as the MetaInfo. See Explanation &gt; Data structure for general information about data structures and schemas in NOMAD.</p> <p>The following diagram is an overarching visualization of the most important archive sections for computational data:</p> <pre><code>archive\n\u251c\u2500\u2500 run\n\u2502  \u00a0 \u251c\u2500\u2500 method\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 atom_parameters\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 dft\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 forcefield\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u251c\u2500\u2500 system\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 atoms\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502     \u251c\u2500\u2500 positions\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502     \u251c\u2500\u2500 lattice_vectors\n\u2502  \u00a0 \u2502      \u2502     \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 calculation\n\u2502  \u00a0        \u251c\u2500\u2500 energy\n\u2502  \u00a0        \u251c\u2500\u2500 forces\n\u2502  \u00a0        \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 workflow2\n \u00a0\u00a0  \u251c\u2500\u2500 method\n \u00a0\u00a0  \u251c\u2500\u2500 inputs\n \u00a0\u00a0  \u251c\u2500\u2500 tasks\n \u00a0\u00a0  \u251c\u2500\u2500 outputs\n \u00a0\u00a0  \u2514\u2500\u2500 results\n</code></pre> <p>The most important section of the archive for computational data is the <code>run</code> section, which is divided into three main subsections: <code>method</code>, <code>system</code>, and <code>calculation</code>. <code>method</code> stores information about the computational model used to perform the calculation. <code>system</code> stores attributes of the atoms involved in the calculation, e.g., atom types, positions, lattice vectors, etc. <code>calculation</code> stores the output of the calculation, e.g., energy, forces, etc.</p> <p>The <code>workflow</code> section of the archive then stores information about the series of tasks performed to accumulate the (meta)data in the run section. The relevant input parameters for the workflow are stored in <code>method</code>, while the <code>results</code> section stores output from the workflow beyond observables of single configurations. For example, any ensemble-averaged quantity from a molecular dynamics simulation would be stored under <code>workflow/results</code>. Then, the <code>inputs</code>, <code>outputs</code>, and <code>tasks</code> sections define the specifics of the workflow. For some standard workflows, e.g., geometry optimization and molecular dynamics, the NOMAD normalizers For non-standard workflows, the parser (or more appropriately the corresponding normalizer) must populate these sections accordingly. See Standard and Custom Computational Workflows in NOMAD for more information about the structure of the workflow section, as well as instructions on how to upload custom workflows to link individual Entries in NOMAD.</p> <p>Attention</p> <p>We are currently performing a complete refactoring of the computational MetaInfo schema. Details and updates about this task, and how it may benefit your future usage of NOMAD, will be added below.</p>"},{"location":"examples/computational_data/parser_plugins.html","title":"Guide to computational parser plugins","text":"<p>NOMAD uses parsers to convert raw data (for example, output from computational software, instruments, or electronic lab notebooks) into NOMAD's common Archive format. This page provides a guide to the existing standard computational parsers in NOMAD.</p>"},{"location":"examples/computational_data/parser_plugins.html#parser-organization","title":"Parser organization","text":"<p>Note</p> <p>The majority of NOMAD's computational parsers do not yet exist as plugins. Instead, they are linked to the nomad-lab software as submodules. We will be migrating these parsers to proper plugins in the near future, and may reorganize the projects described below. We will update this page accordingly.</p> <p>The NOMAD computational parsers can be found within your local NOMAD distribution under <code>&lt;path_to_nomad-lab&gt;/dependencies/parsers/&lt;parserproject&gt;</code>, where <code>&lt;parser_project&gt;</code> corresponds to the following organization:</p> <ul> <li>atomistic - Parsers for output from classical molecular simulations, e.g., from Gromacs, Lammps, etc.</li> <li>database - Parsers for various databases, e.g., OpenKim.</li> <li>eelsdb - Parser for the EELS database (https://eelsdb.eu/; to be integrated in the database project).</li> <li>electronic - Parsers for output from electronic structure calculations, e.g., from Vasp, Fhiaims, etc. </li> <li>nexus - Parsers for combining various instrument output formats and electronic lab notebooks.</li> <li>workflow - Parsers for output from task managers and workflow schedulers.</li> </ul> <p>You can also examine the source code of the parsers by following the above links to the corresponding GitHub repository for each project. Within each project folder you will find a <code>test/</code> directory, containing the parser tests, and also a directory containing the parsers' source code, <code>&lt;parserproject&gt;parser</code> or <code>&lt;parserproject&gt;parsers</code>, depending on if one or more parsers are contained within the project, respectively. In the case of multiple parsers, the files for individual parsers are contained within a corresponding subdirectory: <code>&lt;parserproject&gt;parsers/&lt;parsername&gt;</code> For example, the Quantum Espresso parser files are found in <code>dependencies/parsers/electronic/electronicparsers/quantumespresso/</code>.</p>"},{"location":"examples/computational_data/parser_plugins.html#developing-your-own-parser-plugin","title":"Developing your own parser plugin","text":""},{"location":"examples/computational_data/parser_plugins.html#prerequisites","title":"Prerequisites","text":"<p>The general docs contain information about the nuts and bolts of developing a plugin:</p> <ul> <li> <p>How to write a plugin: Some basic information about different types of plugins, plugin anatomy, and creating a plugin project.</p> </li> <li> <p>How to write a parser: The basics of how NOMAD parsers work, how to configure the files that your parser will match, and how to utilize existing parser classes.</p> </li> </ul> <p>Attention</p> <p>This page is under construction as we convert NOMAD's standard computational parsers to parser plugins. Along the way, we will add content below to guide you in the development of your own computational parser plugins.</p>"},{"location":"examples/computational_data/schema_plugins.html","title":"Guide to computational schema plugins","text":"<p>NOMAD uses Schemas to define the data structures and organization of Processed Data. Schemas can be defined in yaml or python formats. How to write a schema describes the basics of writing a schema, in the yaml format. Computational schemas in NOMAD have historically been written in python. There are several existing computational schema plugin projects for reference:</p> <ul> <li>nomad-schema-plugin-run: contains schemas for standard processed computational data, stored in the <code>run</code> section within the NOMAD archive.</li> </ul> <ul> <li> <p>nomad-schema-plugin-simulation-data: contains schemas for standard processed computational data, stored in the <code>data</code> section within the NOMAD archive.</p> </li> <li> <p>nomad-schema-plugin-simulation-workflow: contains schemas for standard computational workflows defined in NOMAD.</p> </li> <li> <p>nomad-normalizer-plugin-simulation-workflow: contains schemas for standard computational \"normalized\" data.</p> </li> <li> <p>nomad-schema-plugin-example: contains an example for NOMAD schema plugins. It should be forked to create actual plugins.</p> </li> </ul> <p>Guide to Computational MetaInfo describes how these schemas are used to organize standard computational data within an Entry in the NOMAD repository.</p> <p>Attention</p> <p>This page is under construction. We will be adding content below to guide you in the development of your own computational schema plugins.</p>"},{"location":"examples/computational_data/uploading.html","title":"Quick Start: Uploading computational data in NOMAD","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p> <p>This page provides an overview of NOMAD's usage with computational data. If you are completely new to NOMAD, we recommend to first read through the Navigating to NOMAD, Uploading and publishing data, and Exploring data tutorials.</p> <p>Uploading data in NOMAD can be done in several ways:</p> <ul> <li>By dragging-and-dropping your files into the <code>PUBLISH &gt; Uploads</code> page: suitable for users who have a relatively small amount of data.</li> <li>By using the Python-based NOMAD API: suitable for users who have larger datasets and need to automatize the upload.</li> <li>By using the shell command <code>curl</code> for sending files to the upload: suitable for users who have larger datasets and need to automatize the upload.</li> </ul> <p>You can upload the files one by one or you can zip them in <code>.zip</code> or <code>.tar.gz</code> formats to upload a larger amount of files at once.</p>"},{"location":"examples/computational_data/uploading.html#drag-and-drop-uploads","title":"Drag-and-drop uploads","text":"<p>On the top-left menu, click on <code>PUBLISH &gt; Uploads</code>.</p> <p></p> <p>You can then click on <code>CREATE A NEW UPLOAD</code> or try one of the example uploads by clicking in <code>ADD EXAMPLE UPLOADS</code> and selecting one of the multiple options, including data from an ELN, various instruments, or computational software. For a clear demonstration of the entire process, we will use the following example data:</p> <p> Download Example Data </p> <p>This particular example represents a computational workflow to investigate some properties of Si~2~, however the details are not important for our demonstration here.</p> <p>After downloading the example <code>.zip</code> file, you can drag-and-drop it or click on the <code>CLICK OR DROP FILES</code> button to browse through your local directories.</p> <p></p> <p>After the files are uploaded, a processing is triggered. This generally includes an automatic identification of the uploaded files that are supported in NOMAD, and then a corresponding processing to harvest all the relevant (meta)data. The precise details of the processing depend on each use-case. For example, you can find out more about the processing of computational data in Processing of computational data.</p> <p>You will receive an email when the upload processing is finished.</p>"},{"location":"examples/computational_data/uploading.html#sections-of-the-uploads-page","title":"Sections of the Uploads page","text":"<p>At the top of the uploads page, you can modify certain general metadata fields.</p> <p></p> <p>The name of the upload can be modify by clicking on the pen icon . The other icons correspond to:</p> <ul> <li> Manage members: allows users to invite collaborators by defining co-authors and reviewers roles.</li> <li> Download files: downloads all files present in the upload.</li> <li> Reload: reloads the uploads page.</li> <li> Reprocess: triggers again the processing of the uploaded data.</li> <li> API: generates a JSON response to use by the NOMAD API.</li> </ul> <ul> <li> Delete the upload: deletes completely the upload.</li> </ul> <p>The remainder of the uploads page is divided in 4 sections.</p>"},{"location":"examples/computational_data/uploading.html#prepare-and-upload-your-files","title":"Prepare and upload your files","text":"<p>This section shows the files and folder structure in the upload. You can add a <code>README.md</code> in the root directory and its content will be shown above this section.</p> <p></p>"},{"location":"examples/computational_data/uploading.html#process-data","title":"Process data","text":"<p>This section shows the processed data and the generated entries in NOMAD.</p> <p></p>"},{"location":"examples/computational_data/uploading.html#edit-author-metadata","title":"Edit author metadata","text":"<p>This section allows users to edit certain metadata fields from all entries recognized in the upload. This includes comments, where you can add as much extra information as you want, references, where you can add a URL to your upload (e.g., an article DOI), and datasets, where you can create or add the uploaded data into a more general dataset (see Organizing data in datasets).</p> <p> </p>"},{"location":"examples/computational_data/uploading.html#publish","title":"Publish","text":"<p>This section lets the user to publish the data with or without an embargo.</p> <p></p>"},{"location":"examples/computational_data/uploading.html#publishing","title":"Publishing","text":"<p>After uploading and a successful parsing, congratulations! Now you can publish your data and let other users browse through it and re-use it for other purposes.</p> <p></p> <p>You can define a specific <code>Embargo period</code> of up to 36 months, after which the data will be made publicly available under the CC BY 4.0 license.</p> <p>After publishing by clicking on <code>PUBLISH</code>, the uploaded files cannot be altered. However, you can still edit the metadata fields.</p>"},{"location":"examples/computational_data/uploading.html#organizing-data-in-datasets","title":"Organizing data in datasets","text":"<p>You can organize your uploads and individual entries by grouping them into common datasets.</p> <p>In the uploads page, click on <code>EDIT AUTHOR METADATA OF ALL ENTRIES</code>.</p> <p>Under <code>Datasets</code> you can either <code>Create a new dataset</code> or <code>Search for an existing dataset</code>. After selecting the dataset, click on <code>SUBMIT</code>.</p> <p>Now, the defined dataset will be defined under <code>PUBLISH &gt; Datasets</code>.</p> <p></p> <p>The icon  allows you to assign a DOI to a specific dataset. Once a DOI has been assign to a dataset, no more data can be added to it. This can then be added into your publication so that it can be used as a reference, e.g., see the Data availability statement in M. Kuban et al., Similarity of materials and data-quality assessment by fingerprinting, MRS Bulletin 47, 991-999 (2022).</p>"},{"location":"examples/computational_data/uploading.html#processing-of-computational-data","title":"Processing of computational data","text":"<p>See From files to data and Processing for full explanations about data processing in NOMAD.</p> <p>When data is uploaded to NOMAD, the software interprets the files and determines which of them is a mainfile. Any other files in the upload can be viewed as auxiliary files. In the same upload, there might be multiple mainfiles and auxiliary files organized in a folder tree structure.</p> <p>The mainfiles are the main output file of a calculation. The presence of a mainfile in the upload is key for NOMAD to recognize a calculation. In NOMAD, we support an array computational codes for first principles calculations, molecular dynamics simulations, and lattice modeling, as well as workflow and database managers. For each code, NOMAD recognizes a single file as the mainfile. For example, the VASP mainfile is by default the <code>vasprun.xml</code>, although if the <code>vasprun.xml</code> is not present in the upload NOMAD searches the <code>OUTCAR</code> file and assigns it as the mainfile (see VASP POTCAR stripping).</p> <p>The rest of files which are not the mainfile are auxiliary files. These can have several purposes and be supported and recognized by NOMAD in the parser. For example, the <code>band*.out</code> or <code>GW_band*</code> files in FHI-aims are auxiliary files that allows the NOMAD FHI-aims parser to recognize band structures in DFT and GW, respectively.</p> <p>You can see the full list of supported codes, mainfiles, and auxiliary files in the general NOMAD documentation under Supported parsers.</p> <p>We recommend that the user keeps the folder structure and files generated by the simulation code, but without reaching the uploads limits. Please, also check our recommendations on Best Practices: preparing the data and folder structure.</p>"},{"location":"examples/computational_data/uploading.html#structured-data-with-the-nomad-metainfo","title":"Structured data with the NOMAD metainfo","text":"<p>Once the mainfile has been recognized, a new entry in NOMAD is created and a specific parser is called. The auxliary files are searched by and accessed within the parser.</p> <p>For this new entry, NOMAD generates a NOMAD archive. It will contain all the (meta)information extracted from the unstructured raw data files but in a structured, well defined, and machine readable format. This metadata provides context to the raw data, i.e., what were the input methodological parameters, on which material the calculation was performed, etc. We define the NOMAD Metainfo as all the set of sections, subsections, and quantities used to structure the raw data into a structured schema. Further information about the NOMAD Metainfo is available in the general NOMAD documentation page in Learn &gt; Structured data.</p> <p></p>"},{"location":"examples/computational_data/uploading.html#nomad-sections-for-computational-data","title":"NOMAD sections for computational data","text":"<p>Under the <code>Entry</code> / <code>archive</code> section, there are several sections and quantities being populated by the parsers. For computational data, only the following sections are populated:</p> <ul> <li><code>metadata</code>: contains general and non-code specific metadata. This is mainly information about authors, creation of the entry time, identifiers (id), etc.</li> <li><code>run</code>: contains the parsed and normalized raw data into the structured NOMAD schema. This is all the possible raw data which can be translated into a structured way.</li> <li><code>workflow2</code>: contains metadata about the specific workflow performed within the entry. This is mainly a set of well-defined workflows, e.g., <code>GeometryOptimization</code>, and their parameters.</li> <li><code>results</code>: contains the normalized and search indexed metadata. This is mainly relevant for searching, filtering, and visualizing data in NOMAD.</li> </ul> <code>workflow</code> and <code>workflow2</code> sections: development and refactoring <p>You have probably noticed the name <code>workflow2</code> but also the existence of a section called <code>workflow</code> under <code>archive</code>. This is because <code>workflow</code> is an old version of the workflow section, while <code>workflow2</code> is the new version. Sometimes, certain sections suffer a rebranding or refactoring, in most cases to add new features or to polish them after we receive years of feedback. In this case, the <code>workflow</code> section will remain until all older entries containing such section are reprocessed to transfer this information into <code>workflow2</code>.</p>"},{"location":"examples/computational_data/uploading.html#parsing","title":"Parsing","text":"<p>A parser is a Python module which reads the code-specific mainfile and auxiliary files and populates the <code>run</code> and <code>workflow2</code> sections of the <code>archive</code>, along with all relevant subsections and quantities.</p> <p>Parsers are added to NOMAD as plugins and are divided in a set of Github sub-projects under the main NOMAD repository.</p>"},{"location":"examples/computational_data/uploading.html#normalizing","title":"Normalizing","text":"<p>After the parsing populates the <code>run</code> and <code>workflow2</code> sections, an extra layer of Python modules is executed on top of the processed NOMAD metadata. This has two main purposes: 1. normalize or homogenize certain metadata parsed from different codes, and 2. populate the <code>results</code> section. For example, this is the case of normalizing the density of states (DOS) to its size intensive value, independently of the code used to calculate the DOS. The set of normalizers relevant for computational data are listed in <code>/nomad/config/models.py</code> and are executed in the specific order defined there. Their roles are explained more in detail in Processing.</p>"},{"location":"examples/computational_data/uploading.html#search-indexing-and-storing","title":"Search indexing (and storing)","text":"<p>The last step is to store the structured metadata and pass some of it to the search index. The metadata which is passed to the search index is defined in the <code>results</code> section. These metadata can then be searched by filtering in the Entries page of NOMAD or by writing a Python script which searches using the NOMAD API.</p>"},{"location":"examples/computational_data/uploading.html#entries-overview-page","title":"Entries OVERVIEW page","text":"<p>Once the parsers and normalizers finish, the Uploads page will show if the processing of the entry was a <code>SUCCESS</code> or a <code>FAILURE</code>. The entry information can be browsed by clicking on the  icon.</p> <p>You will land on the <code>OVERVIEW</code> page of the entry. On the top menu you can further select the <code>FILES</code> page, the <code>DATA</code> page, and the <code>LOGS</code> page.</p> <p></p> <p>The overview page contains a summary of the parsed metadata, e.g., tabular information about the material and methodology of the calculation (in the example, a G0W0 calculation done with the code exciting for bulk Si<sub>2</sub>), and visualizations of the system and some relevant properties. We note that all metadata are read directly from <code>results</code>.</p>"},{"location":"examples/computational_data/uploading.html#logs-page","title":"LOGS page","text":"<p>In the <code>LOGS</code> page, you can find information about the processing. You can read error, warning, and critical messages which can provide insight if the processing of an entry was a <code>FAILURE</code>.</p> <p></p> <p>We recommend you to Get support or contact our team in case you find <code>FAILURE</code> situations. These might be due to bugs which we are rapidly fixing, and whose origin might be varied: from a new version of a code which is not yet supported to wrong handling of potential errors in the parser script. It may also be a problem with the organization of the data in the folders. In order to minimize these situations, we suggest that you read Best Practices: preparing the data and folder structure.</p>"},{"location":"examples/computational_data/uploading.html#data-page","title":"DATA page","text":"<p>The <code>DATA</code> page contains all the structured NOMAD metainfo populated by the parser and normalizers. This is the most important page in the entry, as it contains all the relevant metadata which will allow users to find that specific simulation.</p> <p></p> <p>Furthermore, you can click on the  icon to download the NOMAD <code>archive</code> in a JSON format.</p>"},{"location":"examples/computational_data/uploading.html#best-practices-preparing-the-data-and-folder-structure","title":"Best Practices: preparing the data and folder structure","text":"<p>Attention</p> <p>Under construction.</p>"},{"location":"examples/computational_data/uploading.html#vasp-potcar-stripping","title":"VASP POTCAR stripping","text":"<p>For VASP data, NOMAD complies with the licensing of the <code>POTCAR</code> files. In agreement with Georg Kresse, NOMAD extracts the most important information of the <code>POTCAR</code> file and stores them in a stripped version called <code>POTCAR.stripped</code>. The <code>POTCAR</code> files are then automatically removed from the upload, so that you can safely publish your data.</p>"},{"location":"examples/computational_data/workflows.html","title":"Standard and Custom Computational Workflows in NOMAD","text":"<p>The following examples contain the basic knowledge on understanding and learning to use NOMAD workflows, and its relation with DFT and beyond-DFT (GW, BSE, DMFT, etc.) methodologies. You will use a fictitious example of a simulation workflow with the following files and folder structure: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u2514\u2500\u2500 pressure2\n \u00a0\u00a0 \u251c\u2500\u2500 temperature1\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 temperature2\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n \u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n \u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n</code></pre></p> <p>which can be downloaded here:  Download example_files.zip </p> <p>Each of the mainfiles represent an electronic-structure calculation (either DFT{:target=\"blank\"}, TB, or DMFT) which in turn is then parsed into a singular _entry in NOMAD. When dragged into the NOMAD Upload page{:target=\"blank\"}, these files should generate 8 entries in total. This folder structure presents a typical workflow calculation which can be represented as a provenance graph: <pre><code>graph LR;\n    A2((Inputs)) --&gt; B2[DFT];\n    A1((Inputs)) --&gt; B1[DFT];\n    subgraph pressure P2\n    B2[DFT] --&gt; C2[TB];\n    C2[TB] --&gt; D21[DMFT at T1];\n    C2[TB] --&gt; D22[DMFT at T2];\n    end\n    D21[DMFT at T1] --&gt; E21([Output calculation P2, T1])\n    D22[DMFT at T2] --&gt; E22([Output calculation P2, T2])\n    subgraph pressure P1\n    B1[DFT] --&gt; C1[TB];\n    C1[TB] --&gt; D11[DMFT at T1];\n    C1[TB] --&gt; D12[DMFT at T2];\n    end\n    D11[DMFT at T1] --&gt; E11([Output calculation P1, T1])\n    D12[DMFT at T2] --&gt; E12([Output calculation P1, T2])</code></pre> Here, \"Input\" refers to the all _input information given to perform the calculation (e.g., atom positions, model parameters, experimental initial conditions, etc.). \"DFT\", \"TB\" and \"DMFT\" refer to individual tasks of the workflow, which each correspond to a SinglePoint entry in NOMAD. \"Output calculation\" refers to the output data of each of the final DMFT tasks.</p> <p>The goal of this part is to set up the following workflows:</p> <ol> <li>A <code>SinglePoint</code> workflow for one of the calculations (e.g., the DFT one) in the <code>pressure1</code> subfolder.</li> <li>An overarching workflow entry for each pressure P<sub>i=1,2</sub>, grouping all <code>SinglePoint</code> \"DFT\", \"TB\", \"DMFT at T<sub>1</sub>\", and \"DMFT at T<sub>2</sub>\" tasks.</li> <li>A top level workflow entry, grouping together all pressure calculations.</li> </ol> <p>The files for all these cases can be downloaded here:  Download worfklowyaml_files.zip </p> <p>You can try writing these files yourself first, and then compare them with the tested files.</p>"},{"location":"examples/computational_data/workflows.html#starting-example-singlepoint-workflow","title":"Starting example: SinglePoint workflow","text":"<p>NOMAD is able to recognize certain workflows in an automatic way, such as the <code>SinglePoint</code> case mentioned above. However, to showcase how to the use workflows in NOMAD, you will learn how to \"manually\" construct the SinglePoint workflow, represented by the following provenance graph: <pre><code>graph LR;\n    A((Inputs)) --&gt; B[DFT];\n    B[DFT] --&gt; C([Output calculation]);</code></pre> To define a workflow manually in NOMAD, you must add a YAML file to the upload folder that contains the relevant input, output, and task information. This file should be named <code>&lt;filename&gt;.archive.yaml</code>. In this case, you should include the file <code>single_point.archive.yaml</code> with the following content:</p> <pre><code>workflow2:\n  name: SinglePoint\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n  outputs:\n    - name: Output calculation\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at Pressure P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>Note several things about the content of this file:</p> <ol> <li><code>name</code> keys are optional.</li> <li>The root path of the upload can be referenced with <code>../upload/archive/mainfile/</code>. Starting from there, the original directory tree structure of the upload is maintained.</li> <li><code>inputs</code> reference the section containing inputs of the whole workflow. In this case this is the section <code>run[0].system[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>outputs</code> reference the section containing outputs of the whole workflow. In this case this is the section <code>run[0].calculation[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>tasks</code> reference the section containing tasks of each step in the workflow. These must also contain <code>inputs</code> and <code>outputs</code> properly referencing the corresponding sections; this will then link inputs/outputs/tasks in the NOMAD Archive. In this case this is a <code>TaskReference</code> to the section <code>workflow2</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>section</code> reference to the uploaded mainfile specific section. The left side of the <code>#</code> symbol contains the path to the mainfile, while the right contains the path to the section.</li> </ol> <p>This will produce an extra entry with the following Overview content:</p> <p></p> <p>Note that you are referencing sections which are lists. Thus, in each case you should be careful to reference the correct section for inputs and outputs (example: a <code>GeometryOptimization</code> workflow calculation will have the \"Input structure\" as <code>run[0].system[0]</code>, while the \"Output calculation\" would also contain <code>run[0].system[-1]</code>, and all intermediate steps must input/output the corresponding section system).</p> <p>NOMAD workflow filename</p> <p>The NOMAD workflow YAML file name, i.e., <code>&lt;filename&gt;</code> in the explanation above, can be any custom name defined by the user, but the file must keep the extension <code>.archive.yaml</code> at the end. This is done in order for NOMAD to recognize this file as a custom schema. Custom schemas are widely used in experimental parsing, and you can learn more about them in the FAIRmat tutorial 8.</p> <p>You can extend the workflow meta-information by adding the metholodogical input parameters. These are stored in NOMAD in the section path <code>run[0].method[-1]</code>. The new <code>single_point.archive.yaml</code> will be:</p> <pre><code>workflow2:\n  name: SinglePoint\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n    - name: Input methodology parameters\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\n  outputs:\n    - name: Output calculation\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at Pressure P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n        - name: Input methodology parameters\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\n      outputs:\n        - name: Output calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>which in turn produces a similar workflow than before, but with an extra input node:</p> <p></p>"},{"location":"examples/computational_data/workflows.html#pressure-workflows","title":"Pressure workflows","text":"<p>Now that you know the basics of the workflow YAML schema, let's try to define an overarching workflow for each of the pressures. For this section, you will learn how to create the workflow YAML schema for the P<sub>1</sub> case; the extension for P<sub>2</sub> is then a matter of changing names and paths in the YAML files. For simplicity, you can skip referencing to methodologies.</p> <p>Thus, the <code>inputs</code> can be defined as: <pre><code>workflow2:\n  name: DFT+TB+DMFT at P1\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n</code></pre> and there are two <code>outputs</code>, one for each of the DMFT calculations at distinct temperatures: <pre><code>  outputs:\n    - name: Output DMFT at P1, T1 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n    - name: Output DMFT at P1, T2 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Now, <code>tasks</code> are defined for each of the methodologies performed (each corresponding to an underlying SinglePoint workflow). To define a valid workflow, each task must contain an input that corresponds to one of the outputs of the previous task. Moreover, the first task should take as input the overall input of the workflow, and the final task should also have as an output the overall workflow output. Then: <pre><code>  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DFT at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/tb_p1.wout#/workflow2'\n      name: TB at P1\n      inputs:\n        - name: Input DFT at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n      outputs:\n        - name: Output TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\n      name: DMFT at P1 and T1\n      inputs:\n        - name: Input TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n      outputs:\n        - name: Output DMFT at P1, T1 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\n      name: DMFT at P1 and T2\n      inputs:\n        - name: Input TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n      outputs:\n        - name: Output DMFT at P1, T2 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Note here:</p> <ul> <li>The <code>inputs</code> for each subsequent step are the <code>outputs</code> of the previous step.</li> <li>The final two <code>outputs</code> coincide with the <code>workflow2</code> <code>outputs</code>.</li> </ul> <p>This workflow (<code>pressure1.archive.yaml</code>) file will then produce an entry with the following Overview page:</p> <p></p> <p>Similarly, for P<sub>2</sub> you can upload a new <code>pressure2.archive.yaml</code> file with the same content, except when substituting 'pressure1' and 'p1' by their counterparts. This will produce a similar graph than the one showed before but for \"P2\".</p>"},{"location":"examples/computational_data/workflows.html#the-top-level-workflow","title":"The top-level workflow","text":"<p>After adding the workflow YAML files, Your upload folder directory now looks like: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure1.archive.yaml\n\u251c\u2500\u2500 pressure2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure2.archive.yaml\n\u2514\u2500\u2500 single_point.archive.yaml\n</code></pre> In order to define the general workflow that groups all pressure calculations, YOU can reference directly the previous <code>pressureX.archive.yaml</code> files as tasks. Still, <code>inputs</code> and <code>outputs</code> must be referenced to their corresponding mainfile and section paths.</p> <p>Create a new <code>fullworkflow.archive.yaml</code> file with the <code>inputs</code>: <pre><code>workflow2:\n  name: Full calculation at different pressures for SrVO3\n  inputs:\n    - name: Input structure at P1\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n    - name: Input structure at P2\n      section: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n</code></pre> And <code>outputs</code>: <pre><code>  outputs:\n    - name: Output DMFT at P1, T1 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n    - name: Output DMFT at P1, T2 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n    - name: Output DMFT at P2, T1 calculation\n      section: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n    - name: Output DMFT at P2, T2 calculation\n      section: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Finally, <code>tasks</code> references the previous YAML schemas as follows: <pre><code>  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1.archive.yaml#/workflow2'\n      name: DFT+TB+DMFT at P1\n      inputs:\n        - name: Input structure at P1\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DMFT at P1, T1 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n        - name: Output DMFT at P1, T2 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure2.archive.yaml#/workflow2'\n      name: DFT+TB+DMFT at P2\n      inputs:\n        - name: Input structure at P2\n          section: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DMFT at P2, T1 calculation\n          section: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n        - name: Output DMFT at P2, T2 calculation\n          section: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre></p> <p>This will produce the following entry and its Overview page:</p> <p></p>"},{"location":"examples/computational_data/workflows.html#automatic-workflows","title":"Automatic workflows","text":"<p>There are some cases where the NOMAD infrastructure is able to recognize certain workflows automatically when processing the uploaded files. The simplest example is any <code>SinglePoint</code> calculation, as explained above. Other examples include <code>GeometryOptimization</code>, <code>Phonons</code>, <code>GW</code>, and <code>MolecularDynamics</code>. Automated workflow detection may require your folder structure to fulfill certain conditions.</p> <p>Here are some general guidelines for preparing your upload folder in order to make it easier for the automatic workflow recognition to work:</p> <ul> <li>Always organize your files in an top-down structure, i.e., the initial tasks should be upper in the directory tree, while the later tasks lower on it.</li> <li>Avoid having to go up and down between folders if some properties are derived between these files. These situations are very complicated to predict for the current NOMAD infrastructure.</li> <li>Avoid duplication of files in subfolders. If initially you do a calculation A from which a later calculation B is derived and you want to store B in a subfolder, there is no need to copy the A files inside the subfolder B.</li> </ul> <p>The folder structure used throughout this part is a good example of a clean upload which is friendly and easy to work with when defining NOMAD workflows.</p>"},{"location":"examples/experiment_data/apm.html","title":"Domain-specific examples for atom probe tomography and field ion microscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p>"},{"location":"examples/experiment_data/apm.html#contextualization-for-the-technique-and-the-scientific-domain","title":"Contextualization for the technique and the scientific domain","text":"<p>A variety of file formats are used in the research field of atom probe tomography to document atom probe measurements and computer simulations. The pynxtools-apm plugin of the pynxtools parsing library solves the challenge of how these formats can be parsed and normalized into a common representation that increases interoperability and adds semantic expressiveness.</p> <ul> <li>pynxtools-apm</li> </ul> <p>The plugin uses the ifes_apt_tc_data_modeling Python library that is developed together with the International Field Emission Society.</p>"},{"location":"examples/experiment_data/apm.html#work-with-standardized-atom-probe-data-in-nomad-and-north","title":"Work with standardized atom probe data in NOMAD and NORTH","text":"<p>Once standardized, NXapm-compliant data in NOMAD can be explored with domain-specific software tools using a convenient JupyterHub-based service offered by the NOMAD Remote Tools Hub (NORTH).</p> <p>For this you should go to <code>Analyze</code> (menu bar) and select <code>NOMAD Remote Tools Hub</code>. If you are using the public Oasis the hub is available here. This service and toolset of NOMAD offers a set of preconfigured containers. We have configured the <code>apmtools</code> container to provide you an example of the capabilities that come with such customizable containers interfaced to an Oasis. Mind that if you are using a local Oasis the hub may not have been activated and that specific containers need to be pull from the container registry as they are not part of a default Oasis development installation.</p> <p>Currently, <code>apmtools</code> is a specific docker container that offers a graphical user interface through the web browser that serves your NOMAD GUI. The container includes three data analysis tools: - aptyzer by Alexander Reichmann et al. - paraprobe-toolbox by Markus K\u00fchbach et al. - apav by Jesse Smith et al.</p> <p>The container is configured such that the data in your uploads are available via the <code>/config/uploads</code> sub-directory from within the container. Thereby, you can move data in between the container. The connection between the container image and NOMAD allows you to enrich your upload with specific post-processing results generated by any of the tools in the container. The paraprobe-toolbox exemplifies how NeXus can be used to document the specific settings and results of such processing via an open standard and documentation.</p> <p>The <code>apmtools</code> container comes with a collection of getting started tutorials via a <code>Cheatsheet</code> jupyter notebook which you can access from the <code>/home/atom_probe_tools</code> sub-directory. Details dependent on the specific configuration of your NOMAD OASIS.</p>"},{"location":"examples/experiment_data/em.html","title":"Domain-specific examples for electron microscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> <ul> <li>pynxtools-em</li> </ul>"},{"location":"examples/experiment_data/mpes.html","title":"Domain-specific examples for photoemission spectroscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p>"},{"location":"examples/experiment_data/nexus.html","title":"Standardized ingestion of data from materials characterization using NeXus","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> <ul> <li>FAIRmat NeXus extension proposal</li> <li>NeXusOntology</li> <li>NeXus</li> </ul>"},{"location":"examples/experiment_data/opt.html","title":"Domain-specific examples for optical spectroscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> <ul> <li>pynxtools-ellips</li> </ul>"},{"location":"examples/experiment_data/pynxtools.html","title":"Parse materials characterization data with pynxtools","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> <ul> <li>pynxtools</li> </ul>"},{"location":"examples/experiment_data/sts.html","title":"Domain-specific examples for STS / STM (scanning tunneling spectroscopy / microscopy)","text":"<p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p>"},{"location":"examples/experiment_data/sts.html#contextualization-for-the-technique-and-the-scientific-domain","title":"Contextualization for the technique and the scientific domain","text":"<p>A variety of file formats (coming technology instrumens) are used in the research field of scanning tunneling microscopy (STM) and scanning tunneling spectroscopy (STS) to investigate topological propertise of surface of subjected material. The pynxtools-stm plugin (note: The single plugin handles both STM as well STS techniques) of the pynxtools parsing library solves the challenges of how these formats can be parsed and normalized into a common representation that increases interoperability and adds semantic expressiveness.</p> <p>The pynxtools-stm provides a indispensable tool to transfor the STM as well as STS experimental data (sometime refered as raw data or machine data) to common standarized structure defined in NXsts (GitHub page) application definition build with the help of NeXus ontology (GitHub page). One of the main goals of such effort is to make the data comming from diverse sources comparable, searchable and shearable under the hood of NONAD research data management platform.</p> <p>For full benefits and usages of the reader please following links:</p> <ul> <li>Full Reader Documentation</li> <li>GitHub Repository</li> <li>Issue Tracker</li> </ul>"},{"location":"examples/experiment_data/sts.html#how-to-upload-xps-data-to-nomad","title":"How to upload XPS data to NOMAD","text":"<p>Documentation on how to upload STM / STS data sets from different sources can be found here</p>"},{"location":"examples/experiment_data/sts.html#supported-file-formats","title":"Supported file formats","text":"<p>A list of the supported file formats can be found in the <code>pynxtools-stm</code> documentation.</p>"},{"location":"examples/experiment_data/xps.html","title":"Domain-specific examples for X-ray photoelectron spectroscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p>"},{"location":"examples/experiment_data/xps.html#contextualization-for-the-technique-and-the-scientific-domain","title":"Contextualization for the technique and the scientific domain","text":"<p>A variety of file formats are used in the research field of X-ray photoelectron spectroscopy and related techniques. The <code>pynxtools-xps</code> plugin of the <code>pynxtools</code> parsing library solves the challenge of how these formats can be parsed and normalized into a common representation that increases interoperability and adds semantic expressiveness.</p> <p><code>pynxtools-xps</code>, which is a plugin for pynxtools, provides a tool for reading data from various propietary and open data formats from technology partners and the wider XPS community and standardizing it such that it is compliant with the NeXus application definitions <code>NXmpes</code> and <code>NXxps</code>, which is an extension of <code>NXmpes</code>.</p> <ul> <li>Documentation</li> <li>GitHub repository</li> <li>Issue tracker</li> </ul>"},{"location":"examples/experiment_data/xps.html#how-to-upload-xps-data-to-nomad","title":"How to upload XPS data to NOMAD","text":"<p>Documentation on how to upload XPS data sets from different sources can be found here</p>"},{"location":"examples/experiment_data/xps.html#supported-file-formats","title":"Supported file formats","text":"<p>A list of the supported file formats can be found in the <code>pynxtools-xps</code> documentation.</p>"},{"location":"explanation/architecture.html","title":"Architecture","text":""},{"location":"explanation/architecture.html#a-containerized-cloud-enabled-architecture","title":"A containerized cloud enabled architecture","text":"<p>NOMAD is a modern web-application that requires a lot of services to run. Some are NOMAD specific, others are 3rd party products. While all services can be traditionally installed and run on a single sever, NOMAD advocates the use of containers and operating NOMAD in a cloud environment.</p> <p> </p> NOMAD architecture <p>NOMAD comprises two main services, its app and the worker. The app services our API, graphical user interface, and documentation. It is the outward facing part of NOMAD. The worker runs all the processing (parsing, normalization). Their separation allows to scale the system for various use-cases.</p> <p>Other services are:</p> <ul> <li>rabbitmq: a task queue that we use to distribute tasks for the worker containers</li> <li>mongodb: a no-sql database used to maintain processing state and user-metadata</li> <li>elasticsearch: a no-sql database and search engine that drives our search</li> <li>a regular file system to maintain all the files (raw and archive)</li> <li>jupyterhub: run ai toolkit notebooks</li> <li>keycloak: our SSO user management system (can be used by all Oasises)</li> <li>a content management system to provide other web-page content (not part of the Oasis)</li> </ul> <p>All NOMAD software is bundled in a single NOMAD docker image and a Python package (nomad-lab on pypi). The NOMAD docker image can be downloaded from our public registry. NOMAD software is organized in multiple git repositories. We use continuous integration to constantly provide the latest version of docker image and Python package.</p>"},{"location":"explanation/architecture.html#nomad-uses-a-modern-and-rich-stack-frameworks-systems-and-libraries","title":"NOMAD uses a modern and rich stack frameworks, systems, and libraries","text":"<p>Besides various scientific computing, machine learning, and computational material science libraries (e.g. numpy, skikitlearn, tensorflow, ase, spglib, matid, and many more), Nomad uses a set of freely available technologies that already solve most of its processing, storage, availability, and scaling goals. The following is a non comprehensive overview of used languages, libraries, frameworks, and services.</p> <p> </p> NOMAD components and dependencies"},{"location":"explanation/architecture.html#python-3","title":"Python 3","text":"<p>The backend of nomad is written in Python. This includes all parsers, normalizers, and other data processing. We only use Python 3 and there is no compatibility with Python 2. Code is formatted close to pep8, critical parts use pep484 type-hints. ruff, and mypy (static type checker) are used to ensure quality. Tests are written with pytest. Logging is done with structlog and logstash (see Elasticstack below). Documentation is driven by Sphinx.</p>"},{"location":"explanation/architecture.html#celery","title":"celery","text":"<p>Celery (+ rabbitmq) is a popular combination for realizing long running tasks in internet applications. We use it to drive the processing of uploaded files. It allows us to transparently distribute processing load while keeping processing state available to inform the user.</p>"},{"location":"explanation/architecture.html#elastic-search","title":"elastic search","text":"<p>Elasticsearch is used to store repository data (not the raw files). Elasticsearch enables flexible, scalable search and analytics.</p>"},{"location":"explanation/architecture.html#mongodb","title":"mongodb","text":"<p>Mongodb is used to store and track the state of the processing of uploaded files and the generated entries. We use mongoengine to program with mongodb.</p>"},{"location":"explanation/architecture.html#keycloak","title":"Keycloak","text":"<p>Keycloak is used for user management. It manages users and provides functions for registration, forgetting passwords, editing user accounts, and single sign-on to fairdi@nomad and other related services.</p>"},{"location":"explanation/architecture.html#fastapi","title":"FastAPI","text":"<p>The ReSTful API is build with the FastAPI framework. This allows us to automatically derive a OpenAPI description of the nomad API. Fruthermore, you can browse and use the API via OpenAPI dashboard.</p>"},{"location":"explanation/architecture.html#elasticstack","title":"Elasticstack","text":"<p>The elastic stack (previously ELK stack) is a centralized logging, metrics, and monitoring solution that collects data within the cluster and provides a flexible analytics front end for that data.</p>"},{"location":"explanation/architecture.html#javascript-react-material-ui","title":"Javascript, React, Material-UI","text":"<p>The frontend (GUI) of nomad@FAIRDI is built on the React component framework. This allows us to build the GUI as a set of re-usable components to achieve a coherent representations for all aspects of nomad, while keeping development efforts manageable. React uses JSX (a ES6 variety) that allows to mix HTML with Javascript code. The component library Material-UI (based on Google's popular material design framework) provides a consistent look-and-feel.</p>"},{"location":"explanation/architecture.html#docker","title":"docker","text":"<p>To run a nomad@FAIRDI instance, many services have to be orchestrated: the nomad app, nomad worker, mongodb, Elasticsearch, Keycloak, RabbitMQ, Elasticstack (logging), the nomad GUI, and a reverse proxy to keep everything together. Further services might be needed (e.g. JypiterHUB), when nomad grows. The container platform Docker allows us to provide all services as pre-build images that can be run flexibly on all types of platforms, networks, and storage solutions. Docker-compose allows us to provide configuration to run the whole nomad stack on a single server node.</p>"},{"location":"explanation/architecture.html#kubernetes-helm","title":"kubernetes + helm","text":"<p>To run and scale nomad on a cluster, you can use kubernetes to orchestrated the  necessary containers. We provide a helm chart with all necessary service and deployment descriptors that allow you to set up and update nomad with only a few commands.</p>"},{"location":"explanation/architecture.html#gitlab","title":"GitLab","text":"<p>Nomad as a software project is managed via GitLab. The nomad@FAIRDI project is hosted here. GitLab is used to manage versions, different branches of development, tasks and issues, as a registry for Docker images, and CI/CD platform.</p>"},{"location":"explanation/basics.html","title":"From files to data","text":"<p>NOMAD is based on a bottom-up approach to data management. Instead of only supporting data in a specific predefined format, we process files to extract data from an extendable variety of data formats.</p> <p>Converting heterogenous files into homogeneous machine actionable processed data is the basis to make data FAIR. It allows us to build search interfaces, APIs, visualization, and analysis tools independent from specific file formats.</p> <p> </p> NOMAD's datamodel and processing"},{"location":"explanation/basics.html#uploads","title":"Uploads","text":"<p>Users create uploads to organize files. Think of an upload like a project: many files can be put into a single upload and an upload can be structured with directories. You can collaborate on uploads, share uploads, and publish uploads. The files in an upload are called raw files. Raw files are managed by users and they are never changed by NOMAD.</p> <p>Note</p> <p>As a rule, raw files are not changed during processing (or otherwise). However, to achieve certain functionality, a parser, normalizer, or schema developer might decide to bend this rule. Use-cases include the generation of more mainfiles (and entries) and updating of related mainfiles to automatize ELNs, or generating additional files to convert a mainfile into a standardized format like nexus or cif.</p>"},{"location":"explanation/basics.html#files","title":"Files","text":"<p>We already said that all uploaded files are raw files. Recognized files that have an entry are called mainfiles. Only the mainfile of the entry is passed to the parser during processing. However, a parser can call other tools or read other files. Therefore, we consider all files in the same directory of the mainfile as auxillary files, even though there is not necessarily a formal relationship with the entry. If formal relationships with aux files are established, e.g. via a reference to the file within the processed data, is up to the parser.</p>"},{"location":"explanation/basics.html#entries","title":"Entries","text":"<p>All uploaded raw files are analysed to find files with a recognized format. Each file that follows a recognized format is a mainfile. For each mainfile, NOMAD will create a database entry. The entry is eternally matched to the mainfile. The entry id, for example, is a hash over the upload id and the mainfile path (and an optional key) within the upload. This matching process is automatic, and users cannot create entries manually.</p> <p>Note</p> <p>We say that raw files are not changed by NOMAD and that users cannot create entries, but what about ELNs? There is a create entry button in the UI?</p> <p>However, NOMAD will simply create an editable mainfile that indirectly creates an entry. The user might use NOMAD as an editor to change the file, but the content is determined by the users. Contrary to the processed data that is created from raw files by NOMAD.</p>"},{"location":"explanation/basics.html#datasets","title":"Datasets","text":"<p>Users can build collections of entries to form datasets. You can imagine datasets like tags or albums in other systems. Each entry can be contain in many datasets and a dataset can hold many entries. Datasets can also overlap. Datasets are only indirectly related to files. The main purpose of datasets in NOMAD is to have citable collections of data. Users can get a DOI for their datasets. Datasets have no influence on the processing of data.</p>"},{"location":"explanation/basics.html#processing","title":"Processing","text":"<p>The processing of entries is automatic. Initially and on each mainfile change, the entry corresponding to the mainfile, will be processed. Processing consist of parsing, normalizing, and persisting the created data, as explained in more detail in the Processing section.</p>"},{"location":"explanation/basics.html#parsing","title":"Parsing","text":"<p>Parsers are small programs that transform data from a recognized mainfile into a structured machine processable tree of data that we call the archive or processed data of the entry. Only one parser is used for each entry. The used parser is determined during matching and depends on the file format. A dedicated guide shows how to match a specific file from your parser. Parsers can be added to NOMAD as plugins; this is a list of all built-in parsers.</p> <p>Note</p> <p>A special case is the parsing of NOMAD archive files. Usually a parser converts a file from a source format into NOMAD's archive format for processed data. But users can also create files following this format themselves. They can be uploaded either as <code>.json</code> or <code>.yaml</code> files by using the <code>.archive.json</code> or <code>.archive.yaml</code> extension. In these cases, we also considering these files as mainfiles and they are also going to be processed. Here the parsing is a simple syntax check and basically just copying the data, but normalization might still modify and augment the data substantially. One use-case for these archive files, are ELNs. Here the NOMAD UI acts as an editor for a respective <code>.json</code> file, but on each save, the corresponding file is going through all the regular processing steps. This allows ELN schema developers to add all kinds of functionality such as updating referenced entries, parsing linked files, or creating new entries for automation.</p>"},{"location":"explanation/basics.html#normalizing","title":"Normalizing","text":"<p>While parsing converts a mainfile into processed data, normalizing is only working on the processed data. Learn more about why to normalize in the documentation on structured data. There are two principle ways to implement normalization in NOMAD: normalizers and normalize functions.</p> <p>Normalizers are small programs that take processed data as input. There is a list of normalizers registered in the NOMAD configuration. In the future, normalizers might be added as plugins as well. They run in the configured order. Every normalizer is run on all entries and the normalizer might decide to do something or not, depending on what it sees in the processed data.</p> <p>Normalize functions are special functions implemented as part of section definitions in Python schemas. There is a special normalizer that will go through all processed data and execute these function if they are defined. Normalize functions get the respective section instance as input. This allows schema plugin developers to add normalizing to their sections. Read about our structured data to learn more about the different sections.</p>"},{"location":"explanation/basics.html#storing-and-indexing","title":"Storing and indexing","text":"<p>As a last technical step, the processed data is stored and some information is passed into the search index. The store for processed data is internal to NOMAD and processed data cannot be accessed directly and only via the archive API or ArchiveQuery Python library functionality. What information is stored in the search index is determined by the metadata and results sections and cannot be changed by users or plugins. However, all scalar values in the processed data are also index as key-values pairs.</p> <p>Attention</p> <p>This part of the documentation should be more substantiated. There will be a learn section about the search soon.</p>"},{"location":"explanation/data.html","title":"Data structure","text":"<p>NOMAD structures data into sections, where each section can contain data and more sections. This allows to browse complex data like you would browse files and directories on your computer. Each section follows a definition and all the contained data and subsection have a specific name, description, possible type, shape, and unit. This means that all data follows a schema. This not only helps the human exploration, but also makes it machine interpretable, increases consistency and interoperability, enables search, APIs, visualization, and analysis.</p> <p> </p> Browsing structured data in the NOMAD UI (link)"},{"location":"explanation/data.html#schema-language","title":"Schema language","text":"<p>The bases for structured data are schemas written in a schema language. Our schema language is called the NOMAD Metainfo language. The name is evocative of the rich metadata information that should be associated with the research data and made available in a machine-readable format. It defines the tools to define sections, organize definitions into packages, and define section properties (subsections and quantities).</p> <p> </p> The NOMAD Metainfo schema language for structured data definitions <p>Packages contain section definitions, section definitions contain definitions for subsections and quantities. Sections can inherit the properties of other sections. While subsections allow to define containment hierarchies for sections, quantities can use section definitions (or other quantity definitions) as a type to define references.</p> <p>If you are familiar with other schema languages and means to defined structured data (json schema, XML schema, pydantic, database schemas, ORM, etc.), you might recognize these concept under different names. Sections are similar to classes, concepts, entities, or  tables. Quantities are related to properties, attributes, slots, columns. subsections might be called containment or composition. subsections and quantities with a section type also define relationships, links, or references.</p> <p>Our guide on how to write a schema explains these concepts with an example.</p>"},{"location":"explanation/data.html#schema","title":"Schema","text":"<p>NOMAD represents many different types of data. Therefore, we cannot speak of just the one schema. The entirety of NOMAD schemas is called the NOMAD Metainfo. Definitions used in the NOMAD Metainfo fall into three different categories. First, we have sections that define a shared entry structure. Those are independent of the type of data (and processed file type). They allow to find all generic parts without any deeper understanding of the specific data. Second, we have definitions of re-usable base sections for shared common concepts and their properties. Specific schemas can use and extend these base sections. Base sections define a fixed interface or contract that can be used to build tools (e.g. search, visualizations, analysis) around them. Lastly, there are specific schemas. Those re-use base sections and complement the shared entry structure. They define specific data structures to represent specific types of data.</p> <p> </p>      The three different categories of NOMAD schema definitions"},{"location":"explanation/data.html#base-sections","title":"Base sections","text":"<p>Base section is a very loose category. In principle, every section definition can be inherited from or can be re-used in different contexts. There are some dedicated (or even abstract) base section definitions (mostly defined in the <code>nomad.datamodel.metainfo</code> package and sub-packages), but schema authors should not strictly limit themselves to these definitions. The goal is to re-use as much as possible and to not re-invent the same sections over and over again. Tools build around certain base section, provide an incentive to use them.</p> <p>Attention</p> <p>There is no detailed how-to or reference documentation on the existing base sections and how to use them yet.</p> <p>One example for re-usable base section is the workflow package. These allow to define workflows in a common way. They allow to place workflows in the shared entry structure, and the UI provides a card with workflow visualization and navigation for all entries that have a workflow inside.</p> <p>Attention</p> <p>Currently there are two version of the workflow schema. They are stored in two top-level <code>EntryArchive</code> subsections (<code>workflow</code> and <code>workflow2</code>). This will change soon to something that supports multiple workflows used in specific schemas and results.</p>"},{"location":"explanation/data.html#specific-schemas","title":"Specific schemas","text":"<p>Specific schemas allow users and plugin developers to describe their data in all detail. However, users (and machines) not familiar with the specifics, will struggle to interpret these kinda of data. Therefore, it is important to also translate (at least some of) the data into a more generic and standardized form.</p> <p> </p>      From specific data to more general interoperable data.    <p>The results section provides a shared structure designed around base section definitions. This allows you to put (at least some of) your data where it is easy to find, and in a form that is easy to interpret. Your non-interoperable, but highly detailed data needs to be transformed into an interoperable (but potentially limited) form.</p> <p>Typically, a parser will be responsible to populate the specific schema, and the interoperable schema parts (e.g. section results) are populated during normalization. This allows to separate certain aspects of conversions and potentially enables re-use for normalization routines. The necessary effort for normalization depends on how much the specific schema deviates from base-sections. There are three levels:</p> <ul> <li>the parser (or uploaded archive file) populates section results directly</li> <li>the specific schema re-uses the base sections used for the results and normalization can be automated</li> <li>the specific schema represents the same information differently and a translating normalization algorithm needs to be implemented.</li> </ul>"},{"location":"explanation/data.html#exploring-the-schema","title":"Exploring the schema","text":"<p>All built-in definitions that come with NOMAD or one of the installed plugins can be explored with the Metainfo browser. You can start with the root section <code>EntryArchive</code> and browse based on subsections, or explore the Metainfo through packages.</p> <p>To see all user provided uploaded schemas, you can use a search for the subsection <code>definition</code>. The subsection <code>definition</code> is a top-level <code>EntryArchive</code> subsection. See also our how-to on writing and uploading schemas.</p>"},{"location":"explanation/data.html#contributing-to-the-metainfo","title":"Contributing to the Metainfo","text":"<p>The shared entry structure (including section results) is part of the NOMAD source-code. It interacts with core functionality and needs to be highly controlled. Contributions here are only possible through merge requests.</p> <p>Base sections can be contributed via plugins. Here they can be explored in the Metainfo browser, your plugin can provide more tools, and you can make use of normalize functions. See also our how-to on writing schema packages. You could also provide base sections via uploaded schemas, but those are harder to explore and distribute to other NOMAD installations.</p> <p>Specific schemas can be provided via plugins or as uploaded schemas. When you upload schemas, you most likely also upload data in archive files (or use ELNs to edit such files). Here you can also provide schemas and data in the same file. In many case specific schemas will be small and only re-combine existing base sections. See also our how-to on writing YAML schemas.</p>"},{"location":"explanation/data.html#data","title":"Data","text":"<p>All processed data in NOMAD instantiates Metainfo schema definitions and the archive of each entry is always an instance of <code>EntryArchive</code>. This provides an abstract structure for all data. However, it is independent of the actual representation of data in computer memory or how it might be stored in a file or database.</p> <p>The Metainfo has many serialized forms. You can write <code>.archive.json</code> or <code>.archive.yaml</code> files yourself. NOMAD internally stores all processed data in message pack. Some of the data is stored in mongodb or elasticsearch. When you request processed data via API, you receive it in JSON. When you use the ArchiveQuery, all data is represented as Python objects (see also example in schema package documentation).</p> <p>No matter what the representation is, you can rely on the structure, names, types, shapes, and units defined in the schema to interpret the data.</p>"},{"location":"explanation/data.html#archive-files-a-shared-entry-structure","title":"Archive files: a shared entry structure","text":"<p>Broadening the discussion on the entry files that one can find in NOMAD, both schemas or processed data are serialized as the same kind of archive file, either <code>.archive.json</code> or <code>.archive.yaml</code>.</p> <p>The NOMAD archive file is indeed composed by several sections.</p> <p>NOMAD archive file:<code>EntryArchive</code></p> <ul> <li>definitions: <code>Definitions</code></li> <li>metadata: <code>EntryMetadata</code></li> <li>data: <code>EntryData</code></li> <li>run: <code>Run</code></li> <li>nexus: <code>Nexus</code></li> <li>workflow: <code>LegacyWorkflow</code></li> <li>workflow2: <code>Workflow</code></li> <li>results: <code>Results</code></li> </ul> <p>They all instantiate the same root section <code>EntryArchive</code>. They all share common sections <code>metadata:Metadata</code> and <code>results:Results</code>. They also all contain a data section, but the used section definition varies depending on the type of data of the specific entry. There is the literal <code>data:EntryData</code> subsection. Here <code>EntryData</code> is abstract and specific entries will use concrete definitions that inherit from <code>EntryData</code>. There are also specific data sections, like <code>run</code> for simulation data and <code>nexus</code> for nexus data.</p> <p>Note</p> <p>As shown in Uploading schemas, one can, in principle, create an archive file with both <code>definitions</code> and one of the data sections filled, although this is not always desired because it will stick together a schema and a particular instance of that schema. They should be kept separate so that it is still possible to generate new data files from the same schema file.</p> <p>Attention</p> <p>The results, originally only designed for computational data, will soon be revised an replaced by a different section. However, the necessity and function of a section like this remains.</p> <p> </p>      All entries instantiate the same section share the same structure."},{"location":"explanation/oasis.html","title":"Why you need an Oasis","text":"<p>The software that runs NOMAD is Open-Source and can be used independently of the NOMAD central installation at http://nomad-lab.eu. We call any NOMAD installation that is not the central one a NOMAD Oasis.</p> <p> </p> NOMAD Oasis use-cases <p>There are several use-cases how the NOMAD software could be used. Of course other uses and hybrids are imaginable:</p> <ul> <li>Academia: Use the Oasis for local management of unpublished research data</li> <li>Mirror: Use the Oasis as a mirror that hosts a copy of all published NOMAD data</li> <li>Industry: Use of Oasis to manage private data and full internal use of published data in compliance with strict privacy policies</li> <li>FAIRmat: Use Oasis to form a network of repositories to build a federated data infrastructure for materials science. This is what we do in the FAIRmat project.</li> </ul>"},{"location":"explanation/processing.html","title":"Processing","text":"<p>NOMAD extracts structured data from files via processing. Processing creates entries from files. It produces the schema-based structured data associated with each entry (i.e. the entry archives). To understand the role of processing in NOMAD also read the \"From file to data\" page.</p>"},{"location":"explanation/processing.html#processing-steps","title":"Processing steps","text":"<p>Processing comprises three steps.</p> <ol> <li>Matching files to parsers that can process them. This step also creates empty entries for matched files. Those matched files are now mainfiles and forever paired with the created entries.</li> <li>Parsing and normalizing to produce entry archives for matched mainfile/entry-pairs.</li> <li>Persisting (including indexing of) the extracted data.</li> </ol> <p> </p> Processing steps and how they interact with files, entries, and archives."},{"location":"explanation/processing.html#processing-triggers-scheduling-execution","title":"Processing triggers, scheduling, execution","text":"<p>For most end-users, processing is fully automated and will be automatically run when files are added or changed. Here, processing is triggered by the file upload API. However, in more advanced scenarios, other triggers might apply. The three possible triggers are:</p> <ul> <li>New files are uploaded or existing files are changed. This includes creating and updating ELN entries. The respective file upload API will call the processing.</li> <li>The processing of an upload is manually triggered, e.g. to re-process data. This might be done via the UI, the API, or CLI.</li> <li>The processing of one entry, programmatically triggers the processing of other files, e.g. in a <code>normalize</code> function.</li> </ul> <p>Most processing is done asynchronously. Entities that need processing are scheduled in a queue. Depending on the trigger, this might happen in the NOMAD app (i.a. via API), from a shell on a NOMAD server (i.e. via CLI), or from another processing in the NOMAD worker. Tasks in the queue are picked up by the NOMAD worker. See also the architecture documentation. The worker can process many entities from the queue concurrently.</p> <p>As an exception, entities can also be processed locally and synchronously. This means the processing is not done in the NOMAD worker, but where it is called. This is used for example, when an ELN is saved to have an immediate update on the respective entry with-in one API call.</p>"},{"location":"explanation/processing.html#processed-entities","title":"Processed entities","text":"<p>We differentiate two types of entities that can be processed: uploads and entries. The same entity can only be scheduled for processing, if it is not already scheduled or processing. See also the documentation \"from files to data\" to understand the relationships between all NOMAD entities.</p>"},{"location":"explanation/processing.html#uploads","title":"Uploads","text":"<p>Upload processing is scheduled if one or many files in the upload have changed. Upload processing includes the matching step, it creates new entries, and triggers the processing of new or afflicted entries. An upload is considered processing as long as any of its entries is still processing (or scheduled to be processed).</p>"},{"location":"explanation/processing.html#entries","title":"Entries","text":"<p>In most scenarios, entry processing is not triggered individually, but as part of an upload processing. Many entries of one upload might be processed at the same time. Some order can be enforced through processing levels. Levels are part of the parser metadata and entries paired to parsers with a higher level are processed after entries with a parser of lower level. See also how to write parsers.</p>"},{"location":"explanation/processing.html#customize-processing","title":"Customize processing","text":"<p>NOMAD provides just the framework for processing. The actual work depends on plugins, parsers and schemas for specific file types. While NOMAD comes with a build-in set of plugins, you can build your own plugins to support new file types, ELNs, and workflows.</p>"},{"location":"explanation/processing.html#schemas-parsers-plugins","title":"Schemas, parsers, plugins","text":"<p>The primary function of a parser is to systematically analyze and organize the incoming data, ensuring adherence to the established schema. The interaction between a parser and a schema plays a crucial role in ensuring data consistency to a predefined structure. It takes raw data inputs and utilizes the schema as a guide to interpret and organize the information correctly. By connecting the parser to the schema, users can establish a framework for the expected data structure. The modular nature of the parser and schema relationship allows for flexibility, as the parser can be designed to accommodate various schemas, making it adaptable to different data models or updates in research requirements. This process ensures that the resulting filled template meets the compliance standards dictated by the schema.</p> <p>Processing is run on the NOMAD (Oasis) server as part of the NOMAD app or worker. In principle, executed processing code can access all files, all databases, the underlying host system, etc. Therefore, only audited processing code can be allowed to run. For this reason, only code that is part of the NOMAD installation can be used and no user uploaded Python code is allowed.</p> <p>Therefore, the primary way to customize the processing and add more matching, parsing, and normalizing, is adding plugins to a NOMAD (Oasis) installation. However, NOMAD or NOMAD plugins can define base sections that use <code>normalize</code> functions that act on certain data or annotations. Uploaded <code>.yaml</code> schemas that use a such base section, might indirectly use custom processing functionality.</p> <p>A parser plugin can define a new parser and therefore add to the matching, parsing, (and normalizing). A schema plugin defines new sections that can contain <code>normalize</code> functions that add to the normalizing. See also the how-tos on plugins installation, and development of parsers and schemas.</p>"},{"location":"explanation/processing.html#matching","title":"Matching","text":"<p>All parsers have a <code>is_mainfile</code> function. This is its signature:</p> <pre><code>def is_mainfile(self, filename: str, ...) -&gt; Union[bool, Iterable[str]]\n</code></pre> <p>If this function does not return <code>False</code>, the parser matches with the file and entries are created. If the return is <code>True</code>, exactly one entry will be created. If the result is an iterable of strings, the same entry is sill created, but now also additional entries are created for each string. These strings are called entry keys and the additional entries are child entries. See the also single file, multiple entries scenario.</p> <p>In principle, the <code>is_mainfile</code> implementation can do whatever it wants: consider the filename, open the file, reading it partially, reading it whole, etc. However, most NOMAD parsers extend a specialized parser class called <code>MatchingParser</code>. These parsers share one implementation of <code>is_mainfile</code> that uses certain criteria, for example:</p> <ul> <li>regular expressions on filenames</li> <li>regular expressions on mimetypes</li> <li>regular expressions on header content</li> </ul> <p>See How to write a parser for more details.</p> <p>The matching step of an upload's processing, will call this function for every file and on all parsers. There are some hidden optimizations and additional parameters, but logically every parser is tested against every file, until a parser is matched or not. The first matched parser will be used and the order of configured parser is important. If no parser can be matched, the file is not considered for processing and no entry is created.</p>"},{"location":"explanation/processing.html#parsing","title":"Parsing","text":"<p>All parsers have a <code>parse</code> function. This is its signature:</p> <pre><code>def parse(self, mainfile: str, archive: EntryArchive, ...) -&gt; None\n</code></pre> <p>This function is called when a mainfile has already been matched and the entry has already been created with an empty archive. The entry mainfile and archive are passed as parameters.</p> <p>In the case of mainfile keys, an additional keyword argument is given: <code>child_archives: Dict[str, EntryArchive]</code>. Additional child entries have been created for each key, and this dictionary allows to populate the child archives.</p> <p>Each <code>EntryArchive</code> has an <code>m_context</code> field. This context provides functions to access the file system, open other files, open the archives of other entries, create or update files, spawn the processing of created or updated files. See also the create files, spawn entries scenario.</p>"},{"location":"explanation/processing.html#normalizing","title":"Normalizing","text":"<p>After parsing, entries are \"normalized\". We distinguish normalizers and <code>normalize</code> functions.</p> <p>Normalizers are Python classes. All normalizers are called for all archives. What normalizers are called and in what order is part of the NOMAD (Oasis) configuration. Normalizers have been developed to implement code independent processing steps that have to be applied to all entries in processing computational material science data. We are deprecating the use of normalizers and prefer the use of <code>normalize</code> functions.</p> <p>When you define a schema in Python, sections are defined as Python classes. These classes can have a function with the following signature:</p> <p><pre><code>def normalize(self, archive: EntryArchive, ...)\n</code></pre> These <code>normalize</code> functions can modify the given section instance <code>self</code> or the entry <code>archive</code> as a whole. Through the <code>archive</code>, Normalize functions also have access to the <code>context</code> and can do all kinds of file or processing operations from there. See also processing scenarios.</p> <p>Normalize functions are called in a particular order that follows a depth first traversal of the archives containment tree. If section definition classes inherit from each other, it is important to include respective <code>super</code> calls in the normalize function implementation.</p>"},{"location":"explanation/processing.html#processing-scenarios","title":"Processing scenarios","text":""},{"location":"explanation/processing.html#single-file-single-entry","title":"Single file, single entry","text":"<p>This is the \"normal\" case. A parser is matched to a mainfile. During processing, only the mainfile is read to populate the <code>EntryArchive</code> with data.</p>"},{"location":"explanation/processing.html#multiple-files-single-entry","title":"Multiple files, single entry","text":"<p>Same as above: a parser is matched to a mainfile. But, during processing, the mainfile and other files are read to populate the <code>EntryArchive</code> with data. Only the mainfile path is handed to the parser. Nothing, prevents it (or subsequent <code>normalize</code> functions) to open and read other auxiliary files.</p> <p>A notable special case are ELNs with <code>normalize</code> functions and references to files. ELNs can be designed to link the ELN with uploaded files via <code>FileEditQuantities</code> (see also How to define ELNs or ELN Annotations). The underlying ELN's schema usually defines <code>normalize</code> functions that open the referenced files for more data. Certain modes of the tabular parser, for example, use this.</p>"},{"location":"explanation/processing.html#single-file-multiple-entries","title":"Single file, multiple entries","text":"<p>A parser can match a mainfile in two ways. It's <code>is_mainfile</code> function can simply return <code>True</code>, meaning that the parser can parse this file. Or, it returns a list of keys. With the list of keys NOMAD will create an additional child entry for each key, and the parser's <code>parse</code> function is passed the archives of the additional child entries. One example is a parser for a tabular format that produces individual entries for each row of the table.</p> <p>Since <code>normalize</code> functions are defined as part of <code>Sections</code> in a schema and called for section instances, the <code>normalize</code> functions are called individually for the entry and all child entries.</p> <p>Normally the IDs of entries are computed from the upload ID and the mainfile path. For entries created from a mainfile and a key, the key is also included in the ID. Also here the entry identity is internally locked to the mainfile (and the respective key).</p>"},{"location":"explanation/processing.html#creating-files-spawning-entries","title":"Creating files, spawning entries","text":"<p>During processing, parsers or <code>normalize</code> functions can also create new (or update existing) files. And the processing might also trigger processing of these freshly created (or updated) files.</p> <p>For examples an ELN might be used to link multiple files of proprietary type. A <code>normalize</code> function can mix the data from those files with data from the ELN itself to convert everything into a file with a more standardized format. In experimental settings, this allows to merge proprietary instrument output and manual notes into a standardized representation of a measurement. The goal is not to fill an archive with all data directly, but have the created file parsed and create an entry with all data this way.</p> <p>Another use-case is automation. The <code>normalize</code> function of on ELN schema might use user input to create many more ELNs. This can be use-ful for parameter studies, where one experiment has to be repeated many times in a consistent way.</p>"},{"location":"explanation/processing.html#re-processing","title":"Re-processing","text":"<p>Typically a new file is uploaded, an entry created, and processed. But, we also need to re-processing entries because either the file was changed (e.g. an ELN was changed and saved or a new version of a file was uploaded), or because the parser or schema (including <code>normalize</code> functions) was changed. While the first case, usually triggers automated re-processing, the later case required manual intervention.</p> <p>Either the user manually re-processes an upload from the UI, because they know that they changed a respective schema for example, or the NOMAD (Oasis) admin re-processes (all) uploads, e.g. via the CLI, because they know the NOMAD version or some plugin has changed.</p> <p>Depending on configuration, re-processing might only process existing entries, match for new entries, or remove entries that no longer match.</p>"},{"location":"explanation/processing.html#strategies-for-re-usable-processing-code","title":"Strategies for re-usable processing code","text":"<p>Processing takes files as input and creates entries with structured data. It is about transforming data from one representation into another. Sometime, different input representations have to be transformed into the same output. Sometimes, the same input representation has to be transformed to different outputs. Input and output representation might evolve over time and processing has to be adopted. To avoid combinatorial explosion and manage moving sources and targets, it might be wise to modularize processing code more than just having <code>is_mainfile</code>, <code>parse</code>, and <code>normalize</code> functions.</p>"},{"location":"explanation/processing.html#readers-different-input-similar-output","title":"Readers: different input, similar output","text":"<p>Consider different file formats that capture similar data. Sometimes it is even the same container format (e.g. JSON or yaml), but slightly different keys, numbers are scaled differently, units are different, etc.</p> <p>We could write multiple parser, but the part of the parser that populates the archive, would be very much the same, only the part that reads information from the files, would be different.</p> <p>It might be good to define a reader like this: <pre><code>def reader(mainfile: str) -&gt; Dict[str, Any]\n</code></pre></p> <p>and capture data in a flexible intermediate format.</p>"},{"location":"explanation/processing.html#writers-metadata-into-archives-data-into-files","title":"Writers: metadata into archives, data into files","text":"<p>Two readers that produce a dictionary with the same keys and compatible values would allow to re-use the same writer that populates an archive, e.g.:</p> <pre><code>def writer(data: Dict[str, Any], archive: EntryArchive)\n</code></pre> <p>Similar to multiple readers, we might also use multiple writers. In certain scenarios, we need to write read data both into an archive, but also into an additional file. For example, if data is large, we can only keep metadata in archives, the actual data should be written to additional HFD5 or Nexus files.</p> <p>Separation of readers and writers also allows to re-use parser components in a <code>normalize</code> function. For example, if we consider the creating files, spawning entries scenario.</p>"},{"location":"explanation/processing.html#evolution-of-data-representations","title":"Evolution of data representations","text":"<p>If the input representation changes, only the readers have to change. If output representations change, only the writers have to be changed. If a new input is added, only a reader has to be developed. If a new output becomes necessary, only a new writer has to be developed.</p> <p>Since we use a flexible intermediate representation such as a <code>Dict[str, Any]</code>, it is easy to add new keys, but it is also dangerous because no static or semantic checking occurs. Readers and writers might be a good strategy to re-use parts of a parser, but reader and writer implementations remain strongly connected and inter-dependent.</p> <p> </p> Read and written data items might overlap but are rarely the same. <p>It is also important to note, that when using multiple readers and writers, the data items (i.e. keys) put into and read from the intermediate data structure might overlap, but are not necessarily the same sets of items. One reader might read more than another, the data put into an archive (metadata) might be different, from what is put into an HDF5 file (data).</p>"},{"location":"howto/overview.html","title":"NOMAD How-to guides","text":""},{"location":"howto/overview.html#users","title":"Users","text":"<p>These how-to guides target NOMAD users and cover data management, exploration, analysis with NOMAD graphical web-interface and APIs.</p>"},{"location":"howto/overview.html#manage-and-find-data","title":"Manage and find data","text":"<p>Use NOMAD to manage, explore, and analyze data.</p> <ul> <li>Upload and publish data for supported formats</li> <li>Use ELNs</li> <li>Explore data</li> <li>Use NORTH</li> </ul>"},{"location":"howto/overview.html#programmatic-use","title":"Programmatic use","text":"<p>Use NOMAD's functions programmatically and via its APIs.</p> <ul> <li>Use the API</li> <li>Publish data using python</li> <li>Install nomad-lab</li> <li>Access processed data</li> <li>Run a parser</li> </ul>"},{"location":"howto/overview.html#data-stewards-administrators-and-developers","title":"Data stewards, administrators, and developersOne last thing","text":"<p>These how-to guides allow advanced users, NOMAD administrators, data stewards, and developers to customize and operate NOMAD and NOMAD Oasis or contribute to NOMAD's development.</p> <p>If you can't find what you're looking for in our guides, contact our team for personalized help and assistance. Don't worry, we're here to help and learn what we're doing wrong!</p>"},{"location":"howto/overview.html#nomad-oasis","title":"NOMAD Oasis","text":"<p>Host NOMAD for your lab or institution.</p> <ul> <li>Install an Oasis</li> <li>Customize an Oasis</li> <li>Install plugins</li> <li>Migrate Oasis versions</li> <li>Administrate and maintain an Oasis</li> </ul>"},{"location":"howto/overview.html#customization","title":"Customization","text":"<p>Customize NOMAD, write plugins, and tailor NOMAD Oasis.</p> <ul> <li>Write a schema</li> <li>Define ELNs</li> <li>Use base sections</li> <li>Parse tabular data</li> <li>Define workflows</li> <li>Write plugins</li> <li>Write an app</li> <li>Write a normalizer</li> <li>Write a parser</li> <li>Write a schema packages</li> <li>Work with units</li> <li>Use HDF5 to handle large quantities</li> </ul>"},{"location":"howto/overview.html#development","title":"Development","text":"<p>Become a NOMAD developer and contribute to the source code.</p> <ul> <li>Get started</li> <li>Navigate the code</li> <li>Contribute</li> <li>Extend the search</li> </ul>"},{"location":"howto/customization/base_sections.html","title":"How to use base sections","text":"<p>As previously mentioned in How to write a schema, base sections can be used when writing custom schemas to inherit properties and functionality from already defined sections. Here we explain the properties and functionality of specific base sections and how they can be used.</p>"},{"location":"howto/customization/base_sections.html#datamodelmetainfobasesections","title":"<code>datamodel.metainfo.basesections</code>","text":"<p>This built-in nomad module contains a set of base sections based on an entity-activity model.</p> <p>Info</p> <p>In this part of the documentation we use UML Class diagrams to illustrate the inheritance, composition and association between the base sections. For more information on UML Class diagrams please see en.wikipedia.org/wiki/Class_diagram.</p> <p></p> <p>All the base sections defined in this model are abstract in the sense that they cannot be instantiated in NOMAD directly. Instead, the user is expected to implement these in their own schemas by inheriting a base section and <code>nomad.datamodel.EntryData</code>. Furthermore, it is strongly encouraged to use the most specialized section applicable.</p> <p>Example</p> <p>If the user is writing a schema for an instrument in their lab, they should ideally inherit from <code>Instrument</code> (and <code>EntryData</code>) rather than directly from <code>Entity</code> or <code>BaseSection</code>.</p> <p>All sections that are intended to eventually become entries in NOMAD by inheriting from the entity-activity base sections inherit from <code>BaseSection</code>. This section provides a set of global quantities that provides basic information about the entry. Theses are:</p> <ul> <li><code>name</code>: A short human readable and descriptive name.</li> <li><code>datetime</code>: The date and time associated with this section.</li> <li><code>lab_id</code>: An ID string that is unique at least for the lab that produced this data.</li> <li><code>description</code>: Any information that cannot be captured in the other fields.</li> </ul>"},{"location":"howto/customization/base_sections.html#entity","title":"<code>Entity</code>","text":"<p>Info</p> <p>By \"Entity\" we mean:</p> <p>\"An object that persists, endures, or continues to exist through time while maintaining its identity.\"</p> <p>See BFO_0000002 for semantic context.</p> <p>The <code>Entity</code> section is currently subclassed by <code>System</code>, <code>Collection</code> and <code>Instrument</code>.</p> <p></p>"},{"location":"howto/customization/base_sections.html#collection","title":"<code>Collection</code>","text":"<p>The <code>Collection</code> section should be inherited when attempting to group entities together.</p> <p></p> <p>Example</p> <p>The user wants to write a data schema for a batch of substrates. As this is grouping entities together, they should inherit from <code>Collection</code>.</p>"},{"location":"howto/customization/base_sections.html#entityreference","title":"<code>EntityReference</code>","text":"<p>The <code>EntityReference</code> section can be composed in any <code>Activity</code> (see <code>Activity</code> below) to provide a reference to an <code>Entity</code>. The section contains the following quantities:</p> <ul> <li><code>reference</code>: A reference to a NOMAD <code>Entity</code> entry.</li> <li><code>lab_id</code>: The readable identifier for the entity.</li> <li><code>name</code>: A short descriptive name for the role of this reference (inherited from <code>SectionReference</code>).</li> </ul> <p>The normalizer for the <code>EntityReference</code> class will attempt to fill the <code>reference</code> from the <code>lab_id</code> or vice versa.</p>"},{"location":"howto/customization/base_sections.html#instrument","title":"<code>Instrument</code>","text":"<p>The <code>Instrument</code> section should be inherited when describing any tools used for material creation or characterization.</p> <p></p>"},{"location":"howto/customization/base_sections.html#system","title":"<code>System</code>","text":"<p>The main <code>Entity</code> section is <code>System</code> which is intended to cover any material system from atomic to device scale. This section adds the property <code>elemental_composition</code> which is a repeating subsection of <code>ElementalComposition</code> sections. Each <code>elemental_composition</code> section keeps track of a single element and its atomic fraction within the system.</p> <p>There are two specializations of <code>System</code> which differentiates between the theoretical concept of a pure material, <code>PureSubstance</code>, and an actual physical material combining several pure substances, <code>CompositeSystem</code>.</p> <p></p>"},{"location":"howto/customization/base_sections.html#pubchempuresubstancesection","title":"<code>PubChemPureSubstanceSection</code>","text":"<p>This is a specialization of the <code>PureSubstanceSection</code> which will automatically search the PubChem database for additional information about the substance. If a PubChem CID is specified the details are retrieved directly. Otherwise a search query is made for the filled attributes in the following order:</p> <ol> <li><code>smile</code></li> <li><code>canonical_smile</code></li> <li><code>inchi_key</code></li> <li><code>iupac_name</code></li> <li><code>name</code></li> <li><code>molecular_formula</code></li> <li><code>cas_number</code></li> </ol>"},{"location":"howto/customization/base_sections.html#activity","title":"<code>Activity</code>","text":"<p>Info</p> <p>By \"Activity\" we mean:</p> <p>\"An action that has a temporal extension and for some time depends on some entity.\"</p> <p>See BFO_0000015 for semantic context.</p> <p>The <code>Activity</code> section is currently subclassed by <code>Process</code>, <code>Measurement</code>, <code>Analysis</code>, and <code>Experiment</code>. These subclasses are intended to cover all types of activities and should be used instead of inheriting directly from <code>Activity</code>.</p> <p></p>"},{"location":"howto/customization/base_sections.html#experiment","title":"<code>Experiment</code>","text":"<p>The <code>Experiment</code> section should be inherited when attempting to group activities together.</p> <p></p> <p>Example</p> <p>In a sample centric view the activities are grouped together by the sample but if the researcher is instead interested in an experiment containing activities on multiple samples, the <code>Experiment</code> section can be inherited to group these together.</p>"},{"location":"howto/customization/base_sections.html#process","title":"<code>Process</code>","text":"<p>Info</p> <p>By \"Process\" we mean:</p> <p>\"A planned process which results in physical changes in a specified input material. [ obi : prs obi : mc obi : fg obi : jf obi : bp ]</p> <p>Synonyms:</p> <ul> <li>preparative method</li> <li>sample preparation</li> <li>sample preparative method</li> <li>material transformations\"</li> </ul> <p>See OBI_0000094 for semantic context.</p> <p>The <code>Process</code> section is the base for the <code>SynthesisMethod</code> section which in turn is specialized further in the <code>nomad-material-processing</code> plugin detailed below. The main feature of the <code>Process</code> section is that it adds <code>ProcessSteps</code> with a duration.</p> <p></p> <p>Info</p> <p>By \"SynthesisMethod\" we mean:</p> <p>\"A method used to synthesise a sample.\"</p> <p>See CHMO_0001301 for semantic context.</p>"},{"location":"howto/customization/base_sections.html#measurement","title":"<code>Measurement</code>","text":"<p>Info</p> <p>By \"Measurement\" we mean:</p> <p>\"A planned process with the objective to produce information about the material entity that is the evaluant, by physically examining it or its proxies. [ obi : pppb ]\"</p> <p>See OBI_0000070 for semantic context.</p> <p>The <code>Measurement</code> section adds <code>samples</code> which are references to instances of (subclasses of) <code>CompositeSystem</code>.</p> <p></p>"},{"location":"howto/customization/base_sections.html#analysis","title":"<code>Analysis</code>","text":"<p>Info</p> <p>By \"Analysis\" we mean:</p> <p>\"A planned process that produces output data from input data.</p> <p>Synonyms:</p> <ul> <li>data processing</li> <li>data analysis\"</li> </ul> <p>See OBI_0200000 for semantic context.</p> <p>The <code>Analysis</code> section provides <code>inputs</code> which are references to any section (including sub sections) of some archive. In addition, it provides the <code>outputs</code> which is a repeating section of <code>AnalysisResult</code> which are intended to be further specialized by the user.</p> <p></p>"},{"location":"howto/customization/base_sections.html#readableidentifiers","title":"<code>ReadableIdentifiers</code>","text":"<p>This base sub section is meant to be composed into the entity-activity sections mentioned above to provide a standardized readable identifier.</p> <p>It is in turn composed by the following quantities:</p> <ul> <li><code>institute</code>: Alias/short name of the home institute of the owner, i.e. HZB.</li> <li><code>owner</code>: Alias for the owner of the identified thing. This should be unique within the institute.</li> <li><code>datetime</code>: A datetime associated with the identified thing. In case of an <code>Activity</code>, this should be the starting time and, in case of an <code>Entity</code>, the creation time.</li> <li><code>short_name</code>: A short name of the the identified thing (e.g. the identifier scribed on the sample, the process number, or machine name), e.g. 4001-8, YAG-2-34. This is to be managed and decided internally by the labs, although we recommend to avoid the following characters in it: \"_\", \"/\", \"\\\" and \".\".</li> <li><code>lab_id</code>: Full readable id. Ideally a human readable id convention, which is simple, understandable and still have chances of becoming unique. If the <code>owner</code>, <code>short_name</code>, <code>\u00ecnstitute</code>, and <code>datetime</code> are provided, this will be formed automatically by joining these components by an underscore (_). Spaces in any of the individual components will be replaced with hyphens (-). An example would be hzb_oah_20200602_4001-08.</li> </ul> <p>If owner is not filled the field will be filled by the first two letters of the first name joined with the first two letters of the last name of the author. If the institute is not filled a institute abreviations will be constructed from the author's affiliation. If no datetime is filled, the datetime will be taken from the <code>datetime</code> property of the parent, if it exists, otherwise the current date and time will be used. If no short name is filled, the name will be taken from the parent name, if it exists, otherwise it will be taken from the archive metadata entry name, if it exists, and finally if no other options are available it will use the name of the mainfile.</p> <p>Example</p> <p>The user has created a sample section by inheriting from <code>CompositeSystem</code> and <code>EntryData</code>. Now, the user wants to automatically generate a readable <code>lab_id</code> based on the logged in author. This can be accomplished by composing the <code>ReadableIdentifiers</code> section into the users sample section:</p> <pre><code>class MySample(CompositeSystem, EntryData):\n    '''\n    A custom sample section.\n    '''\n    m_def = Section(\n        a_template=dict(\n            sample_identifiers=dict(),\n        ),\n    )\n    sample_identifiers = SubSection(\n        section_def=ReadableIdentifiers,\n    )\n</code></pre>"},{"location":"howto/customization/base_sections.html#plugin-nomad-material-processing","title":"Plugin: <code>nomad-material-processing</code>","text":"<p>This plugin contains more specialized base sections for material processing, is maintained by FAIRmat and is currently hosted on https://github.com/FAIRmat-NFDI.</p>"},{"location":"howto/customization/basics.html","title":"How to write a YAML schema package","text":"<p>This guide explains how to write and upload NOMAD schema packages in the YAML format that can be uploaded as part of your data. This is a good way to start out experimenting with custom data structures in NOMAD, but for more advanced use cases you may need to use Python schema packages. For more information on how an archive file is composed, visit Explanation &gt; Data structure.</p>"},{"location":"howto/customization/basics.html#example-data","title":"Example data","text":"<p>Let's assume we want to describe chemical compositions using the elements they contain. The following structured data (in this example as a <code>.yaml</code> document) could describe the composition of water. <pre><code>composition: H2O\nelements:\n- label: H\n  density: 8.375e-05\n  isotopes: [1, 2, 3]\n- label: O\n  density: 1.141\n  isotopes: [16, 17, 18]\n</code></pre></p> <p>In structured data formats (such as <code>.yaml</code> or <code>.json</code>), data is put into combinations of primitive values (e.g. <code>'H2O'</code>, <code>1.141</code>), objects (a set of keys and value pairs, where values can be objects, lists, or primitive values), and lists of values.</p>"},{"location":"howto/customization/basics.html#sections","title":"Sections","text":"<p>In a schema package, we want to describe the structure of data, i.e. what are the allowed combinations of objects, lists, and primitive values. The crucial task here is to define what keys certain types of objects can have and what possible values might exist for each of these keys.</p> <p>In NOMAD, we call objects sections and we define types of objects with section definitions. Since objects can be nested, sections become like the sections and subsections of a book or paper. Sections are a representation of data and they are the building blocks for archives. Section definitions form a schema package and they are the building blocks for the metainfo.</p> <p>In the above example, we have two types of objects: an overaching object for the entire structure (with keys for <code>composition</code> and <code>elements</code>), and an additional object which describes the internal structure of <code>elements</code> (with keys for <code>label</code>, <code>density</code>, and <code>isotopes</code>). Let's start with the definition for elements. This is what the section definition looks like in NOMAD's yaml-based schema package format:</p> <pre><code>Element:\n  quantities:\n    label:\n      type: str\n    density:\n      type: np.float64\n      unit: g/cm**3\n    isotopes:\n      type: int\n      shape: ['*']\n</code></pre> <p>A section definition provides all the available keys for a section that instantiates this definition. For each key, e.g. <code>label</code>, <code>density</code>, <code>isotopes</code>, it provides more information on the possible values.</p> <p>Let's have a look at the overall definition for our chemical composition:</p> <pre><code>Composition:\n  quantities:\n    composition:\n      type: str\n  sub_sections:\n    elements:\n      section: Element\n      repeats: true\n</code></pre> <p>Again, all possible keys (<code>composition</code> and <code>elements</code>) are defined. But now we see that there are two different types of keys, quantities and subsections. We say that section definitions can have properties (e.g. the keys they define) and there are two distinct types of properties.</p>"},{"location":"howto/customization/basics.html#quantities","title":"Quantities","text":"<p>Quantities define possible primitive values. The basic properties that go into a quantity definition are:</p> <ul> <li>type: what kind of primitive value can be used, e.g. <code>str</code> or <code>np.float64</code></li> <li>shape: what is the shape of the value, e.g. scalar or list (<code>['*']</code>)</li> <li>unit: what is the physical meaning of the value</li> </ul> <p>The names of quantity definitions serve as the key, used in respective section objects.</p>"},{"location":"howto/customization/basics.html#type","title":"Type","text":"<p>This is a list of supported quantity types.</p> type description <code>string</code> <code>str</code> <code>float</code> <code>integer</code> <code>int</code> <code>boolean</code> <code>bool</code> <code>np.int32</code> Numpy based integer with 32 bits. <code>np.int64</code> Numpy based integer with 64 bits. <code>np.float32</code> Numpy based float with 32 bits. <code>np.float64</code> Numpy based float with 64 bits. <code>Datetime</code> <code>User</code> A type for NOMAD users as values. <code>Author</code> A complex type for author information. <code>{type_kind: Enum, type_data: []}</code> Use <code>type_data</code> to specify enum values as list of strings. To define a quantity that is a reference to a specific section."},{"location":"howto/customization/basics.html#shape","title":"Shape","text":"<p>The shape of a quantity is a list of dimensions, where each dimension defines the possible size of that dimension. The empty list (or no shape) describes a scalar value, a list with one dimension a list or vector, a list with two dimensions a matrix, etc.</p> <p>Dimensions can be given as:</p> <ul> <li>an integer number to define a fixed size, e.g. a 3x3 matrix would have shape <code>[3, 3]</code>.</li> <li>the string <code>'*'</code> to denote am arbitrary sized dimension, e.g. a list quantity would have shape <code>['*']</code>.</li> <li>A string that describes the name of a sibling quantity with an integer type, e.g. <code>['number_of_atoms', 3]</code></li> </ul>"},{"location":"howto/customization/basics.html#unit","title":"Unit","text":"<p>NOMAD manages units and data with units via the Pint Python package. A unit is given as a string that is parsed by pint. These strings can be simple units (or their aliases) or complex expressions. Here are a few examples: <code>m</code>, <code>meter</code>, <code>mm</code>, <code>millimeter</code>, <code>m/s</code>, <code>m/s**2</code>.</p> <p>While you can use all kinds of units in your uploaded schema packages, the built-in NOMAD schema (Metainfo) uses only SI units.</p>"},{"location":"howto/customization/basics.html#subsections","title":"Subsections","text":"<p>Subsections define a part-of-relationship between two sections. Subsection definitions are properties of the parent section definition and name a child section definition. In the data, we can now contain instances of the target (e.g. <code>Element</code>) in instances of the source (e.g. <code>Composition</code>). A subsection can be defined as repeating to allow many child sections of the same type. In our example, one <code>Composition</code> can contain many <code>Elements</code>.</p> <p>The names of subsection definitions serve as the key, used in respective section objects.</p>"},{"location":"howto/customization/basics.html#uploading-schema-packages","title":"Uploading schema packages","text":"<p>NOMAD archive files allow you to upload data in NOMAD's native file format. An archive file can be a .yaml or .json file. It ends with <code>.archive.json</code> or <code>.archive.yaml</code>. Archive files are mainly used to convey data. Since YAML schema packages are also \"just\" data, archive files can also be used to convey a schema package.</p> <p>You can upload schema packages and data in separate files. <code>schema_package.archive.yaml</code> <pre><code>definitions:\n  sections:\n    Element:\n      quantities:\n        label:\n          type: str\n        density:\n          type: np.float64\n          unit: g/cm**3\n        isotopes:\n          type: int\n          shape: ['*']\n    Composition:\n      quantities:\n        composition:\n          type: str\n      sub_sections:\n        elements:\n          section: Element\n          repeats: true\n</code></pre></p> <p>and <code>data.archive.yaml</code> <pre><code>data:\n  m_def: '../upload/raw/package.archive.yaml#Composition'\n  composition: 'H2O'\n  elements:\n    - label: H\n      density: 0.00008375\n      isotopes: [1, 2, 3]\n    - label: O\n      density: 1.141\n      isotopes: [16, 17, 18]\n</code></pre></p> <p>Or, you can upload the schema package and data in the same file: <pre><code>definitions:\n  sections:\n    Element:\n      quantities:\n        label:\n          type: str\n        density:\n          type: np.float64\n          unit: g/cm**3\n        isotopes:\n          type: int\n          shape: ['*']\n    Composition:\n      quantities:\n        composition:\n          type: str\n      sub_sections:\n        elements:\n          section: Element\n          repeats: true\ndata:\n  m_def: Composition\n  composition: H2O\n  elements:\n  - label: H\n    density: 8.375e-05\n    isotopes: [1, 2, 3]\n  - label: O\n    density: 1.141\n    isotopes: [16, 17, 18]\n</code></pre></p>"},{"location":"howto/customization/basics.html#references","title":"References","text":""},{"location":"howto/customization/basics.html#reference-quantities","title":"Reference quantities","text":"<p>We already saw that we can define a part-of relationship between sections. When we want to represent highly inter-linked data, this is often insufficient. References allow us to create a more lose relationship between sections.</p> <p>A reference is a uni-directional link between a source section and a target section. References can be defined in a schema package as a quantity in the source section definition that uses the target section definition as a type.</p> <p>Instead of connecting the elements in a composition with subsections, we can also connect a composition section to elements with a quantity:</p> <pre><code>Composition:\n  quantities:\n    composition:\n      type: str\n    elements:\n      type: Element\n      shape: ['*']\n</code></pre> <p>Here, <code>type: Element</code> refers to the section definition <code>Element</code>, very similar to <code>section: Element</code> in a subsection definition.</p> <p>We saw above that subsections are represented as nested objects in data (forcing a part-of relationship). References are represented as string-typed primitive values in serialized data. Here is an example <code>Composition</code> with references to elements:</p> <pre><code>composition: H2O\nelements: ['#/data/periodic_table/elements/0', '#/data/periodic_table/elements/1']\n</code></pre> <p>These string-references determine the target section's place in the same archive. Each <code>/</code>-separated segment represents a key. A reference starts from the root object and following the sequence of keys to a specific object (i.e. section). Here is the full archive data:</p> <pre><code>data:\n periodic_table:\n   elements:\n   - label: H\n     density: 8.375e-05\n     isotopes: [1, 2, 3]\n   - label: O\n     density: 1.141\n     isotopes: [16, 17, 18]\n compositions:\n - composition: H2O\n   elements: ['#/data/periodic_table/elements/0', '#/data/periodic_table/elements/1']\n</code></pre> <p>If you follow the keys <code>data</code>, <code>periodic_table</code>, <code>elements</code>, <code>0</code>, you reach the section that represent hydrogen. Keep in mind that lists use index-numbers as keys.</p>"},{"location":"howto/customization/basics.html#schema-package-references","title":"Schema package references","text":"<p>References can look different depending on the context. Above we saw simple references that point from one data section to another. But, you also already a saw a different type of reference. Schema packages themselves contain references: when we used <code>type: Element</code> or <code>section: Element</code> to refer to a section definition, we were writing down references that point to a section definition. Here we can use a convenience representation: <code>Element</code> simply replaces the otherwise cryptic <code>#/definitions/sections/0</code>.</p> <p>So far, we never discussed the use of <code>m_def</code>. In the examples you might have seen this as a special key in some objects. Whenever we cannot determine the section definition for a section by its context (e.g. the key/subsection used to contain it in a parent section), we use <code>m_def</code> to provide a reference to the section definition.</p>"},{"location":"howto/customization/basics.html#different-forms-of-references","title":"Different forms of references","text":"<p>Depending on where references are used, they might take a different serialized form. Here are a few examples for different reference syntax:</p> Example reference Comments <code>#/data/periodic_table/elements/0</code> Reference to a section within the subsection hierarchy of the same archive. <code>Element</code> Reference to a section definition in the same archive. Can only be used to target section definitions. <code>nomad.datamodel.metainfo.workflow</code> Reference to a section definition that was written in Python and is part of the NOMAD code. Can only be used to target section definitions. <code>../upload/raw/data.archive.yaml#/data</code> Reference to a section in a different <code>.archive.yaml</code> file of the same upload. <code>../upload/archive/mainfile/data.archive.yaml#/data</code> Reference to a section in a processed archive given by entry mainfile. <code>../upload/archive/zxhS43h2kqHsVDqMboiP9cULrS_v#/data</code> Reference to a section in a processed archive given by entry-id. <code>../uploads/zxhS43h2kqHsVDqMboiP9cULrS_v/raw/data.archive.yaml#/data</code> Reference to a section in an entry of a different upload. <code>https://mylab.eu/oasis/api/v1/uploads/zxhS43h2kqHsVDqMboiP9cULrS_v/raw/data.archive.yaml#/data</code> Reference to a section in an entry in a different NOMAD installation."},{"location":"howto/customization/basics.html#references-across-entries","title":"References across entries","text":"<p>A references in the archive of one entry can point to a section in a different entry's archive. The following two example files, exemplify this use of reference between two NOMAD entries.</p> <p>periodic_table.archive.yaml <pre><code>definitions:\n  sections:\n    Element:\n      quantities:\n        label:\n          type: str\n        density:\n          type: np.float64\n          unit: g/cm**3\n        isotopes:\n          type: int\n          shape: ['*']\n    PeriodicTable:\n      sub_sections:\n        elements:\n          repeats: true\n          section: Element\ndata:\n  m_def: PeriodicTable\n  elements:\n  - label: H\n    density: 0.00008375\n    isotopes: [1, 2, 3]\n  - label: O\n    density: 1.141\n    isotopes: [16, 17, 18]\n</code></pre></p> <p>composition.archive.yaml <pre><code>definitions:\n  sections:\n    Composition:\n      quantities:\n        composition:\n          type: str\n        elements:\n          type: ../upload/raw/periodic_table.archive.yaml#Element\n          shape: ['*']\ndata:\n  m_def: Composition\n  composition: 'H2O'\n  elements:\n    - ../upload/raw/periodic_table.archive.yaml#data/elements/0\n    - ../upload/raw/periodic_table.archive.yaml#data/elements/1\n</code></pre></p> <p>These inter-entry references have two parts: <code>&lt;entry&gt;#&lt;section&gt;</code>, where entry is a path or URL denoting the target entry and section a path within the target entry's subsection containment hierarchy.</p> <p>Please note that also schema packages can be spread over multiple files. In the above example, one file contained the schema package and data for a periodic table and another file contained schema package and data for the composition of water (using the periodic table).</p>"},{"location":"howto/customization/basics.html#base-sections-and-inheritance","title":"Base sections and inheritance","text":"<p>We add a relationship between section definitions that allows us to create more specialized definitions from more abstract definitions. Here the properties of the abstract definition are inherited by the more specialized definitions</p>"},{"location":"howto/customization/basics.html#base-sections","title":"Base sections","text":"<p>Here is a simple schema package with two specialization of the same abstract section definition: <pre><code>definitions:\n  sections:\n    Process:\n      quantities:\n        time:\n          type: Datetime\n    Evaporation:\n      base_section: Process\n      quantities:\n        pressure:\n          type: np.float64\n          unit: Pa\n    Annealing:\n      base_section: Process\n      quantities:\n        temperature:\n          type: np.float64\n          unit: K\n</code></pre></p> <p>The two specialized definitions <code>Annealing</code> and <code>Evaporation</code> define the abstract definition <code>Process</code> via the <code>base_section</code> property. With this <code>Annealing</code> and <code>Evaporation</code> inherit the quantity <code>time</code>. We do not need to repeat quantities from the base section, and we can add more properties. Here is an example <code>Evaporation</code> using both the inherited and added quantity:</p> <pre><code>data:\n  m_def: Evaporation\n  time: '2022-10-13 12:00:00'\n  pressure: 100\n</code></pre>"},{"location":"howto/customization/basics.html#polymorphy","title":"Polymorphy","text":"<p>What happens if we reference abstract definitions in subsections or reference quantities? Here is an subsection example. In one schema, we define the relationship between <code>Sample</code> and <code>Process</code>. In another schema, we want to add more specializations to what a process is.</p> <p>abstract.archive.yaml <pre><code>definitions:\n  sections:\n    Process:\n      quantities:\n        time:\n          type: Datetime\n    Sample:\n      sub_sections:\n        processes:\n          section: Process\n          repeats: true\n</code></pre></p> <p>specialized.archive.yaml <pre><code>definitions:\n  sections:\n    Evaporation:\n      base_section: ../upload/raw/abstract.archive.yaml#Process\n      quantities:\n        pressure:\n          type: np.float64\n          unit: Pa\n    Annealing:\n      base_section: ../upload/raw/abstract.archive.yaml#Process\n      quantities:\n        temperature:\n          type: np.float64\n          unit: K\n</code></pre></p> <p>The section definition use in the subsection <code>processes</code> defines what a contained section has to be \"at least\". Meaning that any section based on a specialization of <code>Process</code> would be a valid <code>processes</code> subsection.</p> <p>specialized.archive.yaml <pre><code>definitions:\n  # see above\ndata:\n  m_def: ../upload/raw/abstract.archive.yaml#Sample\n  processes:\n  - m_def: Evaporation\n    time: '2022-10-13'\n    pressure: 100\n  - m_def: Annealing\n    time: '2022-10-13'\n    temperature: 342\n</code></pre></p> <p>The fact that a subsection or reference target can have different \"forms\" (i.e. based on different specializations) is called polymorphism in object-oriented data modelling.</p>"},{"location":"howto/customization/basics.html#pre-defined-sections","title":"Pre-defined sections","text":"<p>NOMAD provides a series of built-in section definitions. For example, there is <code>EntryArchive</code>, a definition for the top-level object in all NOMAD archives (e.g. <code>.archive.yaml</code> files). Here is a simplified except of the main NOMAD schema <code>nomad.datamodel</code>:</p> <pre><code>EntryArchive:\n  sub_sections:\n    metadata:\n      section: EntryMetadata\n    definitions:\n      section: nomad.metainfo.Package\n    data:\n      section: EntryData\n    # ... many more\nEntryData:\n  # empty\n</code></pre> <p>Compare this to the previous examples: we used the top-level keys <code>definitions</code> and <code>data</code> without really explaining why. Here you can see why. The <code>EntryArchive</code> property <code>definitions</code> allows us to put a schema package into our archives. And the <code>EntryArchive</code> property <code>data</code> allows us to put data into archives that is a specialization of <code>Schema</code>. The <code>Schema</code> definition is empty. It is merely an abstract placeholder that allows you to add specialized data sections to your archive. Therefore, all section definitions that define a top-level data section, should correctly use <code>nomad.datamodel.Schema</code> as a base section. This would be the first \"correct\" example:</p> <pre><code>definitions:\n  sections:\n    Greetings:\n      base_section: nomad.datamodel.EntryData\n      quantities:\n        message:\n          type: str\ndata:\n  m_def: MyData\n  message: Hello World\n</code></pre> <p>Here are a few other built-in section definitions and packages of definitions:</p> Section definition or package Purpose nomad.datamodel.EntryArchive Used for the root object of all NOMAD entries nomad.datamodel.EntryMetadata Used to add standard NOMAD metadata such as ids, upload, processing, or author information to entries. nomad.datamodel.EntryData An abstract section definition for the <code>data</code> section. nomad.datamodel.ArchiveSection Allows to put <code>normalize</code> functions into your section definitions. nomad.datamodel.metainfo.eln.* A package of section definitions to inherit commonly used quantities for ELNs. These quantities are indexed and allow specialization to utilize the NOMAD search. nomad.parsing.tabular.TableData Allows to inherit parsing of references .csv and .xls files. nomad.datamodel.metainfo.workflow.* A package of section definitions use by NOMAD to define workflows nomad.metainfo.* A package that contains all definitions of definitions, e.g. NOMAD's \"schema language\". Here you find definitions for what a sections, quantity, subsections, etc. is."},{"location":"howto/customization/basics.html#separating-data-and-schema-package","title":"Separating data and schema package","text":"<p>As we saw above, a NOMAD entry can contain schema package <code>definitions</code> and <code>data</code> at the same time. To organize your schema package and data efficiently, it is often necessary to re-use schema packages and certain data in other entries. You can use references to spread your schema packages and data over multiple entries and connect the pieces via references.</p> <p>Here is a simple schema package, stored in a NOMAD entry with mainfile name <code>package.archive.yaml</code>:</p> <pre><code>definitions:\n  sections:\n    Composition:\n      quantities:\n        composition:\n          type: str\n        base_composition:\n          type: Composition\n      sub_sections:\n        elements:\n          section: Element\n          repeats: True\n    Element:\n      quantities:\n        label:\n          type: str\n    Solution:\n      quantities:\n        solvent:\n          type: Composition\n      sub_sections:\n        solute:\n          section: Composition\n</code></pre> <p>Now, we can re-use this schema package in many entries via references. Here, we extend a schema contained in the package and instantiate definitions is a separate mainfile <code>data-and-package.archive.yaml</code>:</p> <pre><code>definitions:\n  sections:\n    SpecialElement:\n      # Extending the definition from another entry\n      base_section: '../upload/raw/package.archive.yaml#Element'\n      quantities:\n        atomic_weight:\n          type: float\n          unit: 'g/mol'\ndata:\n  # Instantiating the definition from another entry\n  m_def: '../upload/raw/package.archive.yaml#Composition'\n  composition: 'H2O'\n  elements:\n    # Implicitly instantiate Element as defined for Composition.elements\n    - label: H\n    # Explicitly instantiate SpecialElement as a polymorph substitute\n    - m_def: SpecialElement\n      label: O\n      atomic_weight: 15.9994\n</code></pre> <p>Here is a last example that re-uses the schema and references data from the two entries above:</p> <pre><code>definitions:\n  sections:\n    Composition:\n      quantities:\n        composition:\n          type: str\n        base_composition:\n          type: Composition\n      sub_sections:\n        elements:\n          section: Element\n          repeats: True\n    Element:\n      quantities:\n        label:\n          type: str\n    Solution:\n      quantities:\n        solvent:\n          type: Composition\n      sub_sections:\n        solute:\n          section: Composition\n</code></pre> <p>Attention</p> <p>You cannot create definitions that lead to circular loading of <code>*.archive.yaml</code> files. Each <code>definitions</code> section in an NOMAD entry represents a schema package. Each schema package needs to be fully loaded and analyzed before it can be used by other schema packages in other entries. Therefore, two schema packages in two entries cannot reference each other.</p>"},{"location":"howto/customization/basics.html#conventions","title":"Conventions","text":""},{"location":"howto/customization/basics.html#conventions-for-labels","title":"Conventions for labels","text":"<p>When assigning labels within your codebase, it's essential to follow consistent naming conventions for clarity and maintainability. The following guidelines outline the conventions for labeling different elements:</p> <ul> <li> <p>Sections: Labels for sections should adhere to Python convention of CapitalizedCamelCase. This means that each word in the label should begin with a capital letter, and there should be no spaces between words. For example: <code>SectionLabelOne</code>, <code>SectionLabelTwo</code>.</p> </li> <li> <p>Quantities and Subsections: Labels for quantities and subsections should be in lower_case. This convention involves writing all lowercase letters and separating words with whitespace. Abbreviations within these labels may be capitalized to enhance scientific readability. For example: <code>quantity label</code>, <code>subsection label</code>, <code>IV label</code>.</p> </li> </ul>"},{"location":"howto/customization/elns.html","title":"How to define and use ELNs in NOMAD","text":""},{"location":"howto/customization/elns.html#schemas-for-elns","title":"Schemas for ELNs","text":"<p>A schema defines all possible data structures. With small additions to our schemas, we can instruct NOMAD to provide respective editors for data. This allows us to build Electronic Lab Notebooks (ELNs) as tools to acquire data in a formal and structured way. For schemas with ELN annotations, users can create new entries in the NOMAD repository and edit the archive (structured data) of these entries directly in the GUI.</p>"},{"location":"howto/customization/elns.html#annotations","title":"Annotations","text":"<p>Definitions in a schema can have annotations. With these annotations you can provide additional information that NOMAD can use to alter its behavior around these definitions. Annotations are named blocks of key-value pairs:</p> <pre><code>definitions:\n  sections:\n    MyAnnotatedSection:\n      m_annotations:\n        annotation_name:\n          key1: value\n          key2: value\n</code></pre> <p>Many annotations control the representation of data in the GUI. This can be for plots or data entry/editing capabilities. There are three main categories of annotations relevant to ELNs. You find a reference of all annotations here.</p>"},{"location":"howto/customization/elns.html#example-eln-schema","title":"Example ELN schema","text":"<p>The is the commented ELN schema from our ELN example upload that can be created from NOMAD's upload page: <pre><code># Schemas can be defined as yaml files like this. The archive.yaml format will be\n# interpreted by nomad as a nomad archive. Therefore, all definitions have to be\n# put in a top-level section called \"definitions\"\ndefinitions:\n  # The \"definitions\" section is interpreted as a nomad schema package\n  # Schema packages can have a name:\n  name: 'Electronic Lab Notebook example schema'\n  # Schema packages contain section definitions. This is where the interesting schema\n  # information begins.\n  sections:\n    # Here we define a section called \"Chemical\":\n    Chemical:\n      # Section definition can have base_sections. Base sections are other schema\n      # definition and all properties of these will be inherited.\n      base_sections:\n        - 'nomad.datamodel.metainfo.eln.Chemical'  # Provides typical quantities like name, descriptions, chemical_formula and makes those available for search\n        - 'nomad.datamodel.data.EntryData'  # Declares this as a top-level entry section. This determines the types of entries you can create. With this we will be able to create a \"Chemical\" entry.\n      # All definitions, sections, sub_sections, quantities, can provide a description.\n      description: |\n        This is an example description for Chemical.\n        A description can contain **markdown** markup and TeX formulas, like $\\sum\\limits_{i=0}^{n}$.\n      # Sections define quantities. Quantities allow to manage actual data. Quantities\n      # can have various types, shapes, and units.\n      quantities:\n        # Here we define a quantity called \"from\"\n        form:\n          # This defines a Enum type with pre-defined possible values.\n          type:\n            type_kind: Enum\n            type_data:\n              - crystalline solid\n              - powder\n          # Annotations allow to provide additional information that is beyond just defining\n          # the possible data.\n          m_annotations:\n            # The eln annotation allows add the quantity to a ELN\n            eln:\n              component: EnumEditQuantity  # A form field component for EnumQuantities that uses a pull down menu.\n        cas_number:\n          type: str\n          m_annotations:\n            eln:\n              component: StringEditQuantity\n        ec_number:\n          type: str\n          m_annotations:\n            eln:\n              component: StringEditQuantity\n    Instrument:\n      base_sections:\n        - nomad.datamodel.metainfo.eln.Instrument\n        - nomad.datamodel.data.EntryData\n    Process:\n      base_section: nomad.datamodel.metainfo.eln.Process\n      quantities:\n        instrument:\n          type: Instrument\n          m_annotations:\n            eln:\n              component: ReferenceEditQuantity\n    Sample:\n      m_annotations:\n        # The template annotation allows to define what freshly created entries (instances of this schema) will look like.\n        # In this example we create a sample with an empty pvd_evaporation process.\n        template:\n          processes:\n            pvd_evaporation: {}\n      base_sections:\n        - 'nomad.datamodel.metainfo.eln.Sample'\n        - 'nomad.datamodel.data.EntryData'\n      quantities:\n        name:\n          type: str  # The simple string type\n          default: Default Sample Name\n          m_annotations:\n            eln:\n              component: StringEditQuantity  # A simple text edit form field\n        tags:\n          type:\n            type_kind: Enum\n            type_data:\n              - internal\n              - collaboration\n              - project\n              - other\n          shape: ['*']  # Shapes define non scalar values, like lists ['*'], vectors ['*', 3], etc.\n          m_annotations:\n            eln:\n              component: AutocompleteEditQuantity  # Allows to edit enums with an auto complete text form field\n        chemicals:\n          type: Chemical  # Types can also be other sections. This allows to reference a different section.\n          shape: ['*']\n          m_annotations:\n            eln:\n              component: ReferenceEditQuantity  # A editor component that allows to select from available \"Chemical\"s\n        substrate_type:\n          type:\n            type_kind: Enum\n            type_data:\n              - Fused quartz glass\n              - SLG\n              - other\n          m_annotations:\n            eln:\n              component: RadioEnumEditQuantity\n        substrate_thickness:\n          type: np.float64\n          unit: m\n          m_annotations:\n            eln:\n              component: NumberEditQuantity\n        sample_is_from_collaboration:\n          type: bool\n          m_annotations:\n            eln:\n              component: BoolEditQuantity\n      # Besides quantities, a section can define sub_sections. This allows hierarchies\n      # of information.\n      sub_sections:\n        # Here we define a sub_section of \"Sample\" called \"processes\"\n        processes:\n          section:\n            # The sub-section's section, is itself a section definition\n            m_annotations:\n              eln:  # adds the sub-section to the eln and allows users to create new instances of this sub-section\n            # We can also nest sub_sections. It goes aribitrarely deep.\n            sub_sections:\n              pvd_evaporation:\n                section:\n                  base_sections: ['Process', 'nomad.parsing.tabular.TableData', 'nomad.datamodel.metainfo.plot.PlotSection']\n                  m_annotations:\n                    # We can use the eln annotations to put the section to the overview\n                    # page, and hide unwanted inherited quantities.\n                    eln:\n                      overview: true\n                      hide: ['name', 'lab_id', 'description', 'method']\n                    # Plots are shown in the eln. Currently we only support simple x,y\n                    # line plots\n                    plotly_graph_object:\n                    - data:\n                        - x: \"#time\"\n                          y: \"#chamber_pressure\"\n                        - x: \"#time\"\n                          y: \"#substrate_temperature\"\n                          yaxis: y2\n                      layout:\n                        title:\n                          text: Pressure and Temperature over Time\n                        yaxis2:\n                          overlaying: y\n                          side: right\n                    - data:\n                        x: \"#time\"\n                        y: \"#chamber_pressure\"\n                    - data:\n                        x: \"#time\"\n                        y: \"#substrate_temperature\"\n                  quantities:\n                    data_file:\n                      type: str\n                      description: |\n                        A reference to an uploaded .csv produced by the PVD evaporation instruments\n                        control software.\n                      m_annotations:\n                        # The tabular_parser annotation, will treat the values of this\n                        # quantity as files. It will try to interpret the files and fill\n                        # quantities in this section (and sub_section) with the column\n                        # data of .csv or .xlsx files. There is also a mode option that by default, is set to column.\n                        tabular_parser:\n                          parsing_options:\n                            sep: '\\t'\n                            comment: '#'\n                        browser:\n                          adaptor: RawFileAdaptor  # Allows to navigate to files in the data browser\n                        eln:\n                          component: FileEditQuantity  # A form field that allows to drop and select files.\n                    time:\n                      type: np.float64\n                      shape: ['*']\n                      unit: s\n                      m_annotations:\n                        # The tabular annotation defines a mapping to column headers used in\n                        # tabular data files\n                        tabular:\n                          name: Process Time in seconds\n                    chamber_pressure:\n                      type: np.float64\n                      shape: ['*']\n                      unit: mbar\n                      m_annotations:\n                        eln:\n                          defaultDisplayUnit: mbar\n                        tabular:\n                          name: Vacuum Pressure1\n                    substrate_temperature:\n                      type: np.float64\n                      shape: ['*']\n                      unit: kelvin\n                      m_annotations:\n                        tabular:\n                          name: Substrate PV\n                          unit: degC\n              hotplate_annealing:\n                section:\n                  base_section: Process\n                  m_annotations:\n                    # We can use the eln annotations to put the section to the overview\n                    # page, and hide unwanted inherited quantities.\n                    eln:\n                      overview: true\n                      hide: ['name', 'lab_id', 'description']\n                  quantities:\n                    set_temperature:\n                      type: np.float64  # For actual numbers, we use numpy datatypes\n                      unit: K  # The unit system is based on Pint and allows all kinds of abreviations, prefixes, and complex units\n                      m_annotations:\n                        eln:\n                          component: NumberEditQuantity  # A component to enter numbers (with units)\n                    duration:\n                      type: np.float64\n                      unit: s\n                      m_annotations:\n                        eln:\n                          component: NumberEditQuantity\n</code></pre></p> <p>NOTE: Defining Labels for Quantities</p> <p>When defining labels for quantities, utilize the display annotations and ensure that you follow the conventions as described  here.</p>"},{"location":"howto/customization/hdf5.html","title":"How to use HDF5 to handle large quantities","text":"<p>The NOMAD schemas and processed data system are designed to describe and manage intricate hierarchies of connected data. This is ideal for metadata and lots of small data quantities, but does not work for large quantities. Quantities are atomic and are always managed as a whole; there is currently no functionality to stream or splice large quantities. Consequently, tools that produce or work with such data cannot scale.</p> <p>To address the issue, the option to use auxiliary storage systems optimized for large data is implemented. In the following we discuss two quantity types to enable the writing of large datasets to HDF5: <code>HDF5Reference</code> and <code>HDF5Dataset</code>. These are defined in <code>nomad.datamodel.hdf5</code>.</p>"},{"location":"howto/customization/hdf5.html#hdf5reference","title":"HDF5Reference","text":"<p>HDF5Reference is a metainfo quantity type intended to reference datasets in external raw HDF5 files. It is assumed that the dataset exists in an HDF5 file and the reference is assigned to this quantity. Static methods to read from and write to an HDF5 file are implemented. The following example illustrates how to use these.</p> <pre><code>from nomad.datamodel import ArchiveSection\nfrom nomad.datamodel.hdf5 import HDF5Reference\n\nclass LargeData(ArchiveSection):\n    value = Quantity(type=HDF5Reference)\n</code></pre> <p>The writing and reading of quantity values to and from an HDF5 file occur during processing. For illustration purposes, we mock this by creating <code>ServerContext</code>. Furthermore, we use this section definition for the <code>data</code> sub-section of EntryArchive.</p> <pre><code>import numpy as np\n\nfrom nomad.datamodel import EntryArchive, EntryMetadata\nfrom nomad.datamodel.context import ServerContext\nfrom nomad.files import StagingUploadFiles\nfrom nomad.processing import Upload\n\nupload_files = StagingUploadFiles(upload_id='test_upload', create=True)\nupload = Upload(upload_id='test_upload')\nupload_files.add_rawfiles('external.h5')\ncontext = ServerContext(upload=upload)\n\narchive = EntryArchive(\n    m_context=context,\n    metadata=EntryMetadata(upload_id=upload.upload_id, entry_id='test_entry'),\n    data=LargeData(),\n)\n\ndata = np.eye(3)\npath = 'external.h5#path/to/data'\nHDF5Reference.write_dataset(archive, data, path)\narchive.data.value = path\nHDF5Reference.read_dataset(archive, path)\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n</code></pre> <p>We use <code>write_dataset</code> to write our data into a raw HDF5 file in <code>test_upload</code> with the filename and dataset location in <code>path</code>. Additionally, archive is required to resolve the upload metadata. We then assign the reference to the dataset to <code>value</code>. To reference a file in another upload, follow the same form for reference values e.g. <code>/uploads/&lt;upload_id&gt;/raw/large_data.hdf5#group/large_field</code>.</p> <p>Important</p> <p>When reassigning a different value for an HDF5 archive quantity, it is necessary that the data attributes (shape and type) are preserved.</p> <p>To read a dataset, use <code>read_dataset</code> and provide a reference. This will return the value cast in the type of the dataset.</p>"},{"location":"howto/customization/hdf5.html#hdf5normalizer","title":"HDF5Normalizer","text":"<p>A different flavor of reading HDF5 files into NOMAD quantities is through defining a custom schema and inheriting <code>HDF5Normalizer</code> into base-sections. Two essential components of using <code>HDF5Normalizer</code> class is to first define a quantity that is annotated with <code>FileEditQuantity</code> field to enable one to drop/upload the <code>*.h5</code> file, and to define relevant quantities annotated with <code>path</code> attribute under <code>hdf5</code>. These quantities are then picked up by the normalizer to extract the values to be found denoted by the <code>path</code>.</p> <p>A minimum example to import your hdf5 and map it to NOMAD quantities is by using the following custom schema:</p> <pre><code>definitions:\n  name: 'hdf5'\n  sections:\n    Test_HDF5:\n      base_sections:\n        - 'nomad.datamodel.data.EntryData'\n        - 'nomad.datamodel.metainfo.basesections.HDF5Normalizer'\n      quantities:\n        datafile:\n          type: str\n          m_annotations:\n            eln:\n              component: FileEditQuantity\n        charge_density:\n          type: np.float32\n          shape: [ '*', '*', '*' ]\n          m_annotations:\n            hdf5:\n              path: '/path/to/charge_density'\n</code></pre>"},{"location":"howto/customization/hdf5.html#hdf5dataset","title":"HDF5Dataset","text":"<p>To use HDF5 storage for archive quantities, one should use <code>HDF5Dataset</code>.</p> <pre><code>from nomad.datamodel.hdf5 import HDF5Dataset\n\nclass LargeData(ArchiveSection):\n    value = Quantity(type=HDF5Dataset)\n</code></pre> <p>The assigned value will also be written to the archive HDF5 file and serialized as <code>/uploads/test_upload/archive/test_entry#/data/value</code>.</p> <pre><code>archive.data.value = np.ones(3)\n\nserialized = archive.m_to_dict()\nserialized['data']['value']\n# '/uploads/test_upload/archive/test_entry#/data/value'\ndeserialized = archive.m_from_dict(serialized, m_context=archive.m_context)\ndeserialized.data.value\n# array([1., 1., 1.])\n</code></pre>"},{"location":"howto/customization/hdf5.html#visualizing-archive-hdf5-quantities","title":"Visualizing archive HDF5 quantities","text":"<p>NOMAD clients (e.g. NOMAD UI) can pick up on these HDF5 serialized quantities and provide respective functionality (e.g. showing a H5Web view).</p> <p> </p> Visualizing archive HDF5 reference quantity using H5Web. <p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"howto/customization/hdf5.html#metadata-for-large-quantities","title":"Metadata for large quantities","text":"<p>Attention</p> <p>This will be implemented and documented soon.</p>"},{"location":"howto/customization/tabular.html","title":"How to parse tabular data","text":"<p>Refer to the Reference guide for the full list of annotations connected to this parser and to the Tabular parser tutorial  for a detailed description of each of them.</p>"},{"location":"howto/customization/tabular.html#preparing-the-tabular-data-file","title":"Preparing the tabular data file","text":"<p>NOMAD and <code>Excel</code> support multiple-sheets data manipulations and imports. Each quantity in the schema will be annotated with a source path composed by sheet name and column header. The path to be used with the tabular data displayed below would be <code>Sheet1/My header 1</code> and it would be placed it the <code>tabular</code> annotation, see schema annotations.</p> <p> </p> <p>In the case there is only one sheet in the Excel file, or when using a <code>.csv</code> file that is a single-sheet format, the sheet name is not required in the path.</p> <p>The data sheets can be stored in one or more files depending on the user needs. Each sheet can independently be organized in one of the following ways:</p> <p>1) Columns:  each column contains an array of cells that we want to parse into one quantity. Example: time and temperature arrays to be plotted as x and y.</p> <p> </p> <p>2) Rows:  each row contains a set of cells that we want to parse into a section, i. e. a set of quantities. Example: an inventory tabular data file (for substrates, precursors, or more) where each column represents a property and each row corresponds to one unit stored in the inventory.</p> <p> </p> <p>3) Rows with repeated columns:</p> <p>in addition to the mode 2), whenever the parser detects the presence of multiple columns (or multiple sets of columns) with same headers, these are taken as multiple instances of a subsection. More explanations will be delivered when showing the schema for such a structure. Example: a crystal growth process where each row is a step of the crystal growth and the repeated columns describe the \"precursor materials\", that can be more than one during such processes and they are described by the same \"precursor material\" section.</p> <p> </p> <p>Furthermore, we can insert comments before our data, we can use a special character to mark one or more rows as comment rows. The special character is annotated within the schema in the <code>tabular</code> annotation, see schema annotations:</p> <p> </p>"},{"location":"howto/customization/tabular.html#inheriting-the-tabledata-base-section","title":"Inheriting the TableData base section","text":"<p><code>TableData</code> can be inherited adding the following lines in the yaml schema file:</p> <pre><code>MySection:\n  base_sections:\n    - nomad.datamodel.data.EntryData\n    - nomad.parsing.tabular.TableData\n</code></pre> <p><code>EntryData</code> is usually also necessary as we will create entries from the section we are defining. <code>TableData</code> provides a customizable checkbox quantity, called <code>fill_archive_from_datafile</code>, to turn the tabular parser <code>on</code> or <code>off</code>. To avoid the parser running everytime a change is made to the archive data, it is sufficient to uncheck the checkbox. It is customizable in the sense that if you do not wish to see this checkbox at all, you can configure the <code>hide</code> parameter of the section's <code>m_annotations</code> to hide the checkbox. This in turn sets the parser to run everytime you save your archive. To hide it, add the following lines:</p> <pre><code>MySection:\n  base_sections:\n    - nomad.datamodel.data.EntryData\n    - nomad.parsing.tabular.TableData\n  m_annotations:\n    eln:\n      hide: ['fill_archive_from_datafile']\n</code></pre> <p>Be cautious though! Turning on the tabular parser (or checking the box) on saving your data will cause losing/overwriting your manually-entered data by the parser!</p>"},{"location":"howto/customization/tabular.html#importing-data-in-nomad","title":"Importing data in NOMAD","text":"<p>After writing a schema file and creating a new upload in NOMAD (or using an existing upload), it is possible to upload the schema file. After creating a new Entry out of one section of the schema, the tabular data file must be dropped in the quantity designated by the <code>FileEditQuantity</code> annotation. After clicking save the parsing will start. In the Overview page of the NOMAD upload, new Entries are created and appended to the Processed data section. In the Entry page, clicking on DATA tab (on top of the screen) and in the Entry lane, the data is populated under the <code>data</code> subsection.</p>"},{"location":"howto/customization/tabular.html#hands-on-examples-of-all-tabular-parser-modes","title":"Hands-on examples of all tabular parser modes","text":"<p>In this section eight examples will be presented, containing all the features available in tabular parser. Refer to the Tutorial for more comments on the implications of the structures generated by the following yaml files.</p>"},{"location":"howto/customization/tabular.html#1-column-mode-current-entry-parse-to-root","title":"1. Column mode, current Entry, parse to root","text":"<p>The first case gives rise to the simplest data archive file. Here the tabular data file is parsed by columns, directly within the Entry where the <code>TableData</code> is inherited and filling the quantities in the root level of the schema (see dedicated how-to to learn how to inherit tabular parser in your schema).</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the same Entry of the parsed quantities.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data (<code>root</code> in this case).</li> <li>quantities parsed in <code>column</code> mode must have the <code>shape: ['*']</code> attribute, that means they are arrays and not scalars.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 1'\n  sections:\n    MySection1:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: column\n                  file_mode: current_entry\n                  sections:\n                    - '#root'\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#2-column-mode-current-entry-parse-to-my-path","title":"2. Column mode, current Entry, parse to my path","text":"<p>The parsing mode presented here only differs from the previous for the <code>sections</code> annotations. In this case the section that we want to fill with tabular data can be nested arbitrarily deep in the schema and the <code>sections</code> annotation must be filled with a forward slash path to the desired section, e. g. <code>my_sub_section/my_sub_sub_section</code>.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the same Entry of the parsed quantities.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reachs it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>).</li> <li>quantities parsed in <code>column</code> mode must have the <code>shape: ['*']</code> attribute, that means they are arrays and not scalars.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 2'\n  sections:\n    MySection2:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: column\n                  file_mode: current_entry\n                  sections:\n                    - my_sub_section_2\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_sub_section_2:\n          section: '#/MySubSection2'\n    MySubSection2:\n      m_annotations:\n        eln:\n      quantities:\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#3-row-mode-current-entry-parse-to-my-path","title":"3. Row mode, current Entry, parse to my path","text":"<p>The current is the first example of parsing in row mode. This means that every row of the excel file while be placed in one instance of the section that is defined in <code>sections</code>. This section must be decorated with <code>repeats: true</code> annotation, it will allow to generate multiple instances that will be appended in a list with sequential numbers. Instead of sequential numbers, the list can show specific names if <code>label_quantity</code> annotation is appended to the repeated section. This annotation is included in the how-to example. The section is written separately in the schema and it does not need the <code>EntryData</code> inheritance because the instances will be grafted directly in the current Entry. As explained below, it is not possible for <code>row</code> and <code>current_entry</code> to parse directly in the root because we need to create multiple instances of the selected subsection and organize them in a list.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the same Entry of the parsed quantities.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reaches it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>).</li> <li>quantities parsed in <code>row</code> mode are scalars.</li> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 3'\n  sections:\n    MySection3:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: current_entry\n                  sections:\n                    - my_repeated_sub_section_3\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_repeated_sub_section_3:\n          repeats: true\n          section: '#/MySubSection3'\n    MySubSection3:\n      m_annotations:\n        eln:\n      more:\n        label_quantity: '#/data/my_quantity_1'\n      quantities:\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_quantity_2:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#4-column-mode-single-new-entry-parse-to-my-path","title":"4. Column mode, single new Entry, parse to my path","text":"<p>One more step of complexity is added here: the parsing is not performed in the current Entry, but a new Entry it automatically generated and filled. This structure foresees a parent Entry where we collect one or more tabular data files and possibly other info while we want to separate a specific entity of our data structure in another searchable Entry in NOMAD, e. g. a substrate Entry or a measurement Entry that would be collected inside a parent experiment Entry. We need to inherit <code>SubSect</code> class from <code>EntryData</code> because these will be standalone archive files in NOMAD. Parent and children Entries are connected by means of the <code>ReferenceEditQuantity</code> annotation in the parent Entry schema. This annotation is attached to a quantity that becomes a hook to the other ones, It is a powerful tool that allows to list in the overview of each Entry all the other referenced ones, allowing to build paths of referencing available at a glance.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the parent Entry, the data is parsed in the child Entry.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reachs it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>)</li> <li>quantities parsed in <code>column</code> mode must have the <code>shape: ['*']</code> attribute, that means they are arrays and not scalars.</li> <li>inherit also the subsection from <code>EntryData</code> as it must be a NOMAD Entry archive file.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 4'\n  sections:\n    MySection4:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: column\n                  file_mode: single_new_entry\n                  sections:\n                    - my_subsection_4\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_subsection_4:\n          section:\n            m_annotations:\n              eln:\n            quantities:\n              my_ref_quantity:\n                type: '#/MySubSection4'\n                m_annotations:\n                  eln:\n                    component: ReferenceEditQuantity\n    MySubSection4:\n      base_sections:\n      - nomad.datamodel.data.EntryData\n      m_annotations:\n        eln:\n      quantities:\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#5-row-mode-single-new-entry-parse-to-my-path","title":"5. Row mode, single new Entry, parse to my path","text":"<p>Example analogous to the previous, where the new created Entry contains now a repeated subsection with a list of instances made from each line of the tabular data file, as show in the Row mode, current Entry, parse to my path case.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the parent Entry, the data is parsed in the child Entry.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reachs it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>)</li> <li>quantities parsed in <code>row</code> mode are scalars.</li> <li>inherit also the subsection from <code>EntryData</code> as it must be a NOMAD Entry archive file.</li> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 5'\n  sections:\n    MySection5:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: single_new_entry\n                  sections:\n                    - my_subsection_5/my_repeated_sub_section\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_subsection_5:\n          section:\n            m_annotations:\n              eln:\n            quantities:\n              my_ref_quantity:\n                type: '#/MySubSection5'\n                m_annotations:\n                  eln:\n                    component: ReferenceEditQuantity\n    MySubSection5:\n      base_sections:\n      - nomad.datamodel.data.EntryData\n      m_annotations:\n        eln:\n      more:\n        label_quantity: '#/data/my_quantity_1'\n      sub_sections:\n        my_repeated_sub_section:\n          repeats: true\n          section:\n            quantities:\n              my_quantity_1:\n                type: str\n                m_annotations:\n                  tabular:\n                    name: \"My header 1\"\n              my_quantity_2:\n                type: str\n                m_annotations:\n                  tabular:\n                    name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#6-row-mode-multiple-new-entries-parse-to-root","title":"6. Row mode, multiple new entries, parse to root","text":"<p>The last feature available for tabular parser is now introduced: <code>multiple_new_entries</code>. It is only meaningful for <code>row</code> mode because each row of the tabular data file will be placed in a new Entry that is an instance of a class defined in the schema, this would not make sense for columns, though, as they usually need to be parsed all together in one class of the schema, for example the \"timestamp\" and \"temperature\" columns in a spreadsheet file would need to lie in the same class as they belong to the same part of experiment. A further comment is needed to explain the combination of this feature with <code>root</code>. As mentioned before, using <code>root</code> foresees to graft data directly in the present Entry. In this case, this means that a manyfold of Entries will be generated based on the only class available in the schema. These Entries will not be bundled together by a parent Entry but just live in our NOMAD Upload as a spare list. They might be referenced manually by the user with <code>ReferenceEditQuantity</code> in other archive files. Bundling them together in one overarching Entry already at the parsing stage would require the next and last example to be introduced.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the parent Entry, the data is parsed in the children Entries.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>quantities parsed in <code>row</code> mode are scalars.</li> <li>inherit also the subsection from <code>EntryData</code> as it must be a NOMAD Entry archive file.</li> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 6'\n  sections:\n    MySection6:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      more:\n        label_quantity: '#/data/my_quantity_1'\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: multiple_new_entries\n                  sections:\n                    - '#root'\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_quantity_2:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#7-row-mode-multiple-new-entries-parse-to-my-path","title":"7. Row mode, multiple new entries, parse to my path","text":"<p>As anticipated in the previous example, <code>row</code> mode in connection to <code>multiple_new_entries</code> will produce a manyfold of instances of a specific class, each of them being a new Entry. In the present case, each instance will also automatically be placed in a <code>ReferenceEditQuantity</code> quantity lying in a subsection defined within the parent Entry, coloured in plum in the following example image.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the same Entry, the data is parsed in the children Entries.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reachs it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>)</li> <li>quantities parsed in <code>row</code> mode are scalars.</li> <li>inherit also the subsection from <code>EntryData</code> as it must be a standalone NOMAD archive file.</li> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 7'\n  sections:\n    MySection7:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: multiple_new_entries\n                  sections:\n                    - my_repeated_sub_section_7\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_repeated_sub_section_7:\n          repeats: true\n          section:\n            m_annotations:\n              eln:\n            quantities:\n              my_ref_quantity:\n                type: '#/MySubSection7'\n                m_annotations:\n                  eln:\n                    component: ReferenceEditQuantity\n    MySubSection7:\n      base_sections:\n      - nomad.datamodel.data.EntryData\n      m_annotations:\n        eln:\n      more:\n        label_quantity: 'my_quantity_1'\n      quantities:\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_quantity_2:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#8-the-sub-subsection-nesting-schema","title":"8. The Sub-Subsection nesting schema","text":"<p>If the tabular data file contains multiple columns with exact same name, there is a way to parse them using <code>row</code> mode. As explained in previous examples, this mode creates an instance of a subsection of the schema for each row of the file. Whenever column with same name are found they are interpreted as multiple instances of a sub-subsection nested inside the subsection. To build a schema with such a feature it is enough to have two nested classes, each of them bearing a <code>repeats: true</code> annotation. This structure can be applied to each and every of the cases above with <code>row</code> mode parsing.</p> <p>Important</p> <ul> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code> and also in the sub-subsection within <code>MySubSect</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 8'\n  sections:\n    MySection8:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: current_entry\n                  sections:\n                    - my_repeated_sub_section_8\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_repeated_sub_section_8:\n          repeats: true\n          section: '#/MySubSection8'\n    MySubSection8:\n      m_annotations:\n        eln:\n      more:\n        label_quantity: '#/data/my_quantity_1'\n      quantities:\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n      sub_sections:\n        my_repeated_sub_sub_section:\n          repeats: true\n          section:\n            more:\n              label_quantity: my_quantity_2\n            quantities:\n              my_quantity_2:\n                type: str\n                m_annotations:\n                  tabular:\n                    name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#9-not-possible-implementations","title":"9. Not possible implementations","text":"<p>Some combinations of <code>mapping_options</code>, namely <code>file_mode</code>, <code>mapping_mode</code>, and <code>sections</code>, can give rise to not interpretable instructions or not useful data structure. For the sake of completeness, a brief explanation of the five not possible cases will be provided.</p>"},{"location":"howto/customization/tabular.html#91-row-mode-current-entry-parse-to-root","title":"9.1 Row mode, current Entry, parse to root","text":"<p><code>row</code> mode always requires a section instance to be populated with one row of cells from the tabular data file. Multiple instances are hence generated from the rows available in the file. The instances are organized in a list and the list must be necessarily hosted as a subsection in some parent section. That's why, within the parent section, a path in <code>sections</code> must be provided different from <code>root</code>.</p>"},{"location":"howto/customization/tabular.html#92-column-mode-single-new-entry-parse-to-root","title":"9.2 Column mode, single new Entry, parse to root","text":"<p>This would create a redundant Entry with the very same structure of the one where the <code>data_file</code> quantity is placed, the structure would furthermore miss a reference between the two Entries. A better result is achieved using a path in <code>sections</code> that would create a new Entry and reference it in the parent one.</p>"},{"location":"howto/customization/tabular.html#93-row-mode-single-new-entry-parse-to-root","title":"9.3 Row mode, single new Entry, parse to root","text":"<p>As explained in the first section of not possible cases, when parsing in row mode we create multiple instances that cannot remain as standalone floating objects. They must be organized as a list in a subsection of the parent Entry.</p>"},{"location":"howto/customization/tabular.html#94-column-mode-multiple-new-entries-parse-to-root","title":"9.4 Column mode, multiple new entries, parse to root","text":"<p>This case would create a useless set of Entries containing one array quantity each. Usually, when parsing in column mode we want to parse together all the columns in the same section.</p>"},{"location":"howto/customization/tabular.html#95-column-mode-multiple-new-entries-parse-to-my-path","title":"9.5 Column mode, multiple new entries, parse to my path","text":"<p>This case would create a useless set of Entries containing one array quantity each. Usually, when parsing in column mode we want to parse together all the columns in the same section.</p>"},{"location":"howto/customization/units.html","title":"How to work with units","text":"<p>Units are a very important part of any scientific work. Units are also a common source of problems when multiple people are working with the same data. Sometimes this has far-reaching consequences as demonstrated by the Mars Climate Orbiter incident. This document explains the possibilities and best practices when working with units within the NOMAD infrastructure.</p>"},{"location":"howto/customization/units.html#available-unit-names","title":"Available unit names","text":"<p>The available unit names are controlled by the <code>nomad/units/defaults_en.txt</code> file. This is a plain text file that is internally read by the Pint library that we use for unit transformations in the NOMAD Python backend. These definitions and the associated conversion factors are then used always when transforming between two units in the NOMAD platform. Our graphical user interface also performs unit conversions in Javascript, and the same definitions and conversion factors are translated to the frontend through an environment config file (<code>env.js</code>).</p> <p>It is possible to add new units in this file to make them available both in the Python and in the Javascript environment. You can read more on how units are defined here.</p> <p>All units support the use of SI-prefixes. This means that if the unit <code>meter</code> has been defined, you are able to then automatically use <code>kilometers</code>, <code>centimeters</code>, <code>millimeters</code>, etc.</p>"},{"location":"howto/customization/units.html#defining-units-for-storing-data","title":"Defining units for storing data","text":"<p>From a practical point of view, there needs to be single choice for the unit in which the data is stored on hard disks, databases or in search engines. This choice is controlled by the <code>unit</code> attribute of a <code>Quantity</code>:</p> <pre><code>from nomad.metainfo import Quantity\n\nmy_energies = Quantity(\n  dtype=float,\n  unit='eV'\n)\n</code></pre> <p>The data will always be stored in this unit and will be returned in this unit when using e.g. the API.</p>"},{"location":"howto/customization/units.html#using-units-in-python","title":"Using units in Python","text":"<p>When creating custom schemas or parsers in the NOMAD framework, it is important to use the same unit definitions and conversion factors throughout. You should never define custom unit conversion routines of factors, but instead use the <code>nomad.units</code> package. This is important to ensure the interoperability of data.</p> <p>Here is an example of how you could work with units in Python:</p> <pre><code>import numpy as np\nfrom nomad.units import ureg  # Always import from here, never use another registry!\n\nfrom nomad.metainfo import MSection, Quantity\n\n\n# Here is a section with a quantity definition\nclass MySection(MSection):\n    my_energies = Quantity(\n        type=np.float64,\n        shape=[2],\n        unit='eV'\n    )\n\n\nmy_section = MySection()\n\n# If we assign a plain number array to a quantity, it is assumed to be given in\n# the units defined in the Quantity\nenergies = np.array([1, 1])\nmy_section.my_energies = energies\nprint(my_section.my_energies)  # [1 1] electron_volt\n\n# We can make a plain number array into a Pint Quantity by multiplying with\n# a unit\nenergies_with_unit = energies * ureg('hartree')\n\n# All numpy operations still work as normal, but the unit information is always\n# stored alongside\nenergies_with_unit *= 10\n\n# If you now assign this data into a NOMAD Quantity, the unit conversion is done\n# automatically\nmy_section.my_energies = energies_with_unit\nprint(my_section.my_energies)  # [272.11386245988473 272.11386245988473] electron_volt\n</code></pre>"},{"location":"howto/customization/units.html#defining-units-for-displaying-data","title":"Defining units for displaying data","text":"<p>When data is being displayed by the GUI, the unit can be choosen independently from the unit used in storing the data. There are several reasons for doing this:</p> <ul> <li>Maybe the unit is stored in SI units for consistency, but when viewing the  data you want to view it in some more field-specific units</li> <li>Maybe the unit is stored in some field specific units, but when demonstrating  your work you wish to use more standardized units.</li> <li>Maybe due to your background, you are more familiar with a specific unit, and  viewing the data in this unit helps you to understand it better. E.g. a  physicist might be familiar with working with electron volts, whereas a chemist  might prefer kilocalorie per mole.</li> </ul> <p>Currently the display unit is controlled through the ELN annotation, like this:</p> <pre><code>distance = Quantity(\n  dtype=float,\n  unit='meter',\n  a_eln=dict(defaultDisplayUnit='millimeter')\n)\n</code></pre>"},{"location":"howto/customization/workflows.html","title":"How to define workflows","text":""},{"location":"howto/customization/workflows.html#the-built-in-abstract-workflow-schema","title":"The built-in abstract workflow schema","text":"<p>Workflows are an important aspect of data as they explain how the data came to be. Let's first clarify that workflow refers to a workflow that already happened and that has produced input and output data that are linked through tasks that have been performed . This often is also referred to as data provenance or provenance graph.</p> <p>The following shows the overall abstract schema for worklows that can be found in <code>nomad.datamodel.metainfo.workflow</code> (blue):</p> <p></p> <p>The idea is that workflows are stored in a top-level archive section along-side other sections that contain the inputs and outputs. This way the workflow or provenance graph is just additional piece of the archive that describes how the data in this (or other archives) is connected.</p> <p>Let'c consider an example workflow. Imagine a geometry optimization and ground state calculation performed by two individual DFT code runs. The code runs are stored in NOMAD entries <code>geom_opt.archive.yaml</code> and <code>ground_state.archive.yaml</code> using the <code>run</code> top-level section.</p>"},{"location":"howto/customization/workflows.html#example-workflow","title":"Example workflow","text":"<p>Here is a logical depiction of the workflow and all its tasks, inputs, and outputs.</p> <p></p>"},{"location":"howto/customization/workflows.html#simple-workflow-entry","title":"Simple workflow entry","text":"<p>The following archive shows how to create such a workflow based on the given schema. Here we only model the <code>GeometryOpt</code> and <code>GroundStateCalculation</code> as two tasks with respective inputs and outputs that use references to entry archives of the respective code runs.</p> <pre><code>workflow2:\n  inputs:\n    - name: input system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n  outputs:\n    - name: relaxed system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n    - name: ground state calculation of relaxed system\n      section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n  tasks:\n    - name: GeometryOpt\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n      outputs:\n        - name: relaxed system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n\n    - name: GroundStateCalculation\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n      outputs:\n        - name: ground state\n          section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n</code></pre>"},{"location":"howto/customization/workflows.html#nested-workflows-in-one-entry","title":"Nested workflows in one entry","text":"<p>Since a <code>Workflow</code> instance is also a <code>Tasks</code> instance due to inheritance, we can nest workflows. Here we detailed the <code>GeometryOpt</code> as a nested workflow:</p> <pre><code>workflow2:\n  inputs:\n    - name: input system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n  outputs:\n    - name: relaxed system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n    - name: ground state calculation of relaxed system\n      section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n  tasks:\n    - name: GeometryOpt\n      m_def: nomad.datamodel.metainfo.workflow.Workflow\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n      outputs:\n        - name: relaxed system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n      tasks:\n        - inputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n          outputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/1'\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/calculation/0'\n        - inputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/1'\n          outputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/2'\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/calculation/1'\n        - inputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/2'\n          outputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/3'\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/calculation/2'\n    - name: GroundStateCalculation\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n      outputs:\n        - name: ground state\n          section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n</code></pre>"},{"location":"howto/customization/workflows.html#nested-workflows-in-multiple-entries","title":"Nested Workflows in multiple entries","text":"<p>Typically, we want to colocate our individual workflows with their inputs and outputs. In the case of the geometry optimization, we might want to put this into the archive of the geometry optimization code run. So the <code>geom_opt.archive.yaml</code> might contain its own section <code>workflow2</code> that only contains the <code>GeometryOpt</code> workflow and uses local references to its inputs and outputs:</p> <pre><code>workflow2:\n  name: GeometryOpt\n  inputs:\n    - name: input system\n      section: '#/run/0/system/0'\n  outputs:\n    - name: relaxed system\n      section: '#/run/0/system/-1'\n  tasks:\n    - inputs:\n        - section: '#/run/0/system/0'\n      outputs:\n        - section: '#/run/0/system/1'\n        - section: '#/run/0/calculation/0'\n    - inputs:\n        - section: '#/run/0/system/1'\n      outputs:\n        - section: '#/run/0/system/2'\n        - section: '#/run/0/calculation/1'\n    - inputs:\n        - section: '#/run/0/system/2'\n      outputs:\n        - section: '#/run/0/system/3'\n        - section: '#/run/0/calculation/2'\nrun:\n  - program:\n      name: 'VASP'\n    system: [{}, {}, {}]\n    calculation: [{}, {}, {}]\n</code></pre> <p>When we want to detail the complex workflow, we now need to refer to a nested workflow in a different entry. This cannot be done directly, because <code>Workflow</code> instances can only contain <code>Task</code> instances and not reference them. Therefore, we added a <code>TaskReference</code> section definition that can be used to create proxy instances for tasks and workflows:</p> <pre><code>workflow2:\n  inputs:\n    - name: input system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n  outputs:\n    - name: relaxed system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n    - name: ground state calculation of relaxed system\n      section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/raw/geom_opt.archive.yaml#/workflow2'\n    - name: GroundStateCalculation\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n      outputs:\n        - name: ground state\n          section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n</code></pre>"},{"location":"howto/customization/workflows.html#extending-the-workflow-schema","title":"Extending the workflow schema","text":"<p>The abstract workflow schema above allows us to build generalized tools for workflows, like workflow searches, navigation in workflow, graphical representations of workflows, etc. But, you can still augment the given section definitions with more information through inheritance. These information can be specialized references to denote inputs and outputs, can be additional workflow or task parameters, and much more.</p> <p>In this example, we created a special workflow section definition <code>GeometryOptimization</code> that defines a parameter <code>threshold</code> and an additional reference to the final calculation of the optimization:</p> <pre><code>definitions:\n  sections:\n    GeometryOptimizationWorkflow:\n      base_section: nomad.datamodel.metainfo.workflow.Workflow\n      quantities:\n        threshold:\n          type: float\n          unit: eV\n        final_calculation:\n          type: runschema.calculation.Calculation\n\nworkflow2:\n  m_def: GeometryOptimizationWorkflow\n  final_calculation: '#/run/0/calculation/-1'\n  threshold: 0.029\n  name: GeometryOpt\n  inputs:\n    ...\n</code></pre>"},{"location":"howto/customization/workflows.html#how-to-use-the-workflow-visualizer","title":"How to use the workflow visualizer","text":"<p>The entry overview page will show an interactive graph of the <code>workflow2</code> section if defined. In the following example, a workflow containing three tasks <code>Single Point</code>, <code>Geometry Optimization</code> and <code>Phonon</code> is shown.</p> <p></p> <p>The nodes (inputs, tasks and outputs) are shown from left to right for the current workflow layer. The edges (arrows) from (to) a node denotes an input (output) to a section in the target node. One can see the description for the nodes and edges by hovering over them. When the inputs and outputs are clicked, the linked section is shown in the archive browser. By clicking on a task, the graph zooms into the nested workflow layer. By clicking on the arrows, only the relevant linked nodes are shown. One can go back to the previous view by clicking on the current workflow node.</p> <p>A number of controls are also provided on top of the graph. The first enables a filtering of the nodes following a python-like syntax i.e., list (comma-separated) or range (colon-separated). Negative index and percent are also supported. By default, the task nodes can be filtered but can be changed to inputs or outputs by clicking on one of the respective nodes. By clicking on the <code>play</code> button, a force-directed layout of the task nodes is enabled. The other tools enable to toggle the legend, go back to a previous view and reset the view.</p>"},{"location":"howto/develop/code.html","title":"How to navigate the code","text":"<p>NOMAD is a complex project with lots of parts. This guide gives you a rough overview about the codebase and ideas about what to look at first.</p>"},{"location":"howto/develop/code.html#git-projects","title":"Git Projects","text":"<p>There is one main NOMAD project (and its fork on GitHub). This project contains all the framework and infrastructure code. It instigates all checks, builds, and deployments for the public NOMAD service, the NOMAD Oasis, and the <code>nomad-lab</code> Python package. All contributions to NOMAD have to go through this project eventually.</p> <p>All (Git) projects that NOMAD depends on are either a Git submodule (you find them all in the <code>dependencies</code> directory or its subdirectories) or they are listed as PyPI packages in the <code>pyproject.toml</code> of the main project (or one of its submodules).</p> <p>You can also have a look at the list of parsers and built-in plugins that constitute the majority of these projects. The only other projects are MatID, DOS fingerprints, and the NOMAD Remote Tools Hub.</p> <p>Note</p> <p>The GitLab organization nomad-lab and the GitHub organizations for FAIRmat and the NOMAD CoE all represent larger infrastructure and research projects, and they include many other Git projects that are not related. When navigating the codebase, only follow the submodules.</p>"},{"location":"howto/develop/code.html#python-code","title":"Python code","text":"<p>There are three main directories with Python code:</p> <ul> <li> <p><code>nomad</code>: The actual NOMAD code. It is structured into more subdirectories and modules.</p> </li> <li> <p><code>tests</code>: Tests (pytest) for the NOMAD code.   It follows the same module structure, but Python files are prefixed with <code>test_</code>.</p> </li> <li> <p><code>examples</code>: A few small Python scripts that might be linked in the documentation.</p> </li> </ul> <p>The <code>nomad</code> directory contains the following \"main\" modules. This list is not extensive but should help you to navigate the codebase:</p> <ul> <li> <p><code>app</code>: The FastAPI APIs: v1 and v1.2 NOMAD APIs,   OPTIMADE, DCAT,   h5grove, and more.</p> </li> <li> <p><code>archive</code>: Functionality to store and access archive files. This is the storage format   for all processed data in NOMAD. See also the docs on   structured data.</p> </li> <li> <p><code>cli</code>: The command line interface (based on Click).   Subcommands are structured into submodules.</p> </li> <li> <p><code>config</code>: NOMAD is configured through the <code>nomad.yaml</code> file. This contains all the   (Pydantic) models and default config parameters.</p> </li> <li> <p><code>datamodel</code>: The built-in schemas (e.g. <code>nomad.datamodel.metainfo.workflow</code> used to construct   workflows). The base sections and section for the shared entry structure.   See also the docs on the datamodel and   processing.</p> </li> <li> <p><code>metainfo</code>: The Metainfo system, e.g. the schema language that NOMAD uses.</p> </li> <li> <p><code>normalizing</code>: All the normalizers. See also the docs on   processing.</p> </li> <li> <p><code>parsing</code>: The base classes for parsers, matching functionality, parser initialization,   some fundamental parsers like the archive parser. See also the docs on   processing.</p> </li> <li> <p><code>processing</code>: It's all about processing uploads and entries. The interface to   Celery and MongoDB.</p> </li> <li> <p><code>units</code>: The unit and unit conversion system based on   Pint.</p> </li> <li> <p><code>utils</code>: Utility modules, e.g. the structured logging system   (structlog), id generation, and hashes.</p> </li> <li> <p><code>files.py</code>: Functionality to maintain the files for uploads in staging and published.   The interface to the file system.</p> </li> <li> <p><code>search.py</code>: The interface to   Elasticsearch.</p> </li> </ul>"},{"location":"howto/develop/code.html#gui-code","title":"GUI code","text":"<p>The NOMAD UI is written as a React single-page application (SPA). It uses (among many other libraries) MUI, Plotly, and D3. The GUI code is maintained in the <code>gui</code> directory. Most relevant code can be found in <code>gui/src/components</code>. The application entry point is <code>gui/src/index.js</code>.</p>"},{"location":"howto/develop/code.html#documentation","title":"Documentation","text":"<p>The documentation is based on MkDocs. The important files and directories are:</p> <ul> <li> <p><code>docs</code>: Contains all the Markdown files that contribute to the documentation system.</p> </li> <li> <p><code>mkdocs.yml</code>: The index and configuration of the documentation. New files have to be   added here as well.</p> </li> <li> <p><code>nomad/mkdocs.py</code>: Python code that defines   macros which can be used in Markdown.</p> </li> </ul>"},{"location":"howto/develop/code.html#other-top-level-directories","title":"Other top-level directories","text":"<ul> <li> <p><code>dependencies</code>: Contains all the submodules, e.g. the parsers.</p> </li> <li> <p><code>ops</code>: Contains artifacts to run NOMAD components, e.g. <code>docker-compose.yaml</code> files,   and our Kubernetes Helm chart.</p> </li> <li> <p><code>scripts</code>: Contains scripts used during the build or for certain development tasks.</p> </li> </ul>"},{"location":"howto/develop/contrib.html","title":"How to contribute","text":"<p>Note</p> <p>The NOMAD source code is maintained in two synchronized projects on GitHub and a GitLab run by MPCDF. Everyone can contribute on GitHub. The GitLab instance requires an account for active contribution. This not an ideal situation: there are historic reasons and there is a lot of buy-in into the GitLab CI/CD system. This guide addresses contributions to both projects.</p>"},{"location":"howto/develop/contrib.html#issues","title":"Issues","text":""},{"location":"howto/develop/contrib.html#issue-trackers","title":"Issue trackers","text":"<p>Everyone can open a new issue in our main GitHub project.</p> <p>Use issues to ask questions, report bugs, or suggest features. If in doubt, use the main project to engage with us. If you address a specific plugin (e.g. parser), you can also post into the respective projects. See also the list of parsers and the list of built-in plugins.</p> <p>If you are a member of FAIRmat, the NOMAD CoE, or are a close collaborator, you probably have an MPCDF GitLab account (or should ask us for one). Please use the issue tracker on our main GitLab project. This is where most of the implementation work is planned and executed.</p>"},{"location":"howto/develop/contrib.html#issue-content","title":"Issue content","text":"<p>A few tips that will help us to solve your issues quicker:</p> <ul> <li> <p>Focus on the issue. You don't need to greet us or say thanks and goodbye. Let's keep it   technical.</p> </li> <li> <p>Use descriptive short issue titles. Use specific words over general words.</p> </li> <li> <p>Describe the observed problem and not the assumed causes. Clearly separate speculation   from the problem description.</p> </li> <li> <p>Bugs: Think how we could reproduce the problem:</p> <ul> <li>What NOMAD URL are you using (UI), which package version (Python)?</li> <li>Is there an upload or entry id that we can look at?</li> <li>Example files or code snippets?</li> <li>Don't screenshot code, copy and paste instead. Use   code blocks.</li> </ul> </li> <li> <p>Features: Augment your feature descriptions with a use case that helps us understand   the feature and its scope.</p> </li> </ul>"},{"location":"howto/develop/contrib.html#issues-labels-gitlab","title":"Issues labels (GitLab)","text":"<p>On the main GitLab project, there are three main categories for labels. Ideally, each issue gets one of each category:</p> <ul> <li> <p>State label (grey): These are used to manage when and how the issue is addressed. This should be given by the NOMAD team member who is currently responsible to moderate the development. If it's a simple fix and you want to do it yourself, assign yourself and use \"bugfixes\".</p> </li> <li> <p>Component label (purple): These denote the part of the NOMAD software that is most likely effected. Multiple purple labels are possible.</p> </li> <li> <p>Kind label (red): Whether this is a bug, feature, refactoring, or documentation issue.</p> </li> </ul> <p>Unlabeled issues will get labeled by the NOMAD team as soon as possible. You can provide labels yourself.</p>"},{"location":"howto/develop/contrib.html#documentation","title":"Documentation","text":"<p>The documentation is part of NOMAD's main source code project. You can raise issues about the documentation as usual. To make changes, create a merge or pull request.</p> <p>See also the documentation part in our code navigation guide.</p>"},{"location":"howto/develop/contrib.html#plugins","title":"Plugins","text":"<p>Also read the guide on how to develop, publish, and distribute plugins.</p>"},{"location":"howto/develop/contrib.html#built-in-plugins-and-submodules","title":"Built-in plugins (and submodules)","text":"<p>Most plugins that are maintained by the NOMAD team are built-in plugins (e.g. all the parsers). These plugins are also available on the public NOMAD service.</p> <p>These plugins are tied to the main project's source code via submodules. They are included in the build and therefore automatically distributed as part of the NOMAD docker images and Python package.</p> <p>To contribute to these plugins, use the respective GitHub projects. See also the list of parsers and the list of built-in plugins. The same rules apply there. A merge request to the main project will also be required to update the submodule.</p> <p>All these submodules are placed in the <code>dependencies</code> directory. After merging or checking out, you have to make sure that the modules are updated to not accidentally commit old submodule commits again. Usually you do the following to check if you really have a clean working directory:</p> <pre><code>git checkout something-with-changes\ngit submodule update --init --recursive\ngit status\n</code></pre>"},{"location":"howto/develop/contrib.html#3rd-party-plugins","title":"3rd-party plugins","text":"<p>Please open an issue, if you want to announce a plugin or contribute a plugin as a potential built-in plugin (i.e. as part of the public NOMAD service).</p>"},{"location":"howto/develop/contrib.html#branches-and-tags","title":"Branches and Tags","text":"<p>On the main GitLab project we use protected and feature branches. You must not commit to protected branches directly (even if you have the rights).</p> <ul> <li> <p><code>develop</code>: a protected branch and the default branch. It contains the latest,   potentially unreleased, features and fixes. This is the main working branch.</p> </li> <li> <p><code>master</code>: a protected branch. Represents the latest stable release (usually what the   current official NOMAD runs on).</p> </li> <li> <p>feature branches: this is where you work. Typically they are automatically (use the   \"Create merge request\" button) named after issues: <code>&lt;issue-number&gt;-&lt;issue-title&gt;</code>.</p> </li> <li> <p><code>vX.X.X</code> or <code>vX.X.XrcX</code>: tags for (pre-)releases.</p> </li> </ul> <p>The <code>develop</code> branch and release tags are automatically synchronized to the GitHub project. Otherwise, this project is mostly the target for pull requests and does not contain other relevant branches.</p>"},{"location":"howto/develop/contrib.html#merge-requests-mr-gitlab","title":"Merge requests (MR, GitLab)","text":""},{"location":"howto/develop/contrib.html#create-the-mr","title":"Create the MR","text":"<p>Ideally, have an issue first and create the merge request (and branch) in the GitLab UI. There is a \"Create merge request\" button on each issue. When done manually, branches should be based on the <code>develop</code> branch and merge request should target <code>develop</code> as well.</p>"},{"location":"howto/develop/contrib.html#commit","title":"Commit","text":"<p>Make sure you follow our code guidelines and set up your IDE to enforce style checks, linting, and static analysis. You can also run tests locally. Try to keep a clean commit history and follow our Git tips.</p> <p>Usually only one person is working on a feature branch. Rebasing, amendments, and force pushes are allowed.</p>"},{"location":"howto/develop/contrib.html#changelog","title":"Changelog","text":"<p>We have an automatically generated changelog in the repository file <code>CHANGELOG.md</code>. This changelog is produced from commit messages and to maintain this file, you need to write commit messages accordingly.</p> <p>To trigger a changelog entry, your commit needs to end with a so-called Git trailer named <code>Changelog</code>. A typical commit message for a changelog entry should look like this:</p> <pre><code>A brief one-line title of the change.\n\nA longer *Markdown*-formatted description of the change. Keep in mind that GitLab will\nautomatically link the changelog entry with this commit and a respective merge requests.\nYou do not need to manually link to any GitLab resources.\n\nThis could span multiple paragraphs. However, keep it short. Documentation should go into\nthe actual documentation, but you should mention breaks in backward compatibility,\ndeprecation of features, etc.\n\nChangelog: Fixed\n</code></pre> <p>The trailer value (<code>Fixed</code> in the example) has to be one of the following values:</p> <ul> <li> <p><code>Fixed</code> for bugfixes.</p> </li> <li> <p><code>Added</code> for new features.</p> </li> <li> <p><code>Changed</code> for general improvements, e.g. updated documentation, refactoring, improving performance, etc.</p> </li> </ul> <p>These categories are consistent with keepachangelog.com. For more information about the changelog generation read the GitLab documentation.</p>"},{"location":"howto/develop/contrib.html#cicd-pipeline-and-review","title":"CI/CD pipeline and review","text":"<p>If you push to your merge requests, GitLab will run an extensive CI/CD pipeline. You need to pay attention to failures and resolve them before review.</p> <p>To review GUI changes, you can deploy your branch to the dev cluster via CI/CD actions.</p> <p>Find someone on the NOMAD developer team to review your merge request and request a review through GitLab. The review should be performed shortly and should not stall the merge request longer than two full work days.</p> <p>The reviewer needs to be able to learn about the merge request and what it tries to achieve. This information can come from:</p> <ul> <li>the linked issue</li> <li>the merge request description (may contain your commit message) and your comments</li> <li>threads that were opened by the author to attach comments to specific places in the code</li> </ul> <p>The reviewer will open threads that need to be solved by the merge request author. If all threads are resolved, you can request another review.</p> <p>For complex merge requests, you can also comment your code and create threads for the reviewer to resolve. This will allow you to explain your changes.</p>"},{"location":"howto/develop/contrib.html#merge","title":"Merge","text":"<p>The branch should be recently rebased with <code>develop</code> to allow a smooth merge:</p> <pre><code>git fetch\ngit rebase origin/develop\ngit push origin &lt;branch&gt; -f\n</code></pre> <p>The merge is usually performed by the merge request author after a positive review.</p> <p>If you could not keep a clean Git history with your commits, squash the merge request. Make sure to edit the commit message when squashing to have a changelog entry.</p> <p>Make sure to delete the branch on merge. The respective issue usually closes itself, due to the references put in by GitLab.</p>"},{"location":"howto/develop/contrib.html#pull-requests-pr-github","title":"Pull requests (PR, GitHub)","text":"<p>You can fork the main NOMAD project and create pull requests following the usual GitHub flow. Make sure to target the <code>develop</code> branch. A team member will pick up your pull request and automatically copy it to GitLab to run the pipeline and potentially perform the merge. This process is made transparent in the pull request discussion. Your commits and your authorship is maintained in this process.</p> <p>Similar rules apply to all the parser and plugin projects. Here, pipelines are run directly on GitHub. A team member will review and potentially merge your pull requests.</p>"},{"location":"howto/develop/contrib.html#tips-for-a-clean-git-history","title":"Tips for a clean Git history","text":"<p>It is often necessary to consider code history to reason about potential problems in our code. This can only be done if we keep a \"clean\" history.</p> <ul> <li> <p>Use descriptive commit messages. Use simple verbs (added, removed, refactored,   etc.) name features and changed components.   Include issue numbers   to create links in GitLab.</p> </li> <li> <p>Learn how to amend to avoid lists of small related commits.</p> </li> <li> <p>Learn how to rebase. Only merging feature branches should create merge commits.</p> </li> <li> <p>Squash commits when merging.</p> </li> <li> <p>Some videos on more advanced Git usage:</p> <ul> <li>Tools &amp; Concepts for Matering Version Control with Git</li> <li>Interactive Rebase, Cherry-Picking, Reflog, Submodules, and more</li> </ul> </li> </ul>"},{"location":"howto/develop/contrib.html#amend","title":"Amend","text":"<p>While working on a feature, there are certain practices that will help us to create a clean history with coherent commits, where each commit stands on its own.</p> <pre><code>git commit --amend\n</code></pre> <p>If you committed something to your own feature branch and then realize by CI that you have some tiny error in it you need to fix, try to amend this fix to the last commit. This will avoid unnecessary tiny commits and foster more coherent single commits. With <code>--amend</code> you are adding changes to the last commit, i.e. editing the last commit. When you push, you need to force it, e.g. <code>git push origin feature-branch --force-with-lease</code>. So be careful, and only use this on your own branches.</p>"},{"location":"howto/develop/contrib.html#rebase","title":"Rebase","text":"<pre><code>git rebase &lt;version-branch&gt;\n</code></pre> <p>Let's assume you work on a bigger feature that takes more time. You might want to merge the version branch into your feature branch from time to time to get the recent changes. In these cases, use <code>rebase</code> and not <code>merge</code>. Rebasing puts your branch commits in front of the merged commits instead of creating a new commit with two ancestors. It moves the point where you initially branched away from the version branch to the current position in the version branch. This will avoid merges, merge commits, and generally leaves us with a more consistent history. You can also rebase before creating a merge request, which allows no-op merges. Ideally, the only real merges that we ever have are between version branches.</p>"},{"location":"howto/develop/contrib.html#squash","title":"Squash","text":"<pre><code>git merge --squash &lt;other-branch&gt;\n</code></pre> <p>When you need multiple branches to implement a feature and merge between them, try to use <code>--squash</code>. Squashing puts all commits of the merged branch into a single commit. It allows you to have many commits and then squash them into one. This is useful if these commits were made just to synchronize between workstations, due to unexpected errors in CI/CD, because you needed a save point, etc. Again the goal is to have coherent commits, where each commit makes sense on its own.</p> <p>Squashing can also be applied on a selection of commits during an interactive rebase.</p>"},{"location":"howto/develop/migrate-to-autoformatter.html","title":"How to migrate existing merge requests to auto-formatted code","text":"<ol> <li>Fetch the latest changes.</li> </ol> <pre><code>git fetch\n</code></pre> <ol> <li>Merge or rebase the changes before the auto-formatter commit. There might be conflicts at this stage, carefully review those changes and resolve the conflicts.</li> </ol> <pre><code>git merge f4b6e09884bb2ea2f9af1129bd5e1bf8f9ef2ffc\n</code></pre> <ol> <li> <p>Install ruff. <pre><code>pip install ruff==0.1.8\n</code></pre></p> </li> <li> <p>Auto-format and commit your code. <pre><code>ruff format . &amp;&amp; git commit -am \"autoformat\"\n</code></pre></p> </li> <li> <p>Merge the formatted code. This shouldn't have any conflicts. <pre><code>git merge 5d5879a4b4f12fec1ebd0d133684190810b6a838 -X ours\n</code></pre></p> </li> <li> <p>Merge the develop branch. There might be conflicts at this stage, carefully review those changes and resolve the conflicts. <pre><code>git merge develop\n</code></pre></p> </li> </ol>"},{"location":"howto/develop/migrate-to-autoformatter.html#optional-step","title":"Optional step:","text":"<ol> <li>Configure your git blame to ignore the formatting changes. <pre><code>git config blame.ignoreRevsFile .git-blame-ignore-revs\n</code></pre></li> </ol>"},{"location":"howto/develop/release.html","title":"How to release a new NOMAD version","text":""},{"location":"howto/develop/release.html#what-is-a-release","title":"What is a release","text":"<p>NOMAD is a public service, a Git repository, a Python package, and a docker image. What exactly is a NOMAD release? It is all of the following:</p> <ul> <li>a version tag on the main NOMAD git project, e.g. <code>v1.3.0</code></li> <li>a gitlab release based on a tag with potential release notes</li> <li>a version of the <code>nomad-lab</code> Python package released to pypi.org, e.g. <code>nomad-lab==1.3.0</code>.</li> <li>a docker image tag, e.g. <code>gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:v1.3.0</code></li> <li>the docker image tag <code>stable</code> points to the image with the latest release tag</li> </ul>"},{"location":"howto/develop/release.html#steps-towards-a-new-release","title":"Steps towards a new release","text":"<ul> <li>Silently create a new version tag in the <code>v1.3.0</code> format.</li> <li>Deploy the build from this tag to the public NOMAD deployments. What deployments are updated might depend on the current needs. But usually the production and test deployment should be updated.</li> <li>Release the Python package to the local gitlab registry. (This will update the NORTH Jupyter image in the next nightly build and most likely effect plugins)</li> <li>Bump the <code>latest</code> docker image tag.</li> <li>For minor and major releases, encourage (Oasis) users to test the public services and the latest docker image for a short trial phase (e.g. 3 days). For patch releases this step should be skipped.</li> <li>Create a gitlab release from the tag with potential release notes. Those notes should also be added to the README.md. It is ok, if the updated README.md is not part of the release itself.</li> <li>Bump the <code>stable</code> docker image tag.</li> <li>Publish the Python package to pypi.org</li> </ul>"},{"location":"howto/develop/release.html#how-to-deal-with-hotfixes","title":"How to deal with hotfixes","text":"<p>This depends on the current <code>develop</code> branch and requires a judgement call. There are two opposing scenarios:</p> <ol> <li> <p>The <code>develop</code> branch only contains minor fixes or fix/features that are not likely to effect the released functionality. In this case, a new release with an increased patch version is the right call.</p> </li> <li> <p>The <code>develop</code> branch adds major refactorings and commits that likely effect the released functionality. In this case, a <code>v1.3.0-hotfix</code> branch should be created. After adding commits with the hotfix, the release process can be applied to the hotfix branch in order to create a <code>v1.3.1</code> release that only contains the hotfixes and not the changes on develop. After the <code>v1.3.1</code> release, the <code>v1.3.0-hotfix</code> branch is merged back into develop. Hotfix branches should not live longer than a week.</p> </li> </ol>"},{"location":"howto/develop/release.html#major-minor-patch-versions","title":"Major, minor, patch versions","text":"<ul> <li> <p>patch: No significant refactorings. Only new/updated features behind disabled feature switches. Bugfixes. Might mark features as deprecated.</p> </li> <li> <p>minor: Might enabled new features by default. Can contain major refactorings (especially if they effect to plugin developers, data stewards etc.). Might finally deprecate features. Should \"basically\" be backwards compatible.</p> </li> <li> <p>major: Breaking changes and will require data migration.</p> </li> </ul> <p>What is a breaking change and what does \"basically\" backwards compatible mean? We develop experimental functionality and often need multiple iterations to get a feature right. This also means that we technically introduce breaking changes far more often than we can issue major releases. It is again a judgement call to decide on major vs minor. The following things would generally not be considered breaking and would be considered backwards compatible:</p> <ul> <li>the breaking change is for a feature that is not enabled by default</li> <li>data migration is necessary for new functionality, but optional for existing functionality</li> <li>it is unlikely that plugins not developed by FAIRmat are effected</li> <li>it is unlikely that data beyond the central NOMAD deployments need to be migrated</li> </ul>"},{"location":"howto/develop/release.html#release-schedule","title":"Release schedule","text":"<p>Patch releases should happen frequently and at least once every other month. Also minor releases should be done semi regular. Important new features or at least bi-annual FAIRmat events should trigger a minor release. Major releases require more involved planning, data migration, and respective instructions and assistance to NOMAD (Oasis) users. They are also political. Therefore, they do not a have a regular schedule.</p> <p>With a one <code>develop</code> branch Git strategy, there might be necessary exceptions to regular patch releases. In general, new features should be protected by feature switches, and should not be an issue. However, major refactorings that might effect multiple components are hard to hide behind a feature switch. In such cases, the release schedule might be put on hold for another month or two.</p>"},{"location":"howto/develop/search.html","title":"How to extend the search","text":""},{"location":"howto/develop/search.html#the-search-indices","title":"The search indices","text":"<p>NOMAD uses Elasticsearch as the underlying search engine. The respective indices are automatically populated during processing and other NOMAD operations. The indices are built from some of the archive information of each entry. These are mostly the sections <code>metadata</code> (ids, user metadata, other \"administrative\" and \"internal\" metadata) and <code>results</code> (a summary of all extracted (meta)data). However, these sections are not indexed verbatim. What exactly and how it is indexed is determined by the Metainfo and the <code>elasticsearch</code> Metainfo extension.</p>"},{"location":"howto/develop/search.html#the-elasticsearch-metainfo-extension","title":"The <code>elasticsearch</code> Metainfo extension","text":"<p>Here is the definition of <code>results.material.elements</code> as an example:</p> <pre><code>class Material(MSection):\n    ...\n    elements = Quantity(\n        type=MEnum(chemical_symbols),\n        shape=[\"0..*\"],\n        default=[],\n        description='Names of the different elements present in the structure.',\n        a_elasticsearch=[\n            Elasticsearch(material_type, many_all=True),\n            Elasticsearch(suggestion=\"simple\")\n        ]\n    )\n</code></pre> <p>Extensions are denoted with the <code>a_</code> prefix as in <code>a_elasticsearch</code>. Since extensions can have all kinds of values, the <code>elasticsearch</code> extension is rather complex and uses the <code>Elasticsearch</code> class.</p> <p>There can be multiple values. Each <code>Elasticsearch</code> instance configures a different part of the index. This means that the same quantity can be indexed multiple time. For example, if you need a text- and a keyword-based search for the same data. Here is a version of the <code>metadata.mainfile</code> definition as another example:</p> <pre><code>mainfile = metainfo.Quantity(\n    type=str, categories=[MongoEntryMetadata, MongoSystemMetadata],\n    description='The path to the mainfile from the root directory of the uploaded files',\n    a_elasticsearch=[\n        Elasticsearch(_es_field='keyword'),\n        Elasticsearch(\n            mapping=dict(type='text', analyzer=path_analyzer.to_dict()),\n            field='path', _es_field='')\n    ]\n)\n</code></pre>"},{"location":"howto/develop/search.html#the-different-indices","title":"The different indices","text":"<p>The first (optional) argument for <code>Elasticsearch</code> determines where the data is indexed. There are three principle places:</p> <ul> <li>the entry index (<code>entry_type</code>, default)</li> <li>the materials index (<code>material_type</code>)</li> <li>the entries within the materials index (<code>material_entry_type</code>)</li> </ul>"},{"location":"howto/develop/search.html#entry-index","title":"Entry index","text":"<p>This is the default and is used even if another (additional) value is given. All data is put into the entry index.</p>"},{"location":"howto/develop/search.html#materials-index","title":"Materials index","text":"<p>This is a separate index from the entry index and contains aggregated material information. Each document in this index represents a material. We use a hash over some material properties (elements, system type, symmetry) to define what a material is and which entries belong to which material.</p> <p>Some parts of the material documents contain the material information that is always the same for all entries of this material. Examples are elements, formulas, symmetry.</p>"},{"location":"howto/develop/search.html#material-entries","title":"Material entries","text":"<p>The materials index also contains entry-specific information that allows to filter materials for the existence of entries with certain criteria. Examples are publish status, user metadata, used method, or property data.</p>"},{"location":"howto/develop/search.html#adding-quantities","title":"Adding quantities","text":"<p>In principle, all quantities could be added to the index, but for convention and simplicity, only quantities defined in the sections <code>metadata</code> and <code>results</code> should be added. This means that if you want to add custom quantities from your parser, for example, you will also need to customize the results normalizer to copy or reference parsed data.</p>"},{"location":"howto/develop/search.html#the-search-api","title":"The search API","text":"<p>The search API does not have to change. It automatically supports all quantities with the <code>elasticsearch</code> extension. The keys that you can use in the API are the Metainfo paths of the respective quantities, e.g. <code>results.material.elements</code> or <code>mainfile</code> (note that the <code>metadata.</code> prefix is always omitted). If there are multiple <code>elasticsearch</code> annotations for the same quantity, all but one define a <code>field</code> parameter, which is added to the quantity path, e.g. <code>mainfile.path</code>.</p>"},{"location":"howto/develop/search.html#the-search-web-interface","title":"The search web interface","text":"<p>Attention</p> <pre><code>Coming soon ...\n</code></pre>"},{"location":"howto/develop/setup.html","title":"How to get started in development","text":"<p>This is a step-by-step guide to get started with NOMAD development. You will clone all sources, set up a Python and Node.js environment, install all necessary dependencies, run the infrastructure in development mode, learn to run the test suites, and set up Visual Studio Code for NOMAD development.</p> <p>This is not about working with the NOMAD Python package <code>nomad-lab</code>. You can find its documentation here.</p>"},{"location":"howto/develop/setup.html#clone-the-sources","title":"Clone the sources","text":"<p>If not already done, you should clone NOMAD. If you have an account at the MPDCF Gitlab, you can clone with the SSH URL:</p> <pre><code>git clone git@gitlab.mpcdf.mpg.de:nomad-lab/nomad-FAIR.git nomad\n</code></pre> <p>Otherwise, clone using the HTTPS URL:</p> <pre><code>git clone https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-FAIR.git nomad\n</code></pre> <p>Then change directory to <code>nomad</code></p> <pre><code>cd nomad\n</code></pre> <p>There are several branches in the repository. The <code>master</code> branch contains the latest released version, but there is also a <code>develop</code> (new features) and <code>release</code> branch (hotfixes). There are also tags for each version called <code>vX.X.X</code>. Check out the branch you want to work on.</p> <pre><code>git checkout develop\n</code></pre> <p>The development branches are protected and you should create a new branch including your changes.</p> <pre><code>git checkout -b &lt;my-branch-name&gt;\n</code></pre> <p>This branch can be pushed to the repo, and then later may be merged to the relevant branch.</p>"},{"location":"howto/develop/setup.html#install-submodules","title":"Install submodules","text":"<p>Parts of the NOMAD software, such as parsers, are maintained in separate Git repositories. These are then connected to the main repository as Git submodules. To clone and initialize these submodules, run:</p> <pre><code>git submodule update --init\n</code></pre>"},{"location":"howto/develop/setup.html#installation","title":"Installation","text":""},{"location":"howto/develop/setup.html#set-up-a-python-environment","title":"Set up a Python environment","text":"<p>The NOMAD code currently requires Python 3.11. You should work in a Python virtual environment.</p>"},{"location":"howto/develop/setup.html#pyenv","title":"Pyenv","text":"<p>If your host machine has an older version installed, you can use pyenv to use Python 3.11 in parallel with your system's Python.</p>"},{"location":"howto/develop/setup.html#virtualenv","title":"Virtualenv","text":"<p>Create a virtual environment. It allows you to keep NOMAD and its dependencies separate from your system's Python installation. Make sure that the virtual environment is based on Python 3.11. Use either the built-in <code>venv</code> module (see example) or virtualenv.</p> <pre><code>python3 -m venv .pyenv\nsource .pyenv/bin/activate\n</code></pre>"},{"location":"howto/develop/setup.html#conda","title":"Conda","text":"<p>If you are a conda user, there is an equivalent, but you have to install <code>pip</code> and the right Python version while creating the environment.</p> <pre><code>conda create --name nomad_env pip python=3.11\nconda activate nomad_env\n</code></pre> <p>To install libmagick for Conda, you can use (other channels might also work):</p> <pre><code>conda install -c conda-forge --name nomad_env libmagic\n</code></pre>"},{"location":"howto/develop/setup.html#upgrade-pip","title":"Upgrade pip","text":"<p>Make sure you have the most recent version of <code>pip</code>:</p> <pre><code>pip install --upgrade pip\n</code></pre>"},{"location":"howto/develop/setup.html#install-missing-system-libraries-eg-on-windows-macos","title":"Install missing system libraries (e.g. on Windows, MacOS)","text":"<p>Even though the NOMAD infrastructure is written in Python, there are C libraries  required by some of our Python dependencies. Specifically, the libmagic library,  which allows determining the MIME type of files, and the hdf5 library, which is  essential for handling HDF5 files, must be installed on most Unix/Linux systems. </p> <p>The absence of these libraries can lead to issues during installation or runtime.</p> <p>For macOS:</p> <pre><code>brew install hdf5 libmagic file-formula\n</code></pre> <p>For Windows (pre-compiled binaries for <code>hdf5</code> are included in the dependencies):</p> <p>-libmagic: We include python-magic-bin as a dependency for Windows users.  If you encounter an error such as NameError: name '_compressions' is not defined, try uninstalling and reinstalling the library:</p> <pre><code>pip uninstall python-magic-bin\npip install python-magic-bin\n</code></pre> <p>You can confirm that the magic library is correctly installed by running:</p> <pre><code>python -c \"import magic\"\n</code></pre>"},{"location":"howto/develop/setup.html#install-nomad","title":"Install NOMAD","text":"<p>The following command can be used to install all dependencies of all submodules and NOMAD itself.</p> <pre><code>./scripts/setup_dev_env.sh\n</code></pre> Installation details <p>Here is more detailed rundown of the installation steps.</p> <p>First we ensure that all submodules are up-to-date:</p> <pre><code>git submodule update --init --recursive\n</code></pre> <p>Previous build is cleaned:</p> <pre><code>rm -rf nomad/app/static/docs\nrm -rf nomad/app/static/gui\nrm -rf site\n</code></pre> <p>We use uv to install the packages. You can install uv using <code>pip install uv</code>.</p> <p>Next we install the <code>nomad</code> package itself (including all extras). The <code>-e</code> option will install NOMAD with symbolic links that allow you to change the code without having to reinstall after each change.</p> <p>The -c flag restricts the installation of dependencies to the versions specified in the provided requirements file, ensuring that only those versions are installed.</p> <pre><code>uv pip install -e .[parsing,infrastructure,dev] -c requirements-dev.txt\n</code></pre> <p>Install \"default\" plugins. TODO: This can be removed once we have proper proper distribution <pre><code>uv pip install -r requirements-plugins.txt\n</code></pre></p> <p>Next we build the documentation.</p> <pre><code>sh scripts/generate_docs_artifacts.sh\nmkdocs build\nmkdir -p nomad/app/static/docs\ncp -r site/* nomad/app/static/docs\n</code></pre> <p>The NOMAD GUI requires a static <code>.env</code> file, which can be generated with:</p> <pre><code>python -m nomad.cli dev gui-env &gt; gui/.env.development\n</code></pre> <p>This file includes some of the server details needed so that the GUI can make the initial connection properly. If, for example, you change the server address in your NOMAD configuration file, it will be necessary to regenerate this <code>.env</code> file. In production this file will be overridden.</p> <p>In addition, you have to do some more steps to prepare your working copy to run all the tests, see below.</p>"},{"location":"howto/develop/setup.html#run-the-infrastructure","title":"Run the infrastructure","text":""},{"location":"howto/develop/setup.html#install-docker","title":"Install Docker","text":"<p>You need to install Docker. Docker nowadays comes with Docker Compose (<code>docker compose</code>) built-in. Prior, you needed to install the standalone Docker Compose (<code>docker-compose</code>).</p>"},{"location":"howto/develop/setup.html#run-required-3rd-party-services","title":"Run required 3rd party services","text":"<p>To run NOMAD, some 3rd party services are needed</p> <ul> <li>Elasticsearch: NOMAD's search and analytics engine</li> <li>MongoDB: used to store processing state</li> <li>RabbitMQ: a task queue used to distribute work in a cluster</li> </ul> <p>All 3rd party services should be run via <code>docker compose</code> (see below). Keep in mind that <code>docker compose</code> configures all services in a way that mirrors the configuration of the Python code in <code>nomad/config.py</code> and the GUI config in <code>gui/.env.development</code>.</p> <p>The default virtual memory for Elasticsearch will likely be too low. On Linux, you can run the following command as root:</p> <pre><code>sysctl -w vm.max_map_count=262144\n</code></pre> <p>To set this value permanently, see here. Then you can run all services with:</p> <pre><code>cd ops/docker-compose/infrastructure\ndocker compose up -d elastic mongo rabbitmq\ncd ../../..\n</code></pre> <p>If your system almost ran out of disk space, Elasticsearch enforces a read-only index block (read more), but after clearing up the disk space you need to reset it manually using the following command:</p> <pre><code>curl -XPUT -H \"Content-Type: application/json\" http://localhost:9200/_all/_settings -d '{\"index.blocks.read_only_allow_delete\": false}'\n</code></pre> <p>Note that the Elasticsearch service has a known problem in quickly hitting the virtual memory limits of your OS. If you experience issues with the Elasticsearch container not running correctly or crashing, try increasing the virtual memory limits as shown here.</p> <p>To shut down everything, just <code>ctrl-c</code> the running output. If you started everything in deamon mode (<code>-d</code>) use:</p> <pre><code>docker compose down\n</code></pre> <p>Usually these services are used only by NOMAD, but sometimes you also need to check something or do some manual steps. You can access MongoDB and Elasticsearch via your preferred tools. Just make sure to use the right ports.</p>"},{"location":"howto/develop/setup.html#run-nomad","title":"Run NOMAD","text":""},{"location":"howto/develop/setup.html#nomadyaml","title":"<code>nomad.yaml</code>","text":"<p>Before you run NOMAD for development purposes, you should configure it to use the test realm of our user management system. By default, NOMAD will use the <code>fairdi_nomad_prod</code> realm. Create a <code>nomad.yaml</code> file in the root folder:</p> <pre><code>keycloak:\n  realm_name: fairdi_nomad_test\n</code></pre> <p>You might also want to exclude some of the default plugins, or only include the plugins you'll need. Especially plugins with slower start-up and import times due to instantiation of large schemas (e.g. nexus create couple thousand definitions for 70+ applications) can often be excluded:</p> <pre><code>plugins:\n  exclude:\n    - parsers/nexus\n</code></pre> <p>Note that this will lead to failing tests for the excluded plugins.</p>"},{"location":"howto/develop/setup.html#app-and-worker","title":"App and worker","text":"<p>NOMAD consists of the NOMAD app/API, a worker, and the GUI. You can run the app and the worker with the NOMAD CLI. These commands will run the services and display their log output. You should open them in separate shells as they run continuously. They will not watch code changes and you have to restart manually.</p> <pre><code>nomad admin run app\n</code></pre> <pre><code>nomad admin run worker\n</code></pre> <p>Or both together in one process:</p> <pre><code>nomad admin run appworker\n</code></pre> <p>On MacOS you might run into multiprocessing errors. That can be solved as described here.</p> <p>The app will run at port 8000 by default.</p> <p>To run the worker directly with Celery, do (from the root)</p> <pre><code>celery -A nomad.processing worker -l info\n</code></pre> <p>If you run the GUI on its own (e.g. with the React dev server below), you also need to start the app manually. The GUI and its dependencies run on Node.js and the Yarn dependency manager. Read their documentation on how to install them for your platform.</p> <pre><code>cd gui\nyarn\nyarn start\n</code></pre> <p>Note that the current codebase requires Node.js version 20. </p>"},{"location":"howto/develop/setup.html#jupyterhub","title":"JupyterHub","text":"<p>NOMAD also has a built-in JupyterHub that is used to launch remote tools (e.g. Jupyter notebooks).</p> <p>To run JupyterHub, some additional configuration might be necessary.</p> <pre><code>north:\n  hub_connect_ip: 'host.docker.internal'\n  jupyterhub_crypt_key: '&lt;crypt key&gt;'\n</code></pre> <p>On Windows system, you might have to activate further specific functionality:</p> <pre><code>north:\n  hub_connect_ip: 'host.docker.internal'\n  hub_connect_url: 'http://host.docker.internal:8081'\n  windows: true\n  jupyterhub_crypt_key: '&lt;crypt key&gt;'\n</code></pre> <ul> <li> <p>If you are not on Linux, you need to configure how JupyterHub can reach your host   network from docker containers. For Windows and MacOS you need to set <code>hub_connect_ip</code>   to <code>host.docker.internal</code>. For Linux you can leave it out and use the default   <code>172.17.0.1</code>, unless you changed your docker configuration.</p> </li> <li> <p>You have to generate a <code>crypt key</code> with <code>openssl rand -hex 32</code>.</p> </li> <li> <p>You might need to install   configurable-http-proxy.</p> </li> </ul> <p>The <code>configurable-http-proxy</code> comes as a Node.js package. See Node.js for how to install <code>npm</code>. The proxy can be globally installed with:</p> <pre><code>npm install -g configurable-http-proxy\n</code></pre> <p>JupyterHub is a separate application. You can run JuypterHub similar to the other part:</p> <pre><code>nomad admin run hub\n</code></pre> <p>To run JupyterHub directly, do (from the root)</p> <pre><code>jupyterhub -f nomad/jupyterhub_config.py --port 9000\n</code></pre>"},{"location":"howto/develop/setup.html#running-tests","title":"Running tests","text":""},{"location":"howto/develop/setup.html#backend-tests","title":"Backend tests","text":"<p>To run the tests some additional settings and files are necessary that are not part of the codebase.</p> <p>You have to provide static files to serve the docs and NOMAD distribution:</p> <pre><code>./scripts/generate_docs_artifacts.sh\nrm -rf site &amp;&amp; mkdocs build &amp;&amp; mv site nomad/app/static/docs\n</code></pre> <p>You need to have the infrastructure partially running: <code>elastic</code>, <code>mongo</code>, <code>rabbitmq</code>. The rest should be mocked or provided by the tests. Make sure that you do not run any worker, as they will fight for tasks in the queue. To start the infrastructure and run the tests, use:</p> <pre><code>cd ops/docker-compose/infrastructure\ndocker compose up -d elastic mongo rabbitmq\ncd ../../..\npytest -sv tests\n</code></pre> <p>Note</p> <p>Some of these tests will fail because a few large files are not included in the Git repository. You may ignore these for local testing, they are still checked by the CI/CD pipeline:</p> <pre><code>FAILED tests/archive/test_archive.py::test_read_springer - AttributeError: 'NoneType' object has no attribute 'seek'\nFAILED tests/normalizing/test_material.py::test_material_bulk - assert None\nFAILED tests/normalizing/test_system.py::test_springer_normalizer - IndexError: list index out of range\n</code></pre> <p>If you excluded plugins in your NOMAD config, then those tests will also fail.</p> <p>We use Ruff and Mypy to maintain code quality. Additionally, we recommend installing the Ruff plugins for your code editor to streamline the process. To execute Ruff and Mypy from the command line, you can utilize the following command: <pre><code>nomad dev qa --skip-tests\n</code></pre></p> <p>We use ruff as a linter and as an autoformatter. If you only want to lint your code, you can run: <pre><code>ruff check .\n</code></pre></p> <p>To format your code you can run: <pre><code>ruff format .\n</code></pre></p> <p>To run all tests and code QA:</p> <pre><code>nomad dev qa\n</code></pre> <p>This mimics the tests and checks that the GitLab CI/CD will perform.</p> <p>If you are migrating an old merge request to a formatted one, please see find the migration guide here</p>"},{"location":"howto/develop/setup.html#custom-pytest-options","title":"Custom pytest options","text":""},{"location":"howto/develop/setup.html#-celery-inspect-timeout","title":"<code>--celery-inspect-timeout</code>","text":"<p>To ensure that all tests are independent despite reusing the same queue and workers, the <code>worker</code> fixture cleans up all running tasks after the test. This involves by default a timeout of one second, which accumulates over all tests using that fixture. In one local development environment this made a difference of about 14 min vs. 7 min without the timeout.</p> <p>If you want to speed up your local testing, you can use <code>pytest --celery-inspect-timeout 0.1</code> to shorten the timeout to a tenth of a second. Be aware that this might leave tasks running, which can affect later tests.</p>"},{"location":"howto/develop/setup.html#-fixture-filters","title":"<code>--fixture-filters</code>","text":"<p>You may want to run only tests that use a specific fixture, e.g. if you are editing that fixture. For example, to run only tests with the <code>worker</code> fixture, use <code>pytest --fixture-filters worker</code>. If you list more than one, all of them must be requested by the test to be included.</p> <p>You can also negate a fixture by prefixing its name with <code>!</code>. For example, to run all tests that do not depend on the <code>worker</code> fixture, use <code>pytest --fixture-filters '!worker'</code> (quotes are needed for <code>!</code>).</p> <p>Note that if <code>test1</code> depends on <code>fixture1</code>, and <code>fixture1</code> depends on <code>fixture2</code>, then <code>test1</code> also depends on <code>fixture2</code>, even though it was not explicitly listed as a parameter.</p>"},{"location":"howto/develop/setup.html#frontend-tests","title":"Frontend tests","text":"<p>We use <code>testing-library</code> to implement our GUI tests and <code>testing-library</code> itself uses <code>Jest</code> to run the tests. Tests are written in <code>*.spec.js</code> files that accompany the implementation. Tests should focus on functionality, not on implementation details: <code>testing-library</code> is designed to enforce this kind of testing.</p> <p>Note</p> <p>When testing HTML output, the elements are rendered using jsdom: this is not completely identical to using an actual browser (e.g. does not support WebGL), but in practice is realistic enough for the majority of the test.</p>"},{"location":"howto/develop/setup.html#test-structure","title":"Test structure","text":"<p>We have adopted a <code>pytest</code>-like structure for organizing the test utilities: each source code folder may contain a <code>conftest.js</code> file that contains utilities that are relevant for testing the code in that particular folder. These utilities can usually be placed into the following categories:</p> <ul> <li> <p>Custom renders: When testing React components, the   <code>render</code> function   is used to display them on the test DOM. Typically your components require   some parts of the infrastructure to work properly, which is achieved by   wrapping your component with other components that provide a context. Custom   render functions can do this automatically for you, e.g. the default render   as exported from <code>src/components/conftest.js</code> wraps your components with an   infrastructure that is very similar to the production app. See   here   for more information.</p> </li> <li> <p>Custom queries: See   here   for more information.</p> </li> <li> <p>Custom expects: These are reusable functions that perform actual tests using   the <code>expect</code> function. Whenever the same tests are performed by several   <code>*.spec.js</code> files, you should formalize these common tests into an   <code>expect*</code> function and place it in a relevant <code>conftest.js</code> file.</p> </li> </ul> <p>Often your components will need to communicate with the API during tests. One should generally avoid using manually created mocks for the API traffic, and instead prefer using API responses that originate from an actual API call during testing. Manually created mocks require a lot of manual work in creating them and keeping them up-to-date and true integration tests are impossible to perform without live communication with an API. In order to simplify the API communication during testing, you can use the <code>startAPI</code>+<code>closeAPI</code> functions, that will prepare the API traffic for you. A simple example could look like this:</p> <pre><code>import React from 'react'\nimport { waitFor } from '@testing-library/dom'\nimport { startAPI, closeAPI, screen } from '../../conftest'\nimport { renderSearchEntry, expectInputHeader } from '../conftest'\n\ntest('periodic table shows the elements retrieved through the API', async () =&gt; {\n  startAPI('&lt;state_name&gt;', '&lt;snapshot_name&gt;')\n  renderSearchEntry(...)\n  expect(...)\n  closeAPI()\n})\n</code></pre> <p>Here the important parameters are:</p> <ul> <li> <p><code>&lt;state_name&gt;</code>: Specifies an initial backend configuration for this test. These   states are defined as Python functions that are stored in   <code>nomad-FAIR/tests/states</code>, example given below. These functions may, for example,   prepare several uploads entries, datasets, etc. for the test.</p> </li> <li> <p><code>&lt;snapshot_name&gt;</code>: Specifies a filepath for reading/recording pre-recorded API   traffic.</p> </li> </ul> <p>An example of a simple test state could look like this:</p> <pre><code>from nomad import infrastructure\nfrom nomad.utils import create_uuid\nfrom nomad.utils.exampledata import ExampleData\n\ndef search():\n    infrastructure.setup()\n    main_author = infrastructure.user_management.get_user(username=\"test\")\n    data = ExampleData(main_author=main_author)\n    upload_id = create_uuid()\n    data.create_upload(upload_id=upload_id, published=True, embargo_length=0)\n    data.create_entry(\n        upload_id=upload_id,\n        entry_id=create_uuid(),\n        mainfile=\"test_content/test_entry/mainfile.json\",\n        results={\n            \"material\": {\"elements\": [\"C\", \"H\"]},\n            \"method\": {},\n            \"properties\": {}\n        }\n    )\n    data.save()\n</code></pre> <p>When running in the online mode (see below), this function will be executed in order to prepare the application backend. The <code>closeAPI</code> function will handle cleaning the test state between successive <code>startAPI</code> calls: it will completely wipe out MongoDB, Elasticsearch and the upload files.</p>"},{"location":"howto/develop/setup.html#running-frontend-tests","title":"Running frontend tests","text":"<p>The tests can be run in two different modes. Offline testing uses pre-recorded files to mock the API traffic during testing. This allows one to run tests more quickly without a server. During online testing, the tests perform calls to a running server where a test state has been prepared. This mode can be used to perform integration tests but also to record the snapshot files needed by the offline testing.</p>"},{"location":"howto/develop/setup.html#offline-testing","title":"Offline testing","text":"<p>This is the way our CI pipeline runs the tests and should be used locally, e.g. whenever you wish to reproduce pipeline errors or when your tests do not involve any API traffic.</p> <ol> <li>Ensure that the GUI artifacts are up-to-date:</li> </ol> <pre><code>./scripts/generate_gui_test_artifacts.sh\n</code></pre> <p>As snapshot tests do not connect to the server, the artifacts cannot be    fetched dynamically from the server and static files need to be used    instead.</p> <ol> <li>Run <code>yarn test</code> to run the whole suite or <code>yarn test [&lt;filename&gt;]</code> to run a    specific test.</li> </ol>"},{"location":"howto/develop/setup.html#online-testing","title":"Online testing","text":"<p>When you wish to record API traffic for offline testing, or to perform integration tests, you will need to have a server running with the correct configuration. To do this, follow these steps:</p> <ol> <li> <p>Have the docker infrastructure running: <code>docker compose up</code></p> </li> <li> <p>Have the <code>nomad appworker</code> running with the config found in    <code>gui/tests/nomad.yaml</code>:    <code>export NOMAD_CONFIG=gui/tests/nomad.yaml; nomad admin run appworker</code></p> </li> <li> <p>Activate the correct Python virtual environment before running the tests    with Yarn (Yarn will run the Python functions that prepare the state).</p> </li> <li> <p>Run the tests with <code>yarn test-record [&lt;filename&gt;]</code> if you wish to record a    snapshot file or <code>yarn test-integration [&lt;filename&gt;]</code> if you want the    perform the test without any recording.</p> </li> </ol>"},{"location":"howto/develop/setup.html#build-the-docker-image","title":"Build the Docker image","text":"<p>Normally the Docker image is build via a CI/CD pipeline that is run when pushing commits to NOMAD's GitLab at MPCDF. These images are distributed via NOMAD's GitLab container registry. For most purposes you would use these automatically-built images.</p> <p>If you want to build a custom image, e.g. to be used in your NOMAD Oasis, you can run the NOMAD Docker build manually. From the cloned project root run:</p> <pre><code>docker build -t &lt;image-name&gt;:&lt;image-tag&gt; .\n</code></pre> <p>This will build the normal image intended for production use. There are other build targets: <code>dev_python</code> and <code>dev_node</code>. Especially <code>dev_python</code> might be interesting for debugging purposes as it contains all sources and dev dependencies. You can build specific targets with:</p> <pre><code>docker build --target dev_python -t &lt;image-name&gt;:&lt;image-tag&gt; .\n</code></pre> <p>If you want to build an image directly from a remote Git repository (e.g. for a specific <code>branch</code>), run:</p> <pre><code>DOCKER_BUILDKIT=1 docker build --build-arg BUILDKIT_CONTEXT_KEEP_GIT_DIR=1 --pull -t &lt;image-name&gt;:&lt;image-tag&gt; https://github.com/nomad-coe/nomad.git#&lt;branch&gt;\n</code></pre> <p>The BuildKit parametrization ensures that the <code>.git</code> directory is available in the Docker build context. NOMAD's build process requires the <code>.git</code> folder to determine the package version from version tags in the repository.</p> <p>The build process installs a substantial amount of dependencies and requires multiple Docker images for various build stages. Make sure that Docker has at least 20 GB of storage available.</p>"},{"location":"howto/develop/setup.html#setup-your-ide","title":"Setup your IDE","text":"<p>The documentation section for development guidelines (see below) provide details on how the code is organized, tested, formatted, and documented. To help you meet these guidelines, we recommend to use a proper IDE for development and ditch any Vim/Emacs (mal-)practices.</p> <p>We strongly recommend that all developers use Visual Studio Code (VS Code). (This is a completely different product than Visual Studio.) It is available for free for all major platforms here.</p> <p>You should launch and run VS Code directly from the project's root directory. The source code already contains settings for VS Code in the <code>.vscode</code> directory. The settings contain the same setup for style checks, linter, etc. that is also used in our CI/CD pipelines. In order to actually use the these features, you have to make sure that they are enabled in your own User settings:</p> <pre><code>\"python.linting.mypyEnabled\": true,\n\"python.testing.pytestEnabled\": true,\n\"[python]\": {\n  \"editor.formatOnSave\": true,\n  \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n}\n</code></pre> <p>The settings also include a few launch configuration for VS Code's debugger. You can create your own launch configs in <code>.vscode/launch.json</code> (also in <code>.gitignore</code>).</p> <p>The settings expect that you have installed a Python environment at <code>.pyenv</code> as described in this tutorial (see above).</p>"},{"location":"howto/manage/eln.html","title":"How to use ELNs","text":"<p>This guide describes how to manually create entries and enter information via ELNs (electronic lab notebooks). NOMAD ELNs allow you to acquire consistently structured data from users to augment uploaded files.</p> <p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"howto/manage/eln.html#create-a-basic-eln-entry","title":"Create a basic ELN entry","text":"<p>Go to <code>PUBLISH</code> / <code>Uploads</code>. Here you can create an upload with the <code>CREATE A NEW UPLOAD</code> button. This will bring you to the upload page.</p> <p>Click the <code>CREATE ENTRY</code> button. This will bring-up a dialog to choose an ELN schema. All ELNs (as any entry in NOMAD) needs to follow a schema. You can choose from uploaded custom schemas or NOMAD built-in schemas. You can choose the <code>Basic ELN</code> to create a simple ELN entry.</p> <p>The name of your ELN entry, will be the filename for your ELN without the <code>.archive.json</code> ending that will be added automatically. You can always find and download your ELNs on the <code>FILES</code> tab.</p> <p>The <code>Basic ELN</code> offers you simple fields for a name, tags, a date/time, and a rich text editor to enter your notes.</p>"},{"location":"howto/manage/eln.html#add-your-own-eln-schema","title":"Add your own ELN schema","text":"<p>To make NOMAD ELNs more useful, you can define your own schema to create you own data fields, create more subsections, reference other entries, and much more.</p> <p>You should have a look at our ELN example upload. Go to <code>PUBLISH</code> / <code>Uploads</code> and click the <code>ADD EXAMPLE UPLOADS</code> button. The <code>Electronic Lab Notebook</code> example, will contain a schema and entries that instantiate different parts of the schema. The *ELN example sample (<code>sample.archive.json</code>) demonstrates what you can do.</p> <p>Follow the How-to write a schema and How-to define ELN guides to create you own customized of ELNs.</p>"},{"location":"howto/manage/eln.html#integration-of-third-party-elns","title":"Integration of third-party ELNs","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p> <p>NOMAD offers integration with third-party ELN providers, simplifying the process of connecting and interacting with external platforms. Three main external ELN solutions that are integrated into NOMAD are: elabFTW, Labfolder and chemotion. The process of data retrieval and data mapping onto NOMAD's schema varies for each of these third-party ELN provider as they inherently allow for certain ways of communicating with their database. Below you can find a How-to guide on importing your data from each of these external repositories.</p>"},{"location":"howto/manage/eln.html#elabftw-integration","title":"elabFTW integration","text":"<p>elabFTW is part of the ELN Consortium and supports exporting experimental data in ELN file format. ELNFileFormat is a zipped file that contains metadata of your elabFTW project along with all other associated data of your experiments.</p> <p>How to import elabFTW data into NOMAD:</p> <p>Go to your elabFTW experiment and export your project as <code>ELN Archive</code>. Save the file to your filesystem under your preferred name and location (keep the <code>.eln</code> extension intact). To parse your ebalFTW data into NOMAD, go to the upload page of NOMAD and create a new upload. In the <code>overview</code> page, upload your exported file (either by drag-dropping it into the click or drop files box or by navigating to the path where you stored the file). This causes triggering NOMAD's parser to create as many new entries in this upload as there are experiments in your elabFTW project.</p> <p>You can inspect the parsed data of each of your entries (experiments) by going to the DATA tab of each Entry page. Under Entry column, click on data section. Now a new lane titled <code>ElabFTW Project Import</code> should be visible. Under this section, (some of) the metadata of your project is listed. There two subsections: 1) experiment_data, and 2) experiment_files.</p> <p>experiment_data section contains detailed information of the given elabFTW experiment, such as links to external resources and extra fields. experiment_files section is a list of subsections containing metadata and additional info of the files associated with the experiment.</p>"},{"location":"howto/manage/eln.html#labfolder-integration","title":"Labfolder integration","text":"<p>Labfolder provides API endpoints to interact with your ELN data. NOMAD makes API calls to retrieve, parse and map the data from your Labfolder instance/database to a NOMAD's schema. To do so, the necessary information are listed in the table below:</p> <p>project_url:         The URL address to the Labfolder project. it should follow this pattern:         'https://your-labfolder-server/eln/notebook#?projectIds=your-project-id'. This is used to setup         the server and initialize the NOMAD schema.</p> <p>labfolder_email:         The email (user credential) to authenticate and login the user. Important Note: this         information is discarded once the authentication process is finished.</p> <p>password:         The password (user credential) to authenticate and login the user. Important Note: this         information is discarded once the authentication process is finished.</p> <p>How to import Labfolder data into NOMAD:</p> <p>To get your data transferred to NOMAD, first go to NOMAD's upload page and create a new upload. Then click on <code>CREATE ENTRY</code> button. Select a name for your Entry and pick <code>Labfolder Project Import</code> from the <code>Built-in schema</code> dropdown menu. Then click on <code>CREATE</code>. This creates an Entry where you can insert your user information. Fill the <code>Project url</code>, <code>Labfolder email</code> and <code>password</code> fields. Once completed, click on the <code>save icon</code> in the top-right corner of the screen. This triggers NOMAD's parser to populate the schema of current ELN. Now the metadata and all files of your Labfolder project should be populated in this Entry.</p> <p>The <code>elements</code> section lists all the data and files in your projects. There are 6 main data types returned by Labfolder's API: <code>DATA</code>, <code>FILE</code>, <code>IMAGE</code>, <code>TABLE</code>, <code>TEXT</code> and <code>WELLPLATE</code>. <code>DATA</code> element is a special Labfolder element where the data is structured in JSON format. Every data element in NOMAD has a special <code>Quantity</code> called <code>labfolder_data</code> which is a flattened and aggregated version of the data content. <code>IMAGE</code> element contains information of any image stored in your Labfolder project. <code>TEXT</code> element contains data of any text field in your Labfodler project.</p>"},{"location":"howto/manage/eln.html#chemotion-integration","title":"Chemotion integration","text":"<p>NOMAD supports importing your data from Chemotion repository via <code>chemotion</code> parser. The parser maps your data that is structured under chemotion schema, into a predefined NOMAD schema. From your Chemotion repo, you can export your entire data as a zip file which then is used to populate NOMAD schema.</p> <p>How to import Chemotion data into NOMAD:</p> <p>Go to your Chemotion repository and export your project. Save the file to your filesystem under your preferred name and location (<code>your_file_name.zip</code>). To get your data parsed into NOMAD, go to the upload page of NOMAD and create a new upload. In the <code>overview</code> page, upload your exported file (either by drag-dropping it into the click or drop files box or by navigating to the path where you stored the file). This causes triggering NOMAD's parser to create one new Entry in this upload.</p> <p>You can inspect the parsed data of each of this new Entry by navigating to the DATA tab of the current Entry page. Under Entry column, click on data section. Now a new lane titled <code>Chemotion Project Import</code> should be visible. Under this section, (some of) the metadata of your project is listed. Also, there are various (sub)sections which are either filled depending on whether your datafile contains information on them.</p> <p>If a section contains an image (or attachment) it is appended to the same section under <code>file</code> Quantity.</p>"},{"location":"howto/manage/eln.html#openbis-integration","title":"Openbis integration","text":"<p>Openbis provides API endpoints to interact with your ELN data. NOMAD makes API calls to retrieve, parse, and map the data from your Openbis instance/database to NOMAD's schema. The necessary information is listed in the table below:</p> <ul> <li>project_url: The URL address to the Openbis project. It should follow this pattern: <code>https://openbis.example.com</code>.   This is used to set up the server and initialize the NOMAD schema.</li> <li>username: The username (user credential) to authenticate and log in the user. Important Note: this information   is discarded once the authentication process is finished.</li> <li>password: The password (user credential) to authenticate and log in the user. Important Note: this information   is discarded once the authentication process is finished.</li> </ul>"},{"location":"howto/manage/eln.html#how-to-import-openbis-data-into-nomad","title":"How to Import Openbis Data into NOMAD","text":"<p>To get your data transferred to NOMAD, follow these steps:</p> <ol> <li>Go to NOMAD's upload page and create a new upload.</li> <li>Click on the <code>CREATE ENTRY</code> button.</li> <li>Select a name for your entry and pick <code>Openbis Project Import</code> from the <code>Built-in schema</code> dropdown menu.</li> <li>Click on <code>CREATE</code>. This creates an entry where you can insert your user information.</li> <li>Fill in the <code>project url</code>, <code>username</code>, and <code>password</code> fields.</li> <li>Once completed, click on the save icon in the top-right corner of the screen. This triggers NOMAD's parser to    populate the schema of the current ELN. Now, the metadata and all files of your Openbis project should be populated    in this entry.</li> </ol> <p>The normalizer will search for all entries in your Openbis project and attempt to import them one by one.</p>"},{"location":"howto/manage/eln.html#under-development","title":"Under Development","text":"<p>The integration of third-party ELNs suite is planned to be moved to a new plug-in mechanism to allow for a smoother interface for interacting with other ELN providers.</p>"},{"location":"howto/manage/explore.html","title":"How to explore data","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>While we are still working on this, please use our video tutorial as a starting point:</p>"},{"location":"howto/manage/north.html","title":"How to use NORTH","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"howto/manage/upload.html","title":"How to upload and publish data for supported formats","text":"<p>This guide describes how to upload data in NOMAD supported file formats. You find a list of supported formats on top of each upload page, see below.</p>"},{"location":"howto/manage/upload.html#preparing-files","title":"Preparing files","text":"<p>You can upload files one by one, but you can also provider larger <code>.zip</code> or <code>.tar.gz</code> archive files, if this is easier to you. Also the file upload via frp or command line with curl or with wget generates an archive files. The specific layout of these files is up to you. NOMAD will simply extract them and consider the whole directory structure within.</p>"},{"location":"howto/manage/upload.html#create-an-upload-and-add-files","title":"Create an upload and add files","text":"<p>Open NOMAD and log in; if you don't have a NOMAD account, please create one.</p> <p>Go to <code>PUBLISH</code> / <code>Uploads</code>. Here you can create an upload with the <code>CREATE A NEW UPLOAD</code> button. This will bring you to the upload page.</p> <p>Before you start, make sure that the size of your data does not exceed the upload limits. If it does, please contact us.</p> <p>You can drop your files on (or click) the <code>CLICK OR DROP FILES</code> button. On top you will see a list of supported file formats and details on the files to upload. You can also go to the <code>FILES</code> tab. Here you can create directories and drop files into directories.</p>"},{"location":"howto/manage/upload.html#processing-files","title":"Processing files","text":"<p>NOMAD interprets your files. It checks each file and recognizes the main output file of the supported codes. NOMAD creates an entry for this mainfile that represents the respective data of this code run, experiment, etc.</p> <p>While you can browse all files of an upload from its upload page, NOMAD only allows to search for such recognized mainfiles. As long as your upload does not contain any files that are recognized by NOMAD, you cannot publish the data.</p> <p>However, all files that are associated to a recognized mainfile by being in the same directory are displayed as auxiliary files next to the entry represented by the mainfile.</p> <p>Note</p> <p>A note for VASP users. On the handling of POTCAR files: NOMAD takes care of it; you don't need to worry about it. We understand that POTCAR files are not supposed to be visible to the public according to your VASP license. Thus, in agreement with Georg Kresse, NOMAD extracts the most important information of POTCAR files and stores it in the files named <code>POTCAR.stripped</code>. These files can be accessed and downloaded by anyone, while the original POTCAR files are automatically removed.</p>"},{"location":"howto/manage/upload.html#add-user-metadata","title":"Add user metadata","text":"<p>NOMAD automatically extracts as much information as possible from your files but you can still specify additional metadata. This is what we call user metadata. This includes you and your co-authors (use the edit members function on the upload page) as well as comments, additional web-references, and datasets (use the edit metadata function on the upload page).</p> <p>User metadata can also be provided in an uploaded file. This can be a <code>.json</code> or <code>.yaml</code> file. It has to be named <code>nomad.json</code> or <code>nomad.yaml</code>. Here is a JSON example:</p> <pre><code>{\n    \"comment\": \"Data from a cool research project\",\n    \"references\": [\"http://archivex.org/mypaper\"],\n    \"coauthors\": [\n        \"&lt;email-or-username&gt;\",\n        \"&lt;email-or-username&gt;\"\n    ],\n    \"datasets\": [\n        \"&lt;dataset-name&gt;\"\n    ],\n    \"entries\": {\n        \"path/to/entry_dir/vasp.xml\": {\n            \"comment\": \"An entry specific comment.\"\n        }\n    }\n}\n</code></pre> <p>This file is only applied during the initial processing of an entry. So make sure you either upload it first or with everything else as part of an archive file.</p>"},{"location":"howto/manage/upload.html#publish-and-get-a-doi","title":"Publish and get a DOI","text":"<p>After clicking the <code>PUBLISH</code> button, the uploaded files will become immutable, but you can still edit the metadata.</p> <p>As part of the edit metadata functionality, you can create and assign datasets. Go to <code>PUBLISH</code> / <code>Datasets</code> in the menu to see all your datasets. Here you can assign a DOI to created datasets. For a dataset with DOI, you can only add more entries, but not remove entries.</p>"},{"location":"howto/manage/upload.html#upload-limits","title":"Upload limits","text":"<ul> <li>One upload cannot exceed 32 GB in size.</li> <li>Only 10 non published uploads are allowed per user.</li> <li>Only uploads with at least one recognized entry can be published. See also supported codes/formats below.</li> </ul>"},{"location":"howto/manage/upload.html#strategies-for-large-amounts-of-data","title":"Strategies for large amounts of data","text":"<p>Before attempting to upload large amounts of data, run some experiments with a representative and small subset of your data. Use this to simulate a larger upload that you can review and edit in the normal way. You do not have to publish this test upload; simply delete it before publish, once you are satisfied with the results.</p> <p>Ask for assistance and Contact us in advance. This will allow us to react to your specific situation and eventually prepare additional measures. Allow enough time before you need your data to be published. Adding multiple hundreds of GBs to NOMAD isn't a trivial feat and will take some time and effort on all sides.</p> <p>The upload limits above are necessary to keep NOMAD data manageable and we cannot easily grant exceptions to these rules. This means you have to split your data into 32 GB uploads. Uploading these files, observing the processing, and publishing the data can be automatized through NOMAD APIs.</p> <p>When splitting your data, it is important to not split subdirectories that contain files of the same single entry. NOMAD can only bundle those related files to an entry if they are part of the same upload (and directory). Therefore, there is no single recipe to follow, and a script to split your data depends heavily on how your data is organized.</p> <p>If you provide data for a potentially large amount of entries, it might be advisable to provide user metadata via file. See user metadata above for details.</p> <p>To further automate, you can also upload and directly publish data. After performing some smaller test uploads, you should consider skipping our staging and publish the upload right away. This can save you some time and additional API calls. The upload endpoint has a parameter <code>publish_directly</code>. You can modify the upload command you get on the upload page as follows:</p> <pre><code>curl \"http://nomad-lab.eu/prod/v1/uploads/?token=&lt;your-token&gt;&amp;publish_directly=true\" -T &lt;local_file&gt;\n</code></pre> <p>HTTP makes it easy for you to upload files via browser and curl, but it is not an ideal protocol for the stable transfer of large and many files. Alternatively, we can organize a separate manual file transfer to our servers. We will put your prepared upload files (.zip or .tag.gz) on a predefined path on the NOMAD servers. NOMAD allows to \"upload\" files directly from its servers via an additional <code>local_path</code> parameter:</p> <pre><code>curl -X PUT \"http://nomad-lab.eu/prod/v1/api/uploads/?token=&lt;your-token&gt;&amp;local_path=&lt;path-to-upload-file&gt;\"\n</code></pre>"},{"location":"howto/oasis/admin.html","title":"How to perform admin tasks","text":""},{"location":"howto/oasis/admin.html#backups","title":"Backups","text":"<p>To backup your Oasis at least the file data and mongodb data needs to be saved. You determined the path to your file data (your uploads) during the installation. By default all data is stored in a directory called <code>.volumes</code> that is created in the current working directory of your installation/docker-compose. This directory can be backed up like any other file backup (e.g. rsync).</p> <p>To backup the mongodb, please refer to the official mongodb documentation. We suggest a simple mongodump export that is backed up alongside your files. The default configuration mounts <code>.volumes/mongo</code> into the mongodb container (as <code>/backup</code>) for this purpose. You can use this to export the NOMAD mongo database. Combine this with rsync on the <code>.volumes</code> directory and everything should be set. To create a new mongodump run:</p> <pre><code>docker exec nomad_oasis_mongo mongodump -d nomad_oasis_v1 -o /backup\n</code></pre> <p>The elasticsearch contents can be reproduced with the information in the files and the mongodb.</p> <p>To create a new oasis with the backup data, create the <code>.volumes</code> directory from your backup. Start the new oasis. Use mongorestore:</p> <pre><code>docker exec nomad_oasis_mongo mongorestore /backup\n</code></pre> <p>Now you still have to recreate the elasticsearch index: <pre><code>docker exec nomad_oasis_app python -m nomad.cli admin uploads index\n</code></pre></p>"},{"location":"howto/oasis/admin.html#managing-data-with-the-cli","title":"Managing data with the CLI","text":"<p>The NOMAD command line interface (CLI) provides a few useful administrative functions. To use the NOMAD CLI, open a shell into the app container and run it from there:</p> <pre><code>docker exec -ti nomad_oasis_app bash\n</code></pre> <p>For example you can ls or remove uploads: <pre><code>nomad admin uploads ls\nnomad admin uploads rm -- &lt;upload_id&gt; &lt;upload_id&gt;\n</code></pre></p> <p>You can also reset the processing (of \"stuck\") uploads and reprocess: <pre><code>nomad admin uploads reset -- &lt;upload_id&gt;\nnomad admin uploads process -- &lt;upload_id&gt;\n</code></pre></p> <p>You can also use the CLI to wipe the whole installation: <pre><code>nomad admin reset --i-am-really-sure\n</code></pre></p>"},{"location":"howto/oasis/admin.html#upload-commands","title":"Upload commands","text":"<p>The <code>nomad admin uploads</code> group of CLI commands allow you to inspect and modify all or some uploads in your installation. Sub-commands include <code>ls</code>, <code>rm</code>, <code>chown</code>, <code>process</code> (see below), <code>index</code> (see below).</p> <p>The command group takes many different parameters to target specific subsets of uploads. Here are a few examples:</p> <ul> <li><code>--unpublished</code></li> <li><code>--published</code></li> <li><code>--outdated</code> Select published uploads with older NOMAD versions than the current</li> <li><code>--processing-failure</code> Uploads with processing failures.</li> </ul> <p>For a complete list refer to the CLI reference documentation.</p> <p>Alternatively, you can use a list of upload ids at the end of the command, e.g.:</p> <pre><code>nomad admin uploads ls -- &lt;id1&gt; &lt;id2&gt;\n</code></pre> <p>If you have a list of ids (e.g. in a file), you could use <code>xargs</code>:</p> <pre><code>cat file_with_ids.txt | xargs nomad admin uploads ls --\n</code></pre>"},{"location":"howto/oasis/admin.html#re-processing","title":"Re-processing","text":"<p>Processing includes the conversion of raw files into NOMAD entries. Files are parsed, normalizers are called, the processing results are stored, and the search index is updated. In certain scenarios (failed processing, migration, changed plugins) might require that admins process certain uploads again.</p> <pre><code>nomad admin uploads process\n</code></pre>"},{"location":"howto/oasis/admin.html#re-indexing","title":"Re-Indexing","text":"<p>Each NOMAD entry is represented in NOMAD's search index. Only if an entry is in this index, you can find it via the search interface. Some changes between NOMAD versions (see also our migration guide), might require that you re-index all uploads.</p> <pre><code>nomad admin uploads index\n</code></pre>"},{"location":"howto/oasis/admin.html#restricting-access-to-your-oasis","title":"Restricting access to your Oasis","text":"<p>An Oasis works exactly the same way the official NOMAD works. It is open and everybody can access published data. Everybody with an account can upload data. This might not be what you want.</p> <p>Currently there are two ways to restrict access to your Oasis. First, you do not expose the Oasis to the public internet, e.g. you only make it available on an intra-net or through a VPN.</p> <p>Second, we offer a simple white-list mechanism. As the Oasis administrator you provide a list of accounts as part of your Oasis configuration. To use the Oasis, all users have to be logged in and be on your white list of allowed users. To enable white-listing, you can provide a list of NOMAD account email addresses in your <code>nomad.yaml</code> like this:</p> <pre><code>oasis:\n    allowed_users:\n        - user1@gmail.com\n        - user2@gmail.com\n</code></pre>"},{"location":"howto/oasis/admin.html#configuring-for-performance","title":"Configuring for performance","text":"<p>If you run the OASIS on a single computer, like described here (either with docker or bare linux), you might run into problems with processing large uploads. If the NOMAD worker and app are run on the same computer, the app might become unresponsive, when the worker consumes all system resources.</p> <p>By default, the worker container might have as many worker processes as the system as CPU cores. In addition, each worker process might spawn additional threads and consume more than one CPU core.</p> <p>There are multiple ways to restrict the worker's resource consumption:</p> <ul> <li>limit the number of worker processes and thereby lower the number of used cores</li> <li>disable or restrict multithreading</li> <li>limit available CPU utilization of the worker's docker container with docker</li> </ul>"},{"location":"howto/oasis/admin.html#limit-the-number-of-worker-processes","title":"Limit the number of worker processes","text":"<p>The worker uses the Python package celery. Celery can be configured to use less than the default number of worker processes (which equals the number of available cores). To use only a single core only, you can alter the worker service command in the <code>docker-compose.yml</code> and add a <code>--concurrency</code> argument:</p> <pre><code>command: python -m celery -A nomad.processing worker -l info --concurrency=1 -Q celery\n</code></pre> <p>See also the celery documentation.</p>"},{"location":"howto/oasis/admin.html#limiting-the-use-of-threads","title":"Limiting the use of threads","text":"<p>You can also reduce the usable threads that Python packages based on OpenMP could use to reduce the threads that might be spawned by a single worker process. Simply set the <code>OMP_NUM_THREADS</code> environment variable in the worker container in your <code>docker-compose.yml</code>:</p> <pre><code>services:\n    worker:\n        ...\n        environment:\n            ...\n            OMP_NUM_THREADS: 1\n</code></pre>"},{"location":"howto/oasis/admin.html#limit-cpu-with-docker","title":"Limit CPU with docker","text":"<p>You can add a <code>deploy.resources.limits</code> section to the worker service in the <code>docker-compose.yml</code>:</p> <pre><code>services:\n    worker:\n        ...\n        deploy:\n            resources:\n                limits:\n                    cpus: '0.50'\n</code></pre> <p>The number refers to the percentage use of a single CPU core. See also the docker-compose documentation.</p>"},{"location":"howto/oasis/customize.html","title":"How to customize an Oasis","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p> <p>This is an incomplete list of potential customizations. Please read the respective guides to learn more.</p> <ul> <li>Installation specific changes (domain, path-prefix): How to install an Oasis</li> <li>Restricting user access</li> <li>Write .yaml based schemas and ELNs</li> <li>Learn how to use the tabular parser to manage data from .xls or .csv</li> <li>Learn how to develop plugins that can be installed in an Oasis</li> <li>Add specialized NORTH tools</li> </ul>"},{"location":"howto/oasis/install.html","title":"How to install an Oasis","text":"<p>Originally, the NOMAD Central Repository is a service that runs at the Max-Planck's computing facility in Garching, Germany. However, the NOMAD software is Open-Source, and everybody can run it. Any service that uses NOMAD software independently is called a NOMAD Oasis. A NOMAD Oasis does not need to be fully isolated. For example, you can publish uploads from your NOMAD Oasis to the central NOMAD installation.</p> <p>Note</p> <p>Register your Oasis If you installed (or even just plan to install) a NOMAD Oasis, please register your Oasis with FAIRmat and help us to assist you in the future.</p>"},{"location":"howto/oasis/install.html#quick-start","title":"Quick-start","text":"<ul> <li>Find a linux computer.</li> <li>Make sure you have docker installed. Docker nowadays comes with <code>docker compose</code> build in. Prior, you needed to install the stand alone docker-compose.</li> <li>Download our basic configuration files nomad-oasis.zip</li> <li>Run the following commands (skip <code>chown</code> on MacOS and Windows computers)</li> </ul> <pre><code>unzip nomad-oasis.zip\ncd nomad-oasis\nsudo chown -R 1000 .volumes\ndocker compose pull\ndocker compose up -d\ncurl localhost/nomad-oasis/alive\n</code></pre> <ul> <li>Open http://localhost/nomad-oasis in your browser.</li> </ul> <p>To run NORTH (the NOMAD Remote Tools Hub), the <code>north</code> container needs to run docker and the container has to be run under the docker group. You need to replace the default group id <code>991</code> in the <code>docker-compose.yaml</code>'s <code>north</code> section with your systems docker group id. Run <code>id</code> if you are a docker user, or <code>getent group | grep docker</code> to find our your systems docker gid. The user id 1000 is used as the nomad user inside all containers.</p> <p>This is good as a quick test. We strongly recommend to read the following instructions carefully and adapt the configuration files accordingly. The following might also include meaningful help, if you run into problems.</p>"},{"location":"howto/oasis/install.html#before-you-start","title":"Before you start","text":""},{"location":"howto/oasis/install.html#hardware-considerations","title":"Hardware considerations","text":"<p>Of course this depends on how much data you need to manage and process. Data storage is the obvious aspect here. NOMAD keeps all files that it manages as they are. The files that NOMAD processes in addition (e.g. through parsing) are typically smaller than the original raw files. Therefore, you can base your storage requirements based on the size of the data files that you expect to manage. The additional mongo database and elasticsearch index is comparatively small.</p> <p>Storage speed is another consideration. You can work with NAS systems. All that NOMAD needs is a \"regular\" POSIX filesystem as an interface. So everything you can (e.g. docker host) mount should be fine. For processing data obviously relies on read/write speed, but this is just a matter of convenience. The processing is designed to run as managed asynchronous tasks. Local storage might be favorable for mongodb and elasticsearch operation, but it is not a must.</p> <p>The amount of compute resource (e.g. processor cores) is also a matter of convenience (and amount of expected users). Four cpu-cores are typically enough to support a research group and run application, processing, and databases in parallel. Smaller systems still work, e.g. for testing.</p> <p>There should be enough RAM to run databases, application, and processing at the same time. The minimum requirements here can be quite low, but for processing the metadata for individual files is kept in memory. For large DFT geometry-optimizations this can add up quickly, especially if many CPU cores are available for processing entries in parallel. We recommend at least 2GB per core and a minimum of 8GB. You also need to consider RAM and CPU for running tools like jupyter, if you opt to use NOMAD NORTH.</p>"},{"location":"howto/oasis/install.html#sharing-data-through-log-transfer-and-data-privacy-notice","title":"Sharing data through log transfer and data privacy notice","text":"<p>NOMAD includes a log transfer functions. When enabled this it automatically collects and transfers non-personalized logging data to us. Currently, this functionality is experimental and requires opt-in. However, in upcoming versions of NOMAD Oasis, we might change to out-out.</p> <p>To enable this functionality add <code>logtransfer.enabled: true</code> to you <code>nomad.yaml</code>.</p> <p>The service collects log-data and aggregated statistics, such as the number of users or the number of uploaded datasets. In any case this data does not personally identify any users or contains any uploaded data. All data is in an aggregated and anonymized form.</p> <p>The data is solely used by the NOMAD developers and FAIRmat, including but not limited to:</p> <ul> <li>Analyzing and monitoring system performance to identify and resolve issues.</li> <li>Improving our NOMAD software based on usage patterns.</li> <li>Generating aggregated and anonymized reports.</li> </ul> <p>We do not share any collected data with any third parties.</p> <p>We may update this data privacy notice from time to time to reflect changes in our data practices. We encourage you to review this notice periodically for any updates.</p>"},{"location":"howto/oasis/install.html#using-the-central-user-management","title":"Using the central user management","text":"<p>Our recommendation is to use the central user management provided by nomad-lab.eu. We simplified its use and you can use it out-of-the-box. You can even run your system from <code>localhost</code> (e.g. for initial testing). The central user management system is not communicating with your OASIS directly. Therefore, you can run your OASIS without exposing it to the public internet.</p> <p>There are two requirements. First, your users must be able to reach the OASIS. If a user is logging in, she/he is redirected to the central user management server and after login, she/he is redirected back to the OASIS. These redirects are executed by your user's browser and do not require direct communication.</p> <p>Second, your OASIS must be able to request (via HTTP) the central user management and central NOMAD installation. This is necessary for non JWT-based authentication methods and to retrieve existing users for data-sharing features.</p> <p>The central user management will make future synchronizing data between NOMAD installations easier and generally recommend to use the central system. But in principle, you can also run your own user management. See the section on your own user management.</p>"},{"location":"howto/oasis/install.html#docker-and-docker-compose","title":"Docker and docker compose","text":"<p>We recommend the installation via docker and docker-compose. It is the most documented, simplest, easiest to update, and generally the most frequently chosen option.</p>"},{"location":"howto/oasis/install.html#pre-requisites","title":"Pre-requisites","text":"<p>NOMAD software is distributed as a set of docker containers and there are also other services required that can be run with docker. Further, we use docker-compose to setup all necessary containers in the simplest way possible.</p> <p>You will need a single computer, with docker and docker-compose installed. Refer to the official docker (and docker-compose) documentation for installation instructions. Newer version of docker have a re-implementation of docker-compose integrated as the <code>docker compose</code> sub-command. This should be fully compatible and you might chose to can replace <code>docker compose</code> with <code>docker-compose</code> in this tutorial.</p> <p>The following will run all necessary services with docker. These comprise: a mongo database, an elasticsearch, a rabbitmq distributed task queue, the NOMAD app, NOMAD worker, and NOMAD gui. In this architecture documentation, you will learn what each service does and why it is necessary.</p>"},{"location":"howto/oasis/install.html#configuration","title":"Configuration","text":"<p>All docker containers are configured via docker-compose and the respective <code>docker-compose.yaml</code> file. Further, we will need to mount some configuration files to configure the NOMAD services within their respective containers.</p> <p>There are three files to configure:</p> <ul> <li><code>docker-compose.yaml</code></li> <li><code>configs/nomad.yaml</code></li> <li><code>configs/nginx.conf</code></li> </ul> <p>In this example, we have all files in the same directory (the directory we are also working in). You can download minimal example files here.</p>"},{"location":"howto/oasis/install.html#docker-composeyaml","title":"docker-compose.yaml","text":"<p>The most basic <code>docker-compose.yaml</code> to run an OASIS looks like this:</p> <pre><code>version: \"3\"\n\nservices:\n  # broker for celery\n  rabbitmq:\n    restart: unless-stopped\n    image: rabbitmq:3.11.5\n    container_name: nomad_oasis_rabbitmq\n    environment:\n      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG\n      - RABBITMQ_DEFAULT_USER=rabbitmq\n      - RABBITMQ_DEFAULT_PASS=rabbitmq\n      - RABBITMQ_DEFAULT_VHOST=/\n    volumes:\n      - rabbitmq:/var/lib/rabbitmq\n    healthcheck:\n      test: [\"CMD\", \"rabbitmq-diagnostics\", \"--silent\", \"--quiet\", \"ping\"]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # the search engine\n  elastic:\n    restart: unless-stopped\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.1\n    container_name: nomad_oasis_elastic\n    environment:\n      - ES_JAVA_OPTS=-Xms512m -Xmx512m\n      - discovery.type=single-node\n    volumes:\n      - elastic:/usr/share/elasticsearch/data\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"curl\"\n        - \"--fail\"\n        - \"--silent\"\n        - \"http://elastic:9200/_cat/health\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 60s\n\n  # the user data db\n  mongo:\n    restart: unless-stopped\n    image: mongo:5.0.6\n    container_name: nomad_oasis_mongo\n    environment:\n      - MONGO_DATA_DIR=/data/db\n      - MONGO_LOG_DIR=/dev/null\n    volumes:\n      - mongo:/data/db\n      - ./.volumes/mongo:/backup\n    command: mongod --logpath=/dev/null # --quiet\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"mongo\"\n        - \"mongo:27017/test\"\n        - \"--quiet\"\n        - \"--eval\"\n        - \"'db.runCommand({ping:1}).ok'\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad worker (processing)\n  worker:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_worker\n    environment:\n      NOMAD_SERVICE: nomad_oasis_worker\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\n      elastic:\n        condition: service_healthy\n      mongo:\n        condition: service_healthy\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n    command: ./run-worker.sh\n\n  # nomad app (api + proxy)\n  app:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_app\n    environment:\n      NOMAD_SERVICE: nomad_oasis_app\n      NOMAD_SERVICES_API_PORT: 80\n      NOMAD_FS_EXTERNAL_WORKING_DIRECTORY: \"$PWD\"\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n      NOMAD_NORTH_HUB_HOST: north\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\n      elastic:\n        condition: service_healthy\n      mongo:\n        condition: service_healthy\n      north:\n        condition: service_started\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n    command: ./run.sh\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"curl\"\n        - \"--fail\"\n        - \"--silent\"\n        - \"http://localhost:8000/-/health\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad remote tools hub (JupyterHUB, e.g. for AI Toolkit)\n  north:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_north\n    environment:\n      NOMAD_SERVICE: nomad_oasis_north\n      NOMAD_NORTH_DOCKER_NETWORK: nomad_oasis_network\n      NOMAD_NORTH_HUB_CONNECT_IP: north\n      NOMAD_NORTH_HUB_IP: \"0.0.0.0\"\n      NOMAD_NORTH_HUB_HOST: north\n      NOMAD_SERVICES_API_HOST: app\n      NOMAD_FS_EXTERNAL_WORKING_DIRECTORY: \"$PWD\"\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n      - /var/run/docker.sock:/var/run/docker.sock\n    user: '1000:991'\n    command: python -m nomad.cli admin run hub\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"curl\"\n        - \"--fail\"\n        - \"--silent\"\n        - \"http://localhost:8081/nomad-oasis/north/hub/health\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad proxy (a reverse proxy for nomad)\n  proxy:\n    restart: unless-stopped\n    image: nginx:1.13.9-alpine\n    container_name: nomad_oasis_proxy\n    command: nginx -g 'daemon off;'\n    volumes:\n      - ./configs/nginx.conf:/etc/nginx/conf.d/default.conf\n    depends_on:\n      app:\n        condition: service_healthy\n      worker:\n        condition: service_started # TODO: service_healthy\n      north:\n        condition: service_healthy\n    ports:\n      - 80:80\n\nvolumes:\n  mongo:\n    name: \"nomad_oasis_mongo\"\n  elastic:\n    name: \"nomad_oasis_elastic\"\n  rabbitmq:\n    name: \"nomad_oasis_rabbitmq\"\n  keycloak:\n    name: \"nomad_oasis_keycloak\"\n\nnetworks:\n  default:\n    name: nomad_oasis_network\n</code></pre> <p>Changes necessary:</p> <ul> <li>The group in the value of the hub's user parameter needs to match the docker group on the host. This should ensure that the user which runs the hub, has the rights to access the host's docker.</li> <li>On Windows or MacOS computers you have to run the <code>app</code> and <code>worker</code> container without <code>user: '1000:1000'</code> and the <code>north</code> container with <code>user: root</code>.</li> </ul> <p>A few things to notice:</p> <ul> <li>The app, worker, and north service use the NOMAD docker image. Here we use the <code>latest</code> tag, which gives you the latest beta version of NOMAD. You might want to change this to <code>stable</code>, a version tag (format is <code>vX.X.X</code>, you find all releases here), or a specific branch tag.</li> <li>All services use docker volumes for storage. This could be changed to host mounts.</li> <li>It mounts two configuration files that need to be provided (see below): <code>nomad.yaml</code>, <code>nginx.conf</code>.</li> <li>The only exposed port is <code>80</code> (proxy service). This could be changed to a desired port if necessary.</li> <li>The NOMAD images are pulled from our gitlab at MPCDF, the other services use images from a public registry (dockerhub).</li> <li>All containers will be named <code>nomad_oasis_*</code>. These names can be used later to reference the container with the <code>docker</code> cmd.</li> <li>The services are setup to restart <code>always</code>, you might want to change this to <code>no</code> while debugging errors to prevent indefinite restarts.</li> <li>Make sure that the <code>PWD</code> environment variable is set. NORTH needs to create bind mounts that require absolute paths and we need to pass the current working directory to the configuration from the PWD variable (see hub service in the <code>docker-compose.yaml</code>).</li> <li>The <code>north</code> service needs to run docker containers. We have to use the systems docker group as a group. You might need to replace <code>991</code> with your systems docker group id.</li> </ul>"},{"location":"howto/oasis/install.html#nomadyaml","title":"nomad.yaml","text":"<p>NOMAD app and worker read a <code>nomad.yaml</code> for configuration.</p> <pre><code>services:\n  api_host: 'localhost'\n  api_base_path: '/nomad-oasis'\n\noasis:\n  is_oasis: true\n  uses_central_user_management: true\n\nnorth:\n  jupyterhub_crypt_key: '978bfb2e13a8448a253c629d8dd84ff89587f30e635b753153960930cad9d36d'\n\nmeta:\n  deployment: 'oasis'\n  deployment_url: 'https://my-oasis.org/api'\n  maintainer_email: 'me@my-oasis.org'\n\nlogtransfer:\n  enabled: false\n\nmongo:\n    db_name: nomad_oasis_v1\n\nelastic:\n    entries_index: nomad_oasis_entries_v1\n    materials_index: nomad_oasis_materials_v1\n</code></pre> <p>You should change the following:</p> <ul> <li>Replace <code>localhost</code> with the hostname of your server. I user-management will redirect your users back to this host. Make sure this is the hostname, your users can use.</li> <li>Replace <code>deployment</code>, <code>deployment_url</code>, and <code>maintainer_email</code> with representative values. The <code>deployment_url</code> should be the url to the deployment's api (should end with <code>/api</code>).</li> <li>To enable the log transfer set <code>logtransfer.enable: true</code> (data privacy notice above).</li> <li>You can change <code>api_base_path</code> to run NOMAD under a different path prefix.</li> <li>You should generate your own <code>north.jupyterhub_crypt_key</code>. You can generate one with <code>openssl rand -hex 32</code>.</li> <li>On Windows or MacOS, you have to add <code>hub_connect_ip: 'host.docker.internal'</code> to the <code>north</code> section.</li> </ul> <p>A few things to notice:</p> <ul> <li>Under <code>mongo</code> and <code>elastic</code> you can configure database and index names. This might be useful, if you need to run multiple NOMADs with the same databases.</li> <li>All managed files are stored under <code>.volumes</code> of the current directory.</li> </ul>"},{"location":"howto/oasis/install.html#nginxconf","title":"nginx.conf","text":"<p>The GUI container serves as a proxy that forwards requests to the app container. The proxy is an nginx server and needs a configuration similar to this:</p> <pre><code>map $http_upgrade $connection_upgrade {\n    default upgrade;\n    ''      close;\n}\n\nserver {\n    listen        80;\n    server_name   localhost;\n    proxy_set_header Host $host;\n\n    gzip_min_length     1000;\n    gzip_buffers        4 8k;\n    gzip_http_version   1.0;\n    gzip_disable        \"msie6\";\n    gzip_vary           on;\n    gzip on;\n    gzip_proxied any;\n    gzip_types\n        text/css\n        text/javascript\n        text/xml\n        text/plain\n        application/javascript\n        application/x-javascript\n        application/json;\n\n    location / {\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /nomad-oasis\\/?(gui)?$ {\n        rewrite ^ /nomad-oasis/gui/ permanent;\n    }\n\n    location /nomad-oasis/gui/ {\n        proxy_intercept_errors on;\n        error_page 404 = @redirect_to_index;\n        proxy_pass http://app:8000;\n    }\n\n    location @redirect_to_index {\n        rewrite ^ /nomad-oasis/gui/index.html break;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ \\/gui\\/(service-worker\\.js|meta\\.json)$ {\n        add_header Last-Modified $date_gmt;\n        add_header Cache-Control 'no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0';\n        if_modified_since off;\n        expires off;\n        etag off;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /api/v1/uploads(/?$|.*/raw|.*/bundle?$)  {\n        client_max_body_size 35g;\n        proxy_request_buffering off;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /api/v1/.*/download {\n        proxy_buffering off;\n        proxy_pass http://app:8000;\n    }\n\n    location /nomad-oasis/north/ {\n        client_max_body_size 500m;\n        proxy_pass http://north:9000;\n\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        # websocket headers\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n        proxy_set_header X-Scheme $scheme;\n\n        proxy_buffering off;\n    }\n}\n</code></pre> <p>A few things to notice:</p> <ul> <li>It configures the base path (<code>nomad-oasis</code>). It needs to be changed, if you use a different base path.</li> <li>You can use the server for additional content if you like.</li> <li><code>client_max_body_size</code> sets a limit to the possible upload size.</li> </ul> <p>You can add an additional reverse proxy in front or modify the nginx in the docker-compose.yaml to support https. If you operate the GUI container behind another proxy, keep in mind that your proxy should not buffer requests/responses to allow streaming of large requests/responses for <code>api/v1/uploads</code> and <code>api/v1/.*/download</code>. An nginx reverse proxy location on an additional reverse proxy, could have these directives to ensure the correct http headers and allows the download and upload of large files: <pre><code>client_max_body_size 35g;\nproxy_set_header Host $host;\nproxy_pass_request_headers on;\nproxy_buffering off;\nproxy_request_buffering off;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_pass http://&lt;your-oasis-host&gt;/nomad-oasis;\n</code></pre></p>"},{"location":"howto/oasis/install.html#running-nomad","title":"Running NOMAD","text":"<p>If you prepared the above files, simply use the usual <code>docker compose</code> commands to start everything.</p> <p>To make sure you have the latest docker images for everything, run this first: <pre><code>docker compose pull\n</code></pre></p> <p>In the beginning and to simplify debugging, it is recommended to start the services separately: <pre><code>docker compose up -d mongo elastic rabbitmq\ndocker compose up app worker gui\n</code></pre></p> <p>The <code>-d</code> option runs container in the background as daemons. Later you can run all at once: <pre><code>docker compose up -d\n</code></pre></p> <p>Running all services also contains NORTH. When you use a tool in NORTH for the first time, your docker needs to pull the image that contains this tool. Be aware that this might take longer than timeouts allow and starting a tool for the very first time might fail.</p> <p>You can also use docker to stop and remove faulty containers that run as daemons: <pre><code>docker stop nomad_oasis_app\ndocker rm nomad_oasis_app\n</code></pre></p> <p>You can wait for the start-up with curl using the apps <code>alive</code> \"endpoint\": <pre><code>curl http://&lt;your host&gt;/nomad-oasis/alive\n</code></pre></p> <p>If everything works, the gui should be available under: <pre><code>http://&lt;your host&gt;/nomad-oasis/gui/\n</code></pre></p> <p>If you run into problems, use the dev-tools of your browser to check the javascript logs or monitor the network traffic for HTTP 500/400/404/401 responses.</p> <p>To see if at least the api works, check <pre><code>http://&lt;your host&gt;/nomad-oasis/alive\nhttp://&lt;your host&gt;/nomad-oasis/api/info\n</code></pre></p> <p>To see logs or 'go into' a running container, you can access the individual containers with their names and the usual docker commands:</p> <pre><code>docker logs nomad_oasis_app\n</code></pre> <pre><code>docker exec -ti nomad_oasis_app /bin/bash\n</code></pre> <p>If you want to report problems with your OASIS. Please provide the logs for</p> <ul> <li>nomad_oasis_app</li> <li>nomad_oasis_worker</li> <li>nomad_oasis_gui</li> </ul>"},{"location":"howto/oasis/install.html#provide-and-connect-your-own-user-management","title":"Provide and connect your own user management","text":"<p>NOMAD uses keycloak for its user management. NOMAD uses keycloak in two ways. First, the user authentication uses the OpenID Connect/OAuth interfaces provided by keycloak. Second, NOMAD uses the keycloak realm-management API to get a list of existing users. Keycloak is highly customizable and numerous options to connect keycloak to existing identity providers exist.</p> <p>This tutorial assumes that you have some understanding of what keycloak is and how it works.</p> <p>The NOMAD Oasis installation with your own keyloak is very similar to the regular docker-compose installation above. There are just a three changes.</p> <ul> <li>The <code>docker-compose.yaml</code> has an added keycloak service.</li> <li>The <code>nginx.conf</code> is also modified to add another location for keycloak.</li> <li>The <code>nomad.yaml</code> has modifications to tell nomad to use your and not the official NOMAD keycloak.</li> </ul> <p>You can start with the regular installation above and manually adopt the config or download the already updated configuration files: nomad-oasis-with-keycloak.zip. The download also contains an additional <code>configs/nomad-realm.json</code> that allows you to create an initial keycloak realm that is configured for NOMAD automatically.</p> <p>First, the <code>docker-compose.yaml</code>: <pre><code>version: \"3\"\n\nservices:\n  # keycloak user management\n  keycloak:\n    restart: unless-stopped\n    image: quay.io/keycloak/keycloak:16.1.1\n    container_name: nomad_oasis_keycloak\n    environment:\n      - PROXY_ADDRESS_FORWARDING=true\n      - KEYCLOAK_USER=admin\n      - KEYCLOAK_PASSWORD=password\n      - KEYCLOAK_FRONTEND_URL=http://localhost/keycloak/auth\n      - KEYCLOAK_IMPORT=\"/tmp/nomad-realm.json\"\n    command:\n      - \"-Dkeycloak.import=/tmp/nomad-realm.json -Dkeycloak.migration.strategy=IGNORE_EXISTING\"\n    volumes:\n      - keycloak:/opt/jboss/keycloak/standalone/data\n      - ./configs/nomad-realm.json:/tmp/nomad-realm.json\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"curl\"\n        - \"--fail\"\n        - \"--silent\"\n        - \"http://127.0.0.1:9990/health/live\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 30s\n\n  # broker for celery\n  rabbitmq:\n    restart: unless-stopped\n    image: rabbitmq:3.11.5\n    container_name: nomad_oasis_rabbitmq\n    environment:\n      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG\n      - RABBITMQ_DEFAULT_USER=rabbitmq\n      - RABBITMQ_DEFAULT_PASS=rabbitmq\n      - RABBITMQ_DEFAULT_VHOST=/\n    volumes:\n      - rabbitmq:/var/lib/rabbitmq\n    healthcheck:\n      test: [\"CMD\", \"rabbitmq-diagnostics\", \"--silent\", \"--quiet\", \"ping\"]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # the search engine\n  elastic:\n    restart: unless-stopped\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.1\n    container_name: nomad_oasis_elastic\n    environment:\n      - ES_JAVA_OPTS=-Xms512m -Xmx512m\n      - discovery.type=single-node\n    volumes:\n      - elastic:/usr/share/elasticsearch/data\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"curl\"\n        - \"--fail\"\n        - \"--silent\"\n        - \"http://elastic:9200/_cat/health\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 60s\n\n  # the user data db\n  mongo:\n    restart: unless-stopped\n    image: mongo:5.0.6\n    container_name: nomad_oasis_mongo\n    environment:\n      - MONGO_DATA_DIR=/data/db\n      - MONGO_LOG_DIR=/dev/null\n    volumes:\n      - mongo:/data/db\n      - ./.volumes/mongo:/backup\n    command: mongod --logpath=/dev/null # --quiet\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"mongo\"\n        - \"mongo:27017/test\"\n        - \"--quiet\"\n        - \"--eval\"\n        - \"'db.runCommand({ping:1}).ok'\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad worker (processing)\n  worker:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_worker\n    environment:\n      NOMAD_SERVICE: nomad_oasis_worker\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\n      elastic:\n        condition: service_healthy\n      mongo:\n        condition: service_healthy\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n    command: ./run-worker.sh\n\n  # nomad app (api + proxy)\n  app:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_app\n    environment:\n      NOMAD_SERVICE: nomad_oasis_app\n      NOMAD_SERVICES_API_PORT: 80\n      NOMAD_FS_EXTERNAL_WORKING_DIRECTORY: \"$PWD\"\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\n      elastic:\n        condition: service_healthy\n      mongo:\n        condition: service_healthy\n      keycloak:\n        condition: service_started\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n    command: ./run.sh\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"curl\"\n        - \"--fail\"\n        - \"--silent\"\n        - \"http://localhost:8000/-/health\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad remote tools hub (JupyterHUB, e.g. for AI Toolkit)\n  north:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_north\n    environment:\n      NOMAD_SERVICE: nomad_oasis_north\n      NOMAD_NORTH_DOCKER_NETWORK: nomad_oasis_network\n      NOMAD_NORTH_HUB_CONNECT_IP: north\n      NOMAD_NORTH_HUB_IP: \"0.0.0.0\"\n      NOMAD_NORTH_HUB_HOST: north\n      NOMAD_SERVICES_API_HOST: app\n      NOMAD_FS_EXTERNAL_WORKING_DIRECTORY: \"$PWD\"\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    depends_on:\n      keycloak:\n        condition: service_started\n      app:\n        condition: service_started\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n      - /var/run/docker.sock:/var/run/docker.sock\n    user: '1000:991'\n    command: python -m nomad.cli admin run hub\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"curl\"\n        - \"--fail\"\n        - \"--silent\"\n        - \"http://localhost:8081/nomad-oasis/north/hub/health\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad proxy (a reverse proxy for nomad)\n  proxy:\n    restart: unless-stopped\n    image: nginx:1.13.9-alpine\n    container_name: nomad_oasis_proxy\n    command: nginx -g 'daemon off;'\n    volumes:\n      - ./configs/nginx.conf:/etc/nginx/conf.d/default.conf\n    depends_on:\n      keycloak:\n        condition: service_healthy\n      app:\n        condition: service_healthy\n      worker:\n        condition: service_started # TODO: service_healthy\n      north:\n        condition: service_healthy\n    ports:\n      - 80:80\n\nvolumes:\n  mongo:\n    name: \"nomad_oasis_mongo\"\n  elastic:\n    name: \"nomad_oasis_elastic\"\n  rabbitmq:\n    name: \"nomad_oasis_rabbitmq\"\n  keycloak:\n    name: \"nomad_oasis_keycloak\"\n\nnetworks:\n  default:\n    name: nomad_oasis_network\n</code></pre></p> <p>A few notes:</p> <ul> <li>You have to change the <code>KEYCLOAK_FRONTEND_URL</code> variable to match your host and set a path prefix.</li> <li>The environment variables on the keycloak service allow to use keycloak behind the nginx proxy with a path prefix, e.g. <code>keycloak</code>.</li> <li>By default, keycloak will use a simple H2 file database stored in the given volume. Keycloak offers many other options to connect SQL databases.</li> <li>We will use keycloak with our nginx proxy here, but you can also host-bind the port <code>8080</code> to access keycloak directly.</li> <li>We mount and use the downloaded <code>configs/nomad-realm.json</code> to configure a NOMAD compatible realm on the first startup of keycloak.</li> </ul> <p>Second, we add a keycloak location to the nginx config: <pre><code>map $http_upgrade $connection_upgrade {\n    default upgrade;\n    ''      close;\n}\n\nserver {\n    listen        80;\n    server_name   localhost;\n    proxy_set_header Host $host;\n\n    location /keycloak {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        rewrite /keycloak/(.*) /$1 break;\n        proxy_pass http://keycloak:8080;\n    }\n\n    location / {\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /nomad-oasis\\/?(gui)?$ {\n        rewrite ^ /nomad-oasis/gui/ permanent;\n    }\n\n    location /nomad-oasis/gui/ {\n        proxy_intercept_errors on;\n        error_page 404 = @redirect_to_index;\n        proxy_pass http://app:8000;\n    }\n\n    location @redirect_to_index {\n        rewrite ^ /nomad-oasis/gui/index.html break;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ \\/gui\\/(service-worker\\.js|meta\\.json)$ {\n        add_header Last-Modified $date_gmt;\n        add_header Cache-Control 'no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0';\n        if_modified_since off;\n        expires off;\n        etag off;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /api/v1/uploads(/?$|.*/raw|.*/bundle?$)  {\n        client_max_body_size 35g;\n        proxy_request_buffering off;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /api/v1/.*/download {\n        proxy_buffering off;\n        proxy_pass http://app:8000;\n    }\n\n    location /nomad-oasis/north/ {\n        client_max_body_size 500m;\n        proxy_pass http://north:9000;\n\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        # websocket headers\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n        proxy_set_header X-Scheme $scheme;\n\n        proxy_buffering off;\n    }\n}\n</code></pre></p> <p>A few notes:</p> <ul> <li>Again, we are using <code>keycloak</code> as a path prefix. We configure the headers to allow keycloak to pick up the rewritten url.</li> </ul> <p>Third, we modify the keycloak configuration in the <code>nomad.yaml</code>: <pre><code>services:\n  api_host: 'localhost'\n  api_base_path: '/nomad-oasis'\n\noasis:\n  is_oasis: true\n  uses_central_user_management: false\n\nnorth:\n  jupyterhub_crypt_key: '978bfb2e13a8448a253c629d8dd84ff89587f30e635b753153960930cad9d36d'\n\nkeycloak:\n  server_url: 'http://keycloak:8080/auth/'\n  public_server_url: 'http://localhost/keycloak/auth/'\n  realm_name: nomad\n  username: 'admin'\n  password: 'password'\n\nmeta:\n  deployment: 'oasis'\n  deployment_url: 'https://my-oasis.org/api'\n  maintainer_email: 'me@my-oasis.org'\n\nlogtransfer:\n  enabled: false\n\nmongo:\n    db_name: nomad_oasis_v1\n\nelastic:\n    entries_index: nomad_oasis_entries_v1\n    materials_index: nomad_oasis_materials_v1\n</code></pre></p> <p>You should change the following:</p> <ul> <li>There are two urls to configure for keycloak. The <code>server_url</code> is used by the nomad services to directly communicate with keycloak within the docker network. The <code>public_server_url</code> is used by the UI to perform the authentication flow. You need to replace <code>localhost</code> in <code>public_server_url</code> with <code>&lt;yourhost&gt;</code>.</li> </ul> <p>A few notes:</p> <ul> <li>The particular <code>admin_user_id</code> is the Oasis admin user in the provided example realm configuration. See below.</li> </ul> <p>If you open <code>http://&lt;yourhost&gt;/keycloak/auth</code> in a browser, you can access the admin console. The default user and password are <code>admin</code> and <code>password</code>.</p> <p>Keycloak uses <code>realms</code> to manage users and clients. A default NOMAD compatible realm is imported by default. The realm comes with a test user and password <code>test</code> and <code>password</code>.</p> <p>A few notes on the realm configuration:</p> <ul> <li>Realm and client settings are almost all default keycloak settings.</li> <li>You should change the password of the admin user in the nomad realm.</li> <li>The admin user in the nomad realm has the additional <code>view-users</code> client role for <code>realm-management</code> assigned. This is important, because NOMAD will use this user to retrieve the list of possible users for managing co-authors and reviewers on NOMAD uploads.</li> <li>The realm has one client <code>nomad_public</code>. This has a basic configuration. You might want to adapt this to your own policies. In particular you can alter the valid redirect URIs to your own host.</li> <li>We disabled the https requirement on the default realm for simplicity. You should change this for a production system.</li> </ul>"},{"location":"howto/oasis/install.html#base-linux-without-docker","title":"Base Linux (without docker)","text":""},{"location":"howto/oasis/install.html#pre-requisites_1","title":"Pre-requisites","text":"<p>We will run NOMAD from the nomad-lab Python package. This package contains all the necessary code to run the processing, api, and gui. But, it is missing the necessary databases. You might be able to run NOMAD in user space.</p> <p>You will need:</p> <ul> <li>preferably a linux computer, which Python 3.9, preferable a Python virtual environment</li> <li>elasticsearch 7.x, running without users and authentication, preferable on the default settings</li> <li>mongodb 5.x, running without users and authentication, preferable on the default settings</li> <li>rabbitmq 3.x, running without users and authentication, preferable on the default settings</li> <li>nginx</li> <li>an empty directory to work in</li> </ul>"},{"location":"howto/oasis/install.html#install-the-nomad-python-package","title":"Install the NOMAD Python package","text":"<p>You should install everything in a virtual environment. For example like this: <pre><code>virtualenv -p `which python3` nomadpyenv\nsource nomadpyenv/bin/activate\n</code></pre></p> <p>You can simply install the Python package from pypi: <pre><code>pip install nomad-lab[all]\n</code></pre></p> <p>If you need the latest version, you can also download the latest package from our \"beta\" installation. <pre><code>curl \"https://nomad-lab.eu/prod/v1/staging/dist/nomad-lab.tar.gz\" -o nomad-lab.tar.gz\npip install nomad-lab.tar.gz[all]\n</code></pre></p>"},{"location":"howto/oasis/install.html#nomadyaml_1","title":"nomad.yaml","text":"<p>The <code>nomad.yaml</code> is our central config file. You should write a <code>nomad.yaml</code> like this:</p> <pre><code>client:\n  url: 'http://&lt;your-host&gt;/nomad-oasis/api'\n\nservices:\n  api_base_path: '/nomad-oasis'\n  admin_user_id: '&lt;your admin user id&gt;'\n\nkeycloak:\n  realm_name: fairdi_nomad_prod\n  username: '&lt;your admin username&gt;'\n  password: '&lt;your admin user password&gt;'\n  oasis: true\n\nmongo:\n    db_name: nomad_v0_8\n\nelastic:\n    index_name: nomad_v0_8\n</code></pre> <p>You need to change the following:</p> <ul> <li>Replace <code>your-host</code> and admin credentials respectively.</li> <li><code>api_base_path</code> defines the path under which the app is run. It needs to be changed, if you use a different base path.</li> </ul> <p>A few things to notice:</p> <ul> <li>Be secretive about your admin credentials; make sure this file is not publicly readable.</li> </ul>"},{"location":"howto/oasis/install.html#nginx","title":"nginx","text":"<p>You can generate a suitable <code>nginx.conf</code> with the <code>nomad</code> command line command that comes with the nomad-lab Python package. If you server other content but NOMAD with your nginx, you need to incorporate the config accordingly.</p> <p>If you have a standard installation of nginx, this might work. Adapt to your set-up: <pre><code>nomad admin ops nginx-conf &gt; /etc/nginx/conf.d/default.conf\nnginx -t\nnginx -s reload\n</code></pre></p> <p>If you want to run nginx in docker, this might work. Adapt to your set-up: <pre><code>nomad admin ops nginx-conf --host host.docker.internal &gt; nginx.conf\ndocker run --rm -v `pwd`/nginx.conf:/etc/nginx/conf.d/default.conf -p 80:80 nginx:stable nginx -g 'daemon off;' &amp;\n</code></pre></p>"},{"location":"howto/oasis/install.html#running-nomad_1","title":"Running NOMAD","text":"<p>To run NOMAD, you must run two services. One is the NOMAD app, it serves the API and GUI. Here is startup script to run the app. This is exactly, what the docker based install would use. It takes the port (<code>$@</code>) as a parameter. <pre><code>#!/bin/bash\n\npython -m nomad.cli admin run app --with-gui --gunicorn --host 0.0.0.0 $@\n</code></pre></p> <p>The second service is the NOMAD worker, that runs the NOMAD processing. Again this is the startup script. <pre><code>#!/bin/bash\n\npython -m celery -A nomad.processing worker -B -l info  -Q celery\n</code></pre></p> <p>This should give you a working OASIS at <code>http://&lt;your-host&gt;/&lt;your-path-prefix&gt;</code>.</p>"},{"location":"howto/oasis/install.html#kubernetes","title":"Kubernetes","text":"<p>Attention</p> <p>This is just preliminary documentation and many details are missing.</p> <p>There is a NOMAD Helm chart. First we need to add the NOMAD Helm chart repository:</p> <pre><code>helm repo add nomad https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/helm/latest\n</code></pre> <p>New we need a minimal <code>values.yaml</code> that configures the individual kubernetes resources created by our Helm chart:</p> <pre><code>\n</code></pre> <p>The <code>jupyterhub</code>, <code>mongodb</code>, <code>elasticsearch</code>, <code>rabbitmq</code> follow the respective official Helm charts configuration.</p> <p>Run the Helm chart and install NOMAD:</p> <pre><code>helm update --install nomad nomad/nomad -f values.yaml\n</code></pre>"},{"location":"howto/oasis/install.html#troubleshooting","title":"Troubleshooting","text":"<p>Here are some common problems that may occur in an OASIS installation:</p> <ul> <li> <p><code>jwt.exceptions.ImmatureSignatureError: The token is not yet valid (iat)</code>:     The authentication information from central authentication is contained in a special piece of signed information (JWT) that contains details about the signed in person. This information also contains a timestamp, which indicates a point in time at which the information was issued at, called <code>iat</code>. The above error indicates that the server looking at the token thinks that it has not been issued yet.</p> <p>The underlying reason is a time difference between the two different servers (the one creating the JWT, and the one that is validating it) as these might very well be different physical machines. To fix this problem, you should ensure that the time on the servers is up to date (e.g. a network port on the server may be closed, preventing it from synchronizing the time). Note that the servers do not need to be on the same timezone, as internally everything is converted to UTC+0.</p> </li> </ul> <p>### NOMAD in networks with restricted Internet access</p> <p>Some network environments do not allow direct Internet connections, and require the use of an outbound proxy. However, NOMAD needs to connect to the central user management or elasticsearch thus requires an active Internet connection (at least on Windows) to work. In these cases you need to configure docker to use your proxy. See details via this link https://docs.docker.com/network/proxy/. An example file <code>~/.docker/config.json</code> could look like this.</p> <pre><code>{\n  \"proxies\": {\n    \"default\": {\n      \"httpProxy\": \"http://&lt;proxy&gt;:&lt;port&gt;\",\n      \"httpsProxy\": \"http://&lt;proxy&gt;:&lt;port&gt;\",\n      \"noProxy\": \"127.0.0.0/8,elastic,localhost\"\n    }\n  }\n}\n</code></pre> <p>Since not all used services respect proxy variables, one also has to change the docker compose config file <code>docker-compose.yaml</code> for elastic search to:</p> <pre><code>elastic:\n    restart: unless-stopped\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.1\n    container_name: nomad_oasis_elastic\n    environment:\n      - ES_JAVA_OPTS=-Xms512m -Xmx512m\n      - ES_JAVA_OPTS=-Djava.net.useSystemProxies=true\n      - ES_JAVA_OPTS=-Dhttps.proxyHost=&lt;proxy&gt; -Dhttps.proxyPort=port -Dhttps.nonProxyHosts=localhost|127.0.0.1|elastic\n      - discovery.type=single-node\n    volumes:\n      - elastic:/usr/share/elasticsearch/data\n    healthcheck:\n      test:\n        - \"CMD\"\n        - \"curl\"\n        - \"--fail\"\n        - \"--silent\"\n        - \"http://elastic:9200/_cat/health\"\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 60s\n</code></pre> <p>Unfortunately there is no way yet to use the NORTH tools with the central user management, since the jupyterhub spawner does not respect proxy variables. It has not been tested yet when using an authentication which does not require the proxy, e.g. a local keycloak server.</p> <p>If you have issues please contact us on discord n the oasis channel.</p>"},{"location":"howto/oasis/install.html#nomad-behind-a-firewall","title":"NOMAD behind a firewall","text":"<p>It is also possible that your docker container is not able to talk to each other. This could be due to restrictive settings on your server. The firewall shall allow both inbound and outbound HTTP and HTTPS traffic. The corresponding rules need to be added. Furthermore, inbound traffic needs to be enabled for the port used on the <code>nginx</code> service.</p> <p>In this case you should make sure this test runs through: https://docs.docker.com/network/network-tutorial-standalone/</p> <p>If not please contact your server provider for help.</p>"},{"location":"howto/oasis/migrate.html","title":"How to migrate Oasis versions","text":""},{"location":"howto/oasis/migrate.html#software-versions","title":"Software versions","text":"<p>We distribute NOMAD as docker images that are available in our public docker registry. The a NOMAD image names looks like this:</p> <pre><code>gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:v1.2.0\n</code></pre> <p>The version tag (e.g. <code>v1.2.0</code>) follows semantic versioning (<code>major.minor.patch</code>). Images released under a version tag do not change. There are also variable tags like <code>stable</code>, <code>latest</code>, and <code>develop</code>. The image tag for recent feature branches use a encoded variant of the respective merge request name. It is generally the safes to use version tags.</p> <p>Our semantic interpretation of \"minor\" is the following:</p> <ul> <li>there are only additions to programming interfaces and config options</li> <li>NOMAD will still operate on existing data, but the structure of newly processed data might change</li> <li>minor version might introduce new features that are only available after certain actions migration steps.</li> </ul> <p>A road-map for major features can be found on our homepage here. You'll find a detailed change log in the source code here.</p>"},{"location":"howto/oasis/migrate.html#configuration-versions","title":"Configuration versions","text":"<p>Depending on the versions you need to update your docker-compose or NOMAD configuration. This might be necessary for breaking changes, or advisable to activate new features. Therefore, it is important to understand that the installations files that run NOMAD (e.g. <code>docker-compose.yaml</code> or <code>nomad.yaml</code>) are independent of the NOMAD image and they won't automatically change just because you use a new image.</p> <p>We will list respective changes and guides under migration steps below.</p>"},{"location":"howto/oasis/migrate.html#data-versions","title":"Data versions","text":"<p>Different version of NOMAD might represent data differently. For example, the definition of the search index, database schema, file system layout, or internal file formats might change. For such changes, we might offer a migration period, where NOMAD supports two different data representations at the same time. However, eventually this requires some migration of the existing data. This might be necessary for breaking changes (or because an old representation is deprecated) or advisable because a new representation offers new features. Therefore, it is important to understand that the used NOMAD software and data are independent things. Using a new image, does not change the data in your installation.</p> <p>We will list respective changes and guides under migration steps below.</p>"},{"location":"howto/oasis/migrate.html#using-a-new-version","title":"Using a new version","text":"<p>To use a different version of NOMAD, it can be as simple as swapping the tag in the docker-compose services that use the nomad images (i.e. <code>app</code> and <code>worker</code>). Depending on the version change, further steps might be necessary:</p> <ul> <li>for patch releases no further actions should be necessary</li> <li>for minor releases some additional actions might be required to unlock new features</li> <li>for major releases breaking changes are likely and further actions will be required</li> </ul> <p>For changing the minor or major version, please check the migration steps below.</p>"},{"location":"howto/oasis/migrate.html#migration-steps","title":"Migration steps","text":""},{"location":"howto/oasis/migrate.html#to-122","title":"to 1.2.2","text":"<p>We upgraded the Jupyterhub version used for NORTH from 1.0.2 to 4.0.2. By default the Jupyterhub database is persisted in the <code>nomad_oasis_north</code> container. If you want to keep the database (e.g. to not loose any open tools), you will have to upgrade the database. Update the NOMAD docker image version and restart the Oasis like this:</p> <pre><code>docker compose down\ndocker compose pull\ndocker compose run north python -m jupyterhub upgrade-db\ndocker compose up -d\n</code></pre> <p>Alternatively, you can delete the <code>nomad_oasis_north</code> container and start with a fresh database. Make sure that all north tools are stopped and removed.</p> <pre><code>docker compose down\ndocker rm nomad_oasis_north\ndocker compose pull\ndocker compose up -d\n</code></pre>"},{"location":"howto/oasis/migrate.html#to-120","title":"to 1.2.0","text":"<ul> <li> <p>We introduced the plugin mechanism. There are now more options to control which schemas and parsers are available in your installation. By default all the existing and shipped schemas and parsers are enabled. See also here.</p> </li> <li> <p>We changed the archive file format. Re-processing might yield better performance.</p> </li> <li> <p>Parsers are now using a different workflow model and the UI now includes a workflow card on the overview page of entries with workflows for the new model. Re-processing all data will enable this feature for old data. Any analysis build on the old workflow model, might not work for new data.</p> </li> <li> <p>We introduce the log-transfer service. This is currently an opt-in feature.</p> </li> </ul>"},{"location":"howto/oasis/migrate.html#from-08x-to-1x","title":"from 0.8.x to 1.x","text":"<p>Between versions 0.10.x and 1.x we needed to change how archive and metadata data is stored internally in files and databases. This means you cannot simply start a new version of NOMAD on top of the old data. But there is a strategy to adapt the data. This should work for data based on NOMAD &gt;0.8.0 and &lt;= 0.10.x.</p> <p>The overall strategy is to create a new mongo database, copy all information, and then reprocess all data for the new version.</p> <p>First, shutdown the OASIS and remove all old containers. <pre><code>docker compose stop\ndocker compose rm -f\n</code></pre></p> <p>Update your config files (<code>docker-compose.yaml</code>, <code>nomad.yaml</code>, <code>nginx.conf</code>) according to the latest documentation (see above). Make sure to use index and database names that are different. The default values contain a version number in those names, if you don't overwrite those defaults, you should be safe.</p> <p>Make sure you get the latest images and start the OASIS with the new version of NOMAD: <pre><code>docker compose pull\ndocker compose up -d\n</code></pre></p> <p>If you go to the GUI of your OASIS, it should now show the new version and appear empty, because we are using a different database and search index now.</p> <p>To migrate the data, we created a command that you can run within your OASIS' NOMAD application container. This command takes the old database name as an argument, it will copy all data from the old mongodb to the current one. The default v8.x database name was <code>nomad_fairdi</code>, but you might have changed this to <code>nomad_v0_8</code> as recommended by our old Oasis documentation.</p> <pre><code>docker exec -ti nomad_oasis_app bash -c 'nomad admin upgrade migrate-mongo --src-db-name nomad_v0_8'\ndocker exec -ti nomad_oasis_app bash -c 'nomad admin uploads reprocess'\n</code></pre> <p>Now all your data should appear in your OASIS again. If you like, you can remove the old index and database:</p> <pre><code>docker exec nomad_oasis_elastic bash -c 'curl -X DELETE http://elastic:9200/nomad_fairdi'\ndocker exec nomad_oasis_mongo bash -c 'mongo nomad_fairdi --eval \"printjson(db.dropDatabase())\"'\n</code></pre>"},{"location":"howto/oasis/plugins_install.html","title":"How to install plugins into a NOMAD Oasis","text":"<p>Plugins allow the customization of a NOMAD deployment in terms of which apps, normalizers, parsers and schema packages are available. In order for these customization to be activated, they have to be installed into an Oasis.</p> <p>Oasis is controlled and run through a <code>docker-compose.yaml</code> file, which specifies the different software services and how they interact. Some of these services are using a Docker image that contains the actual NOMAD software. It is in this image where we will need to install any additional plugins with <code>pip install</code>.</p> <p>The following sections contain some alternatives for achieving this, with the first option being the preferred one.</p>"},{"location":"howto/oasis/plugins_install.html#option-1-create-a-new-customized-nomad-oasis-distribution-with-your-plugins","title":"Option 1: Create a new customized NOMAD Oasis distribution with your plugins","text":"<p>When initially starting to create a customized NOMAD Oasis distribution, it is strongly advised that you create a GitHub repository to persist your work, collaborate with coworkers and also to automate the building and distribution of your custom image. To streamline this process, we have created a GitHub template repository that helps with all of this. It can do the following for you:</p> <ul> <li>Plugins are controlled with a simple <code>plugins.txt</code> file where it is easy to install plugins from PyPI, Git repositories, local files, etc.</li> <li>The automatic pipeline will create a new Docker image for your Oasis. This image will also be stored on GitHub servers.</li> <li>Initial modifications to the <code>docker-compose.yaml</code> are done automatically so you can boot up the software directly.</li> </ul> <p>To learn more, head over to the template repository and follow the instructions there.</p>"},{"location":"howto/oasis/plugins_install.html#option-2-only-create-a-customized-docker-image","title":"Option 2: Only create a customized Docker image","text":"<p>If you already have an existing NOMAD Oasis setup, or do not wish to use the template, you can also just create a new Docker image which has your plugin installed as a <code>pip</code> package. For this approach, you need to create a new <code>Dockerfile</code>, which runs the installation step on top of our default image. The basic idea is that your Dockerfile looks something like this:</p> <pre><code>FROM gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n\n# Switch to root user to install packages to the system with pip\nUSER root\n\n# Install your plugin here, e.g.:\nRUN pip install git+https://&lt;repository_url&gt;\n\n# Remember to switch back to the 'nomad' user\nUSER nomad\n</code></pre> <p>Depending on how your plugin code is distributed, you have several options for the actual install steps:</p> <ol> <li> <p>Plugin published in PyPI:</p> <pre><code>RUN pip install &lt;package_name&gt;\n</code></pre> </li> <li> <p>Plugin code available in GitHub:</p> <pre><code>RUN pip install git+https://&lt;repository_url&gt;\n</code></pre> </li> <li> <p>Plugin published in MPCDF GitLab registry:</p> <pre><code>RUN pip install nomad-example-schema-plugin --index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre> </li> <li> <p>Copy plugin folder from host machine. Note that the folder needs to be in the Docker build context:</p> <pre><code>COPY &lt;nomad-plugin-folder-name&gt; &lt;nomad-plugin-folder-name&gt;\nRUN cd &lt;nomad-plugin-folder-name&gt; &amp;&amp; pip install .\n</code></pre> </li> </ol> <p>The customized image can then be built like this:</p> <pre><code>docker build -t nomad-with-plugins .\n</code></pre> <p>This will create a new image with the tag <code>nomad-with-plugins</code>, which you can use in your <code>docker-compose.yaml</code> file:</p> <pre><code>#image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\nimage: nomad-with-plugins\n</code></pre>"},{"location":"howto/oasis/plugins_install.html#option-3-deprecated-mount-the-plugin-code-directly-into-the-container","title":"Option 3 (deprecated): Mount the plugin code directly into the container","text":"<p>Attention</p> <p>This option only works with the old plugin mechanism that is based on <code>nomad_plugin.yaml</code> files instead of Python entry points.</p> <p>The NOMAD docker image adds the folder <code>/app/plugins</code> to the <code>PYTHONPATH</code>. This means that you can mount your code into the <code>/app/plugins</code> directory via the volumes section of the <code>app</code> and <code>worker</code> services in your <code>docker-compose.yaml</code>.</p> <p>For example, you can do this by adding an extension to the <code>docker-compose.yaml</code>, e.g. a file called <code>docker-compose.plugins.yaml</code>. Assuming you have cloned three plugins into the Oasis folder as <code>./nomad-schema-plugin-example</code>, <code>./nomad-parser-plugin-example</code> and <code>./nomad-normalizer-plugin-example</code>, your <code>docker-compose.plugins.yaml</code> should look like this:</p> <pre><code>services:\n  worker:\n    volumes:\n      - ./nomad-schema-plugin-example/nomadschemaexample:/app/plugins/nomadschemaexample\n      - ./nomad-parser-plugin-example/nomadparserexample:/app/plugins/nomadparserexample\n      - ./nomad-normalizer-plugin-example/nomadparserexample:/app/plugins/nomadparserexample\n  app:\n    volumes:\n      - ./nomad-schema-plugin-example/nomadschemaexample:/app/plugins/nomadschemaexample\n      - ./nomad-parser-plugin-example/nomadparserexample:/app/plugins/nomadparserexample\n      - ./nomad-normalizer-plugin-example/nomadparserexample:/app/plugins/nomadparserexample\n</code></pre> <p>You have to tell docker that there are now two compose files. This can be done via the <code>COMPOSE_FILE</code> environment variable. This is how you can start the Oasis with the plugins:</p> <pre><code>export COMPOSE_FILE=docker-compose.yaml:docker-compose.plugins.yaml\ndocker compose up -d\n</code></pre> <p>Here is a complete Oasis setup nomad-oasis-with-plugins.zip. Simply download, extract, and start like any other Oasis:</p> <pre><code>unzip nomad-oasis-with-plugins.zip\ncd nomad-oasis-with-plugins\nsudo chown -R 1000 .volumes\nsudo chown -R 1000 nomad-schema-plugin-example\nsudo chown -R 1000 nomad-parser-plugin-example\nsudo chown -R 1000 nomad-normalizer-plugin-example\nexport COMPOSE_FILE=docker-compose.yaml:docker-compose.plugins.yaml\ndocker compose pull\ndocker compose up -d\ncurl localhost/nomad-oasis/alive\n</code></pre> <p>Attention</p> <p>It is important to set up the correct user rights for your volumes and plugins. Our default <code>docker-compose</code> setup uses the user <code>1000</code> in group <code>1000</code> to run the services, this is the reason for the <code>chown</code> commands above that ensure that the processes have access to the data stored in volumes and in the plugins. If you use another user/group to run the docker services, update the commands accordingly.</p> <p>Read the Oasis install guide for more details.</p>"},{"location":"howto/plugins/apps.html","title":"How to write an app","text":"<p>Apps provide customized views of data in the GUI, making it easier for the users to navigate and understand the data related to a specific domain. This typically means that certain domain-specific properties are highlighted, different units may be used for physical properties, and specialized dashboards may be presented. This becomes crucial for NOMAD installations to be able to scale with data that contains a mixture of experiments and simulations, different techniques, and physical properties spanning different time and length scales.</p> <p>Apps only affect the way data is displayed for the user: if you wish to affect the underlying data structure, you will need to write a Python schema package or a YAML schema package.</p> <p>This documentation shows you how to write an plugin entry point for an app. You should read the documentation on getting started with plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/apps.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing an app. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 apps\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/apps.html#app-entry-point","title":"App entry point","text":"<p>The entry point defines basic information about your app and is used to automatically load the app into a NOMAD distribution. It is an instance of an <code>AppEntryPoint</code> and unlike many other plugin entry points, it does not have a separate resource that needs to be lazy-loaded as the entire app is defined in the configuration as an instance of <code>nomad.config.models.ui.App</code>. You will learn more about the <code>App</code> class in the next sections. The entry point should be defined in <code>*/apps/__init__.py</code> like this:</p> <pre><code>from nomad.config.models.plugins import AppEntryPoint\n\nmyapp = MyAppEntryPoint(\n    name = 'MyApp',\n    description = 'My custom app.',\n    app = App(...)\n)\n</code></pre> <p>Here we have instantiated an object <code>myapp</code> in which you specify the default parameterization and other details about the app. In the reference you can see all of the available configuration options for an <code>AppEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for the app to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyapp = \"nomad_example.apps:myapp\"\n</code></pre>"},{"location":"howto/plugins/apps.html#app-class","title":"<code>App</code> class","text":"<p>The definition fo the actual app is given as an instance of the <code>App</code> class specified as part of the entry point. A full breakdown of the model is given below in the app reference, but here is a small example:</p> <pre><code>from nomad.config.models.plugins import AppEntryPoint\nfrom nomad.config.models.ui import App, Column, Columns, FilterMenu, FilterMenus, Filters\n\n\nmyapp = AppEntryPoint(\n    name='MyApp',\n    description='App defined using the new plugin mechanism.',\n    app = App(\n        # Label of the App\n        label='My App',\n        # Path used in the URL, must be unique\n        path='myapp',\n        # Used to categorize apps in the explore menu\n        category='Theory',\n        # Brief description used in the app menu\n        description='An app customized for me.',\n        # Longer description that can also use markdown\n        readme='Here is a much longer description of this app.',\n        # Controls the available search filters. If you want to filter by\n        # quantities in a schema package, you need to load the schema package\n        # explicitly here. Note that you can use a glob syntax to load the\n        # entire package, or just a single schema from a package.\n        filters=Filters(\n            include=['*#nomad_example.schema_packages.mypackage.MySchema'],\n        ),\n        # Controls which columns are shown in the results table\n        columns=Columns(\n            selected=[\n                'entry_id'\n                'data.mysection.myquantity#nomad_example.schema_packages.mypackage.MySchema'\n            ],\n            options={\n                'entry_id': Column(),\n                'upload_create_time': Column(),\n                'data.mysection.myquantity#nomad_example.schema_packages.mypackage.MySchema': Column(),\n            }\n        ),\n        # Dictionary of search filters that are always enabled for queries made\n        # within this app. This is especially important to narrow down the\n        # results to the wanted subset. Any available search filter can be\n        # targeted here. This example makes sure that only entries that use\n        # MySchema are included.\n        filters_locked={\n            \"section_defs.definition_qualified_name:all\": [\n                \"nomad_example.schema_packages.mypackage.MySchema\"\n            ]\n        },\n        # Controls the filter menus shown on the left\n        filter_menus=FilterMenus(\n            options={\n                'material': FilterMenu(label=\"Material\"),\n            }\n        ),\n        # Controls the default dashboard shown in the search interface\n        dashboard={\n            'widgets': [\n                {\n                    'type': 'histogram',\n                    'showinput': False,\n                    'autorange': True,\n                    'nbins': 30,\n                    'scale': 'linear',\n                    'quantity': 'data.mysection.myquantity#nomad_example.schema_packages.mypackage.MySchema',\n                    'layout': {\n                        'lg': {\n                            'minH': 3,\n                            'minW': 3,\n                            'h': 4,\n                            'w': 12,\n                            'y': 0,\n                            'x': 0\n                        }\n                    }\n                }\n            ]\n        }\n    )\n)\n</code></pre> <p>Tip</p> <p>If you want to load an app definition from a YAML file, this can be easily done with the pydantic <code>parse_obj</code> function:</p> <pre><code>    import yaml\n    from nomad.config.models.plugins import AppEntryPoint\n    from nomad.config.models.ui import App\n\n    yaml_data = \"\"\"\n        label: My App\n        path: myapp\n        category: Theory\n    \"\"\"\n    myapp = AppEntryPoint(\n        name='MyApp',\n        description='App defined using the new plugin mechanism.',\n        app=App.parse_obj(\n            yaml.safe_load(yaml_data)\n        ),\n    )\n</code></pre>"},{"location":"howto/plugins/apps.html#loading-custom-quantity-definitions-into-an-app","title":"Loading custom quantity definitions into an app","text":"<p>By default, none of the quantities from custom schemas are available in an app, and they need to be explicitly added. Each app may define additional filters that should be enabled in it. Filters have a special meaning in the app context: filters are pieces of (meta)info that can be queried in the search interface of the app, but also targeted in the rest of the app configuration as explained below in.</p> <p>Note</p> <p>Note that not all of the quantities from a custom schema can be exposed as filters. At the moment we only support targeting scalar quantities from custom schemas.</p> <p>Each schema has a unique name within the NOMAD ecosystem, which is needed to target them in the configuration. The name depends on the resource in which the schema is defined in:</p> <ul> <li>Python schemas are identified by the python path for the class that inherits from <code>Schema</code>. For example, if you have a python package called <code>nomad_example</code>, which has a subpackage called <code>schema_packages</code>, containing a module called <code>mypackage.py</code>, which contains the class <code>MySchema</code>, then the schema name will be <code>nomad_example.schema_packages.mypackage.MySchema</code>.</li> <li>YAML schemas are identified by the entry id of the schema file together with the name of the section defined in the YAML schema. For example if you have uploaded a schema YAML file containing a section definition called <code>MySchema</code>, and it has been assigned an <code>entry_id</code>, the schema name will be <code>entry_id:&lt;entry_id&gt;.MySchema</code>.</li> </ul> <p>The quantities from schemas may be included or excluded as filter by using the <code>filters</code> field in the app config. This option supports a wildcard/glob syntax for including/excluding certain filters. For example, to include all filters from the Python schema defined in the class <code>nomad_example.schema_packages.mypackage.MySchema</code>, you could use:</p> <pre><code>filters=Filters(\n    include=['*#nomad_example.schema_packages.mypackage.MySchema']\n)\n</code></pre> <p>The same thing for a YAML schema could be achieved with:</p> <pre><code>filters=Filters(\n    include=['*#entry_id:&lt;entry_id&gt;.MySchema']\n)\n</code></pre> <p>Once quantities from a schema are included in an app as filters, they can be targeted in the rest of the app. The app configuration often refers to specific filters to configure parts of the user interface. For example, one could configure the results table to show a new column using one of the schema quantities with:</p> <pre><code>columns=Columns(\n    selected=[\n        'entry_id'\n        'data.mysection.myquantity#nomad_example.schema_packages.mypackage.MySchema'\n    ],\n    options={\n        'entry_id': Column(),\n        'upload_create_time': Column(),\n        'data.mysection.myquantity#nomad_example.schema_packages.mypackage.MySchema': Column(),\n    }\n)\n</code></pre> <p>The syntax for targeting quantities depends on the resource:</p> <ul> <li>For python schemas, you need to provide the path and the python schema name separated by a hashtag (#), for example <code>data.mysection.myquantity#nomad_example.schema_packages.mypackage.MySchema</code>.</li> <li>For YAML schemas, you need to provide the path and the YAML schema name separated by a hashtag (#), for example <code>data.mysection.myquantity#entry_id:&lt;entry_id&gt;.MySchema</code>.</li> <li>Quantities that are common for all NOMAD entries can be targeted by using only the path without the need for specifying a schema, e.g. <code>results.material.symmetry.space_group</code>.</li> </ul>"},{"location":"howto/plugins/apps.html#app-reference","title":"App reference","text":""},{"location":"howto/plugins/apps.html#app","title":"App","text":"<p>Defines the layout and functionality for an App.</p> name type label <code>str</code> Name of the App. path <code>str</code> Path used in the browser address bar. resource <code>str</code> Targeted resource.default: <code>entries</code>options: - <code>entries</code> - <code>materials</code> breadcrumb <code>str</code> Name displayed in the breadcrumb, by default the label will be used. category <code>str</code> Category used to organize Apps in the explore menu. description <code>str</code> Short description of the App. readme <code>str</code> Longer description of the App that can also use markdown. pagination <code>Pagination</code> Default result pagination.default: Complex object, default value not displayed. columns <code>Columns</code> Controls the columns shown in the results table. rows <code>Rows</code> Controls the display of entry rows in the results table.default: Complex object, default value not displayed. filter_menus <code>FilterMenus</code> Filter menus displayed on the left side of the screen. filters <code>Filters</code> Controls the filters that are available in this app.default: Complex object, default value not displayed. dashboard <code>Dashboard</code> Default dashboard layout. filters_locked <code>dict</code> Fixed query object that is applied for this search context. This filter will always be active for this context and will not be displayed to the user by default. search_syntaxes <code>SearchSyntaxes</code> Controls which types of search syntax are available."},{"location":"howto/plugins/apps.html#pagination","title":"Pagination","text":"name type order_by <code>str</code> Field used for sorting.default: <code>upload_create_time</code> order <code>str</code> Sorting order.default: <code>desc</code> page_size <code>int</code> Number of results on each page.default: <code>20</code>"},{"location":"howto/plugins/apps.html#rows","title":"Rows","text":"<p>Controls the visualization of rows in the search results.</p> name type actions <code>RowActions</code> details <code>RowDetails</code> selection <code>RowSelection</code>"},{"location":"howto/plugins/apps.html#rowactions","title":"RowActions","text":"<p>Controls the visualization of row actions that are shown at the end of each row.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, RowActionURL]</code> All available row actions. enabled <code>int</code> Whether to enable row actions.default: <code>True</code>"},{"location":"howto/plugins/apps.html#rowactionurl","title":"RowActionURL","text":"<p>Action that will open an external link read from the archive.</p> name type description <code>str</code> Description of the action shown to the user. type <code>str</code> Set as <code>url</code> to get this widget type.default: <code>url</code> path <code>str</code> JMESPath pointing to a path in the archive that contains the URL."},{"location":"howto/plugins/apps.html#rowselection","title":"RowSelection","text":"<p>Controls the selection of rows. If enabled, rows can be selected and additional actions performed on them.</p> name type enabled <code>int</code> Whether to show the row selection.default: <code>True</code>"},{"location":"howto/plugins/apps.html#rowdetails","title":"RowDetails","text":"<p>Controls the visualization of row details that are shown upon pressing the row and contain basic details about the entry.</p> name type enabled <code>int</code> Whether to show row details.default: <code>True</code>"},{"location":"howto/plugins/apps.html#filters","title":"Filters","text":"<p>Controls the availability of filters in the app. Filters are pieces of (meta)info than can be queried in the search interface of the app, but also targeted in the rest of the app configuration. The <code>include</code> and <code>exlude</code> attributes can use glob syntax to target metainfo, e.g. <code>results.*</code> or <code>*.#myschema.schema.MySchema</code>.</p> name type include <code>List[str]</code> List of included options. Supports glob/wildcard syntax. exclude <code>List[str]</code> List of excluded options. Supports glob/wildcard syntax. Has higher precedence than include."},{"location":"howto/plugins/apps.html#columns","title":"Columns","text":"<p>Contains column definitions, controls their availability and specifies the default selection.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, Column]</code> All available column options. Note here that the key must correspond to a quantity path that exists in the metadata. selected <code>List[str]</code> Selected options."},{"location":"howto/plugins/apps.html#column","title":"Column","text":"<p>Option for a column show in the search results.</p> name type label <code>str</code> Label shown in the header. Defaults to the quantity name. align <code>str</code> Alignment in the table.default: <code>AlignEnum.LEFT</code>options: - <code>left</code> - <code>right</code> - <code>center</code> unit <code>str</code> Unit to convert to when displaying. If not given will be displayed in using the default unit in the active unit system. format <code>Format</code> Controls the formatting of the values."},{"location":"howto/plugins/apps.html#format","title":"Format","text":"<p>Value formatting options.</p> name type decimals <code>int</code> Number of decimals to show for numbers.default: <code>3</code> mode <code>str</code> Display mode for numbers.default: <code>ModeEnum.SCIENTIFIC</code>options: - <code>standard</code> - <code>scientific</code> - <code>separators</code> - <code>date</code> - <code>time</code>"},{"location":"howto/plugins/apps.html#filtermenus","title":"FilterMenus","text":"<p>Contains filter menu definitions and controls their availability.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, FilterMenu]</code> Contains the available filter menu options."},{"location":"howto/plugins/apps.html#filtermenu","title":"FilterMenu","text":"<p>Defines the layout and functionality for a filter menu.</p> name type label <code>str</code> Menu label to show in the UI. level <code>int</code> Indentation level of the menu.default: <code>0</code> size <code>str</code> Width of the menu.default: <code>FilterMenuSizeEnum.S</code>options: - <code>s</code> - <code>m</code> - <code>l</code> - <code>xl</code> actions <code>FilterMenuActions</code>"},{"location":"howto/plugins/apps.html#filtermenuactions","title":"FilterMenuActions","text":"<p>Contains filter menu action definitions and controls their availability.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, FilterMenuActionCheckbox]</code> Contains options for filter menu actions."},{"location":"howto/plugins/apps.html#filtermenuactioncheckbox","title":"FilterMenuActionCheckbox","text":"<p>Contains definition for checkbox action in the filter menu.</p> name type type <code>str</code> Action type.options: - <code>checkbox</code> label <code>str</code> Label to show. quantity <code>str</code> Targeted quantity"},{"location":"howto/plugins/apps.html#searchsyntaxes","title":"SearchSyntaxes","text":"<p>Controls the availability of different search syntaxes. These syntaxes determine how raw user input in e.g. the search bar is parsed into queries supported by the API.</p> <p>Currently you can only exclude items. By default, the following options are included:</p> <ul> <li><code>existence</code>: Used to query for the existence of a specific metainfo field in the data.</li> <li><code>equality</code>: Used to query for a specific value with exact match.</li> <li><code>range_bounded</code>: Queries values that are between two numerical limits, inclusive or exclusive.</li> <li><code>range_half_bounded</code>: Queries values that are above/below a numerical limit, inclusive or exclusive.</li> <li><code>free_text</code>: For inexact, free-text queries. Requires that a set of keywords has been filled in the entry.</li> </ul> name type exclude <code>List[str]</code> List of excluded options."},{"location":"howto/plugins/apps.html#dashboard","title":"Dashboard","text":"<p>Dashboard configuration.</p> name type widgets <code>List[Union[WidgetTerms, WidgetHistogram, WidgetScatterPlot, WidgetPeriodicTable]]</code> List of widgets contained in the dashboard."},{"location":"howto/plugins/apps.html#widgetscatterplot","title":"WidgetScatterPlot","text":"<p>Scatter plot widget configuration.</p> name type title <code>str</code> Custom widget title. If not specified, a widget-specific default title is used. type <code>str</code> Set as <code>scatterplot</code> to get this widget type.default: <code>scatterplot</code> layout <code>Dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>. x <code>Union[AxisLimitedScale, str]</code> Configures the information source and display options for the x-axis. y <code>Union[AxisLimitedScale, str]</code> Configures the information source and display options for the y-axis. markers <code>Markers</code> Configures the information source and display options for the markers. color <code>str</code> Quantity used for coloring points. Note that this field is deprecated and <code>markers</code> should be used instead. size <code>int</code> Maximum number of entries to fetch. Notice that the actual number may be more of less, depending on how many entries exist and how many of the requested values each entry contains.default: <code>1000</code> autorange <code>int</code> Whether to automatically set the range according to the data limits.default: <code>True</code>"},{"location":"howto/plugins/apps.html#layout","title":"Layout","text":"<p>Defines widget size and grid positioning for different breakpoints.</p> name type h <code>int</code> Height in grid units w <code>int</code> Width in grid units. x <code>int</code> Horizontal start location in the grid. y <code>int</code> Vertical start location in the grid. minH <code>int</code> Minimum height in grid units.default: <code>3</code> minW <code>int</code> Minimum width in grid units.default: <code>3</code>"},{"location":"howto/plugins/apps.html#axislimitedscale","title":"AxisLimitedScale","text":"<p>Configuration for a plot axis with limited scaling options.</p> name type title <code>str</code> Custom title to show for the axis. unit <code>str</code> Custom unit used for displaying the values. quantity <code>str</code> Path of the targeted quantity. Note that you can most of the features JMESPath syntax here to further specify a selection of values. This becomes especially useful when dealing with repeated sections or statistical values. scale <code>str</code> Defines the axis scaling. Defaults to linear scaling.default: <code>ScaleEnumPlot.LINEAR</code>options: - <code>linear</code> - <code>log</code>"},{"location":"howto/plugins/apps.html#markers","title":"Markers","text":"<p>Configuration for plot markers.</p> name type color <code>Axis</code> Configures the information source and display options for the marker colors."},{"location":"howto/plugins/apps.html#axis","title":"Axis","text":"<p>Configuration for a plot axis with limited scaling options.</p> name type title <code>str</code> Custom title to show for the axis. unit <code>str</code> Custom unit used for displaying the values. quantity <code>str</code> Path of the targeted quantity. Note that you can most of the features JMESPath syntax here to further specify a selection of values. This becomes especially useful when dealing with repeated sections or statistical values. scale <code>str</code> Defines the axis scaling. Defaults to linear scaling.default: <code>ScaleEnum.LINEAR</code>options: - <code>linear</code> - <code>log</code> - <code>1/2</code> - <code>1/4</code> - <code>1/8</code>"},{"location":"howto/plugins/apps.html#widgethistogram","title":"WidgetHistogram","text":"<p>Histogram widget configuration.</p> name type title <code>str</code> Custom widget title. If not specified, a widget-specific default title is used. type <code>str</code> Set as <code>histogram</code> to get this widget type.default: <code>histogram</code> layout <code>Dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>. quantity <code>str</code> Targeted quantity. Note that this field is deprecated and <code>x</code> should be used instead. x <code>Union[Axis, str]</code> Configures the information source and display options for the x-axis. y <code>Union[AxisScale, str]</code> Configures the information source and display options for the y-axis. scale <code>str</code> Statistics scaling.default: <code>ScaleEnum.LINEAR</code>options: - <code>linear</code> - <code>log</code> - <code>1/2</code> - <code>1/4</code> - <code>1/8</code> autorange <code>int</code> Whether to automatically set the range according to the data limits.default: <code>True</code> showinput <code>int</code> Whether to show input text fields for minimum and maximum value.default: <code>True</code> nbins <code>int</code> Maximum number of histogram bins. Notice that the actual number of bins may be smaller if there are fewer data items available."},{"location":"howto/plugins/apps.html#axisscale","title":"AxisScale","text":"<p>Basic configuration for a plot axis.</p> name type scale <code>str</code> Defines the axis scaling. Defaults to linear scaling.default: <code>ScaleEnum.LINEAR</code>options: - <code>linear</code> - <code>log</code> - <code>1/2</code> - <code>1/4</code> - <code>1/8</code>"},{"location":"howto/plugins/apps.html#widgetterms","title":"WidgetTerms","text":"<p>Terms widget configuration.</p> name type title <code>str</code> Custom widget title. If not specified, a widget-specific default title is used. type <code>str</code> Set as <code>terms</code> to get this widget type.default: <code>terms</code> layout <code>Dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>. quantity <code>str</code> Targeted quantity. scale <code>str</code> Statistics scaling.options: - <code>linear</code> - <code>log</code> - <code>1/2</code> - <code>1/4</code> - <code>1/8</code> showinput <code>int</code> Whether to show text input field.default: <code>True</code>"},{"location":"howto/plugins/apps.html#widgetperiodictable","title":"WidgetPeriodicTable","text":"<p>Periodic table widget configuration.</p> name type title <code>str</code> Custom widget title. If not specified, a widget-specific default title is used. type <code>str</code> Set as <code>periodictable</code> to get this widget type.default: <code>periodictable</code> layout <code>Dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>. quantity <code>str</code> Targeted quantity. scale <code>str</code> Statistics scaling.options: - <code>linear</code> - <code>log</code> - <code>1/2</code> - <code>1/4</code> - <code>1/8</code>"},{"location":"howto/plugins/example_uploads.html","title":"How to write an example upload","text":"<p>Example uploads can be used to add representative collections of data for your plugin. Example uploads are available for end-users in the Uploads-page under the Add example uploads-button. There users can instantiate an example upload with a click. This can be very useful for educational or demonstration purposes but also for testing.</p> <p>This documentation shows you how to write a plugin entry point for an example upload. You should read the documentation on getting started with plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/example_uploads.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing an example upload. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 example_uploads\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 getting_started\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u251c\u2500\u2500 MANIFEST.in\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/example_uploads.html#example-upload-entry-point","title":"Example upload entry point","text":"<p>The entry point is an instance of a <code>ExampleUploadEntryPoint</code> or its subclass. It defines basic information about your example upload and is used to automatically load the associated data into a NOMAD distribution. The entry point should be defined in <code>*/example_uploads/__init__.py</code> like this:</p> <pre><code>from nomad.config.models.plugins import ExampleUploadEntryPoint\n\nmyexampleupload = ExampleUploadEntryPoint(\n    title = 'My Example Upload',\n    category = 'Examples',\n    description = 'Description of this example upload.',\n    path='example_uploads/getting_started\n)\n</code></pre> <p>The default method for including the upload data is to place it in the plugin repository and use the <code>path</code> field to specify the location with respect to the package root. You can learn more about different data loading options in the next section. In the reference you can also see all of the available configuration options for a <code>ExampleUploadEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for the example upload to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyexampleupload = \"nomad_example.example_uploads:myexampleupload\"\n</code></pre>"},{"location":"howto/plugins/example_uploads.html#including-data-in-an-example-upload","title":"Including data in an example upload","text":"<p>There are three main ways to include data in an example upload:</p> <ol> <li> <p>Data stored directly in the plugin package using <code>path</code>:</p> <p>This is the default method that assumes you simply store the data under a path in the plugin source code. This is very convenient if you have relative small example data and wish to track this in version control. The path should be given relative to the package installation location (<code>src/&lt;package-name&gt;</code>), and you should ensure that the data is distributed with your Python package. Distribution of additional data files in Python packages is controlled with the <code>MANIFEST.in</code> file. If you create a plugin with our template, the <code>src/&lt;package-name&gt;/example_uploads</code> folder is included automatically in <code>MANIFEST.in</code>. If you later add an example upload entry point to your plugin, remember to include the folder by adding the following line to <code>MANIFEST.in</code>:</p> <pre><code>graft src/&lt;package-name&gt;/&lt;path&gt;\n</code></pre> </li> <li> <p>Data retrieved online during app startup using <code>url</code>:</p> <p>If your example uploads are very large (&gt;100MB), storing them in Git may become unpractical. In order to deal with larger uploads, they can be stored in a separate online service. To load such external resources, you can specify a <code>url</code> parameter to activate online data retrieval. This will retrieve the large online file once upon the first app launch and then cache it for later use:</p> <pre><code>from nomad.config.models.plugins import ExampleUloadEntryPoint\n\nmyexampleupload = ExampleUploadEntryPoint(\n    name = 'MyExampleUpload',\n    description = 'My custom example upload.',\n    url='http://my_large_file_address.zip'\n)\n</code></pre> <p>Note that if the online file changes, you will need to remove the cached file for the new version to be retrieved. You can find the cached file in the package installation location, under folder <code>example_uploads</code>.</p> </li> <li> <p>Data retrieved with a custom method:</p> <p>If the above options do not suite your use case, you can also override the <code>load</code>-method of <code>ExampleUploadEntryPoint</code> to perform completely custom data loading logic. Note that the loaded data should be saved in the package installation directory in order to be accessible. Check the default <code>load</code> function for more details.</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import ExampleUploadEntryPoint\n\n\nclass MyExampleUploadEntryPoint(ExampleUploadEntryPoint):\n\n    def load(self):\n        \"\"\"Add your custom loading logic here.\"\"\"\n        ...\n</code></pre> </li> </ol>"},{"location":"howto/plugins/normalizers.html","title":"How to write a normalizer","text":"<p>A normalizer takes the archive of an entry as input and manipulates (usually expands) the given archive. This way, a normalizer can add additional sections and quantities based on the information already available in the archive. All normalizers are executed in the order determined by their <code>level</code> after parsing, but the normalizer may decide to not do anything based on the entry contents.</p> <p>This documentation shows you how to write a plugin entry point for a normaliser. You should read the documentation on getting started with plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/normalizers.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing a normalizer. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 normalizers\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mynormalizer.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/normalizers.html#normalizer-entry-point","title":"Normalizer entry point","text":"<p>The entry point defines basic information about your normalizer and is used to automatically load the normalizer code into a NOMAD distribution. It is an instance of a <code>NormalizerEntryPoint</code> or its subclass and it contains a <code>load</code> method which returns a <code>nomad.normalizing.Normalizer</code> instance that will perform the actual normalization. You will learn more about the <code>Normalizer</code> class in the next sections. The entry point should be defined in <code>*/normalizers/__init__.py</code> like this:</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import NormalizerEntryPoint\n\n\nclass MyNormalizerEntryPoint(NormalizerEntryPoint):\n\n    def load(self):\n        from nomad_example.normalizers.mynormalizer import MyNormalizer\n\n        return MyNormalizer(**self.dict())\n\n\nmynormalizer = MyNormalizerEntryPoint(\n    name = 'MyNormalizer',\n    description = 'My custom normalizer.',\n)\n</code></pre> <p>Here you can see that a new subclass of <code>NormalizerEntryPoint</code> was defined. In this new class you can override the <code>load</code> method to determine how the <code>Normalizer</code> class is instantiated, but you can also extend the <code>NormalizerEntryPoint</code> model to add new configurable parameters for this normalizer as explained here.</p> <p>We also instantiate an object <code>mynormalizer</code> from the new subclass. This is the final entry point instance in which you specify the default parameterization and other details about the normalizer. In the reference you can see all of the available configuration options for a <code>NormalizerEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for the normalizer to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmynormalizer = \"nomad_example.normalizers:mynormalizer\"\n</code></pre>"},{"location":"howto/plugins/normalizers.html#normalizer-class","title":"<code>Normalizer</code> class","text":"<p>The resource returned by a normalizer entry point must be an instance of a <code>nomad.normalizing.Normalizer</code> class. This normalizer definition should be contained in a separate file (e.g. <code>*/normalizer/mynormalizer.py</code>) and could look like this:</p> <pre><code>from typing import Dict\n\nfrom nomad.datamodel import EntryArchive\nfrom nomad.normalizing import Normalizer\n\n\nclass MyNormalizer(Normalizer):\n    def normalize(\n        self,\n        archive: EntryArchive,\n        logger=None,\n    ) -&gt; None:\n        logger.info('MyNormalizer called')\n</code></pre> <p>The minimal requirement is that your class has a <code>normalize</code> function, which as input takes:</p> <ul> <li><code>archive</code>: The <code>EntryArchive</code> object in which the normalization results will be stored</li> <li><code>logger</code>: Logger that you can use to log normalization events into</li> </ul>"},{"location":"howto/plugins/normalizers.html#systembasednormalizer-class","title":"<code>SystemBasedNormalizer</code> class","text":"<p><code>SystemBasedNormalizer</code> is a special base class for normalizing systems that allows to run the normalization on all (or only the resulting) <code>representative</code> systems:</p> <pre><code>from nomad.normalizing import SystemBasedNormalizer\nfrom nomad.atomutils import get_volume\n\nclass UnitCellVolumeNormalizer(SystemBasedNormalizer):\n    def _normalize_system(self, system, is_representative):\n        system.unit_cell_volume = get_volume(system.lattice_vectors.magnitude)\n</code></pre> <p>For <code>SystemBasedNormalizer</code>, we implement the <code>_normalize_system</code> method. The parameter <code>is_representative</code> will be true for the <code>representative</code> systems. The representative system refers to the system that corresponds to the calculation result. It is determined by scanning the archive sections starting with <code>workflow2</code> until the system fitting the criterion is found. For example, it refers to the final step in a geometry optimization or other workflow.</p> <p>Of course, if you add new information to the archive, this also needs to be defined in the schema (see How-to extend the schema). For example you could extend the section system with a special system definition that extends the existing section system definition:</p> <pre><code>import numpy as np\nfrom nomad.datamodel.metainfo import runschema\nfrom nomad.metainfo import Section, Quantity\n\nclass UnitCellVolumeSystem(runschema.system.System):\n    m_def = Section(extends_base_section=True)\n    unit_cell_volume = Quantity(np.dtype(np.float64), unit='m^3')\n</code></pre> <p>Here, we used the schema definition for the <code>run</code> section defined in this plugin.</p>"},{"location":"howto/plugins/normalizers.html#control-normalizer-execution-order","title":"Control normalizer execution order","text":"<p><code>NormalizerEntryPoints</code> have an attribute <code>level</code>, which you can use to control their execution order. Normalizers are executed in order from lowest level to highest level. The default level for normalizers is <code>0</code>, but this can be changed per installation using <code>nomad.yaml</code>:</p> <pre><code>plugins:\n  entry_points:\n    options:\n      \"nomad_example.normalizers:mynormalizer1\":\n        level: 1\n      \"nomad_example.normalizers:mynormalizer2\":\n        level: 2\n</code></pre>"},{"location":"howto/plugins/normalizers.html#running-the-normalizer","title":"Running the normalizer","text":"<p>If you have the plugin package and <code>nomad-lab</code> installed in your Python environment, you can run the normalization as a part of the parsing process using the NOMAD CLI:</p> <pre><code>nomad parse &lt;filepath&gt; --show-archive\n</code></pre> <p>The output will return the final archive in JSON format.</p> <p>Normalization can also be run within a python script (or Jupyter notebook), e.g., to facilate debugging, with the following code:</p> <pre><code>from nomad.datamodel import EntryArchive\nfrom nomad_example.normalizers.mynormalizer import MyNormalizer\nimport logging\n\np = MyNormalizer()\na = EntryArchive()\np.normalize(a, logger=logging.getLogger())\n\nprint(a.m_to_dict())\n</code></pre>"},{"location":"howto/plugins/normalizers.html#normalizers-developed-by-fairmat","title":"Normalizers developed by FAIRmat","text":"<p>The following is a list of plugins containing normalizers developed by FAIRmat:</p> Normalizer class Path/Project url SimulationWorkflowNormalizer https://github.com/nomad-coe/nomad-schema-plugin-simulation-workflow.git SystemNormalizer https://github.com/nomad-coe/nomad-normalizer-plugin-system.git SoapNormalizer https://github.com/nomad-coe/nomad-normalizer-plugin-soap.git SpectraNormalizer https://github.com/nomad-coe/nomad-normalizer-plugin-spectra.git DosNormalizer https://github.com/nomad-coe/nomad-normalizer-plugin-dos.git BandStructureNormalizer https://github.com/nomad-coe/nomad-normalizer-plugin-bandstructure.git <p>To refine an existing normalizer, you should install it via the <code>nomad-lab</code> package:</p> <pre><code>pip install nomad-lab\n</code></pre> <p>Clone the normalizer project:</p> <pre><code>git clone &lt;normalizer-project-url&gt;\ncd &lt;normalizer-dir&gt;\n</code></pre> <p>Either remove the installed normalizer and <code>pip install</code> the cloned version:</p> <pre><code>rm -rf &lt;path-to-your-python-env&gt;/lib/python3.11/site-packages/&lt;normalizer-module-name&gt;\npip install -e .\n</code></pre> <p>Or set <code>PYTHONPATH</code> so that the cloned code takes precedence over the installed code:</p> <pre><code>PYTHONPATH=. nomad parse &lt;path-to-example-file&gt;\n</code></pre> <p>Alternatively, you can also do a full developer setup of the NOMAD infrastructure and enhance the normalizer there.</p>"},{"location":"howto/plugins/parsers.html","title":"How to write a parser","text":"<p>NOMAD uses parsers to automatically extract information from raw files and output that information into structured archives. Parsers can decide which files act upon based on the filename, mime type or file contents and can also decide into which schema the information should be populated into.</p> <p>This documentation shows you how to write a plugin entry point for a parser. You should read the documentation on getting started with plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/parsers.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing a parser. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 parsers\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 myparser.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugin, including linting, testing and documenting.</p>"},{"location":"howto/plugins/parsers.html#parser-entry-point","title":"Parser entry point","text":"<p>The entry point defines basic information about your parser and is used to automatically load the parser code into a NOMAD distribution. It is an instance of a <code>ParserEntryPoint</code> or its subclass and it contains a <code>load</code> method which returns a <code>nomad.parsing.Parser</code> instance that will perform the actual parsing. You will learn more about the <code>Parser</code> class in the next sections. The entry point should be defined in <code>*/parsers/__init__.py</code> like this:</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import ParserEntryPoint\n\n\nclass MyParserEntryPoint(ParserEntryPoint):\n\n    def load(self):\n        from nomad_example.parsers.myparser import MyParser\n\n        return MyParser(**self.dict())\n\n\nmyparser = MyParserEntryPoint(\n    name = 'MyParser',\n    description = 'My custom parser.',\n    mainfile_name_re = '.*\\.myparser',\n)\n</code></pre> <p>Here you can see that a new subclass of <code>ParserEntryPoint</code> was defined. In this new class you can override the <code>load</code> method to determine how the <code>Parser</code> class is instantiated, but you can also extend the <code>ParserEntryPoint</code> model to add new configurable parameters for this parser as explained here.</p> <p>We also instantiate an object <code>myparser</code> from the new subclass. This is the final entry point instance in which you specify the default parameterization and other details about the parser. In the reference you can see all of the available configuration options for a <code>ParserEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for the parser to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyparser = \"nomad_example.parsers:myparser\"\n</code></pre>"},{"location":"howto/plugins/parsers.html#parser-class","title":"<code>Parser</code> class","text":"<p>The resource returned by a parser entry point must be an instance of a <code>nomad.parsing.Parser</code> class. In many cases you will, however, want to use the already existing <code>nomad.parsing.MatchingParser</code> subclass that takes care of the file matching process for you. This parser definition should be contained in a separate file (e.g. <code>*/parsers/myparser.py</code>) and could look like this:</p> <pre><code>from typing import Dict\n\nfrom nomad.datamodel import EntryArchive\nfrom nomad.parsing import MatchingParser\n\n\nclass MyParser(MatchingParser):\n    def parse(\n        self,\n        mainfile: str,\n        archive: EntryArchive,\n        logger=None,\n        child_archives: Dict[str, EntryArchive] = None,\n    ) -&gt; None:\n        logger.info('MyParser called')\n</code></pre> <p>If you are using the <code>MatchingParser</code> interface, the minimal requirement is that your class has a <code>parse</code> function, which will take as input:</p> <ul> <li><code>mainfile</code>: Filepath to a raw file that the parser should open and run on</li> <li><code>archive</code>: The <code>EntryArchive</code> object in which the parsing results will be stored</li> <li><code>logger</code>: Logger that you can use to log parsing events into</li> </ul> <p>Note here that if using <code>MatchingParser</code>, the process of identifying which files the <code>parse</code> method is run against is take care of by passing in the required parameters to the instance in the <code>load</code> mehod. In the previous section, the <code>load</code> method looked something like this:</p> <pre><code>    def load(self):\n        from nomad_example.parsers.myparser import MyParser\n\n        return MyParser(**self.dict())\n</code></pre> <p>There we are passing all of the entry configuration options to the parser instance, including things like <code>mainfile_name_re</code> and <code>mainfile_contents_re</code>. The <code>MatchingParser</code> constructor uses these parameters to set up the file matching appropriately. If you wish to take full control of the file matching process, you can use the <code>nomad.parsing.Parser</code> class and override the <code>is_mainfile</code> function.</p>"},{"location":"howto/plugins/parsers.html#match-your-raw-file","title":"Match your raw file","text":"<p>If you are using the <code>MatchingParser</code> interface you can configure which files are matched directly in the <code>ParserEntryPoint</code>. For example to match only certain file extensions and file contents, you can use the <code>mainfile_name_re</code> and <code>mainfile_contents_re</code> fields:</p> <pre><code>myparser = MyParserEntryPoint(\n    name = 'MyParser',\n    description = 'My custom parser.',\n    mainfile_name_re = '.*\\.myparser',\n    mainfile_contents_re = '\\s*\\n\\s*HELLO WORLD',\n)\n</code></pre> <p>You can find all of the available matching criteria in the <code>ParserEntryPoint</code> reference</p>"},{"location":"howto/plugins/parsers.html#running-the-parser","title":"Running the parser","text":"<p>If you have the plugin package and <code>nomad-lab</code> installed in your Python environment, you can run the parser against a file using the NOMAD CLI:</p> <pre><code>nomad parse &lt;filepath&gt; --show-archive\n</code></pre> <p>The output will return the final archive in JSON format.</p> <p>Parsing can also be run within a python script (or Jupyter notebook), e.g., to facilate debugging, with the following code:</p> <pre><code>from nomad.datamodel import EntryArchive\nfrom nomad_example.parsers.myparser import MyParser\nimport logging\n\np = ExampleParser()\na = EntryArchive()\np.parse('tests/data/example.out', a, logger=logging.getLogger())\n\nprint(a.m_to_dict())\n</code></pre>"},{"location":"howto/plugins/parsers.html#parsing-text-files","title":"Parsing text files","text":"<p>ASCII text files are amongst the most common files used. Here, we show you how to parse the text by matching specific regular expressions in these files. For the following example, we will use the project file <code>tests/data/example.out</code>:</p> <p>Check out the <code>master</code> branch of the <code>exampleparser</code> project,</p> <pre><code>git checkout master\n</code></pre> <p>and examine the file to be parsed in <code>tests/data/example.out</code>:</p> <pre><code>2020/05/15\n               *** super_code v2 ***\n\nsystem 1\n--------\nsites: H(1.23, 0, 0), H(-1.23, 0, 0), O(0, 0.33, 0)\nlatice: (0, 0, 0), (1, 0, 0), (1, 1, 0)\nenergy: 1.29372\n\n*** This was done with magic source                                ***\n***                                x\u00b042                            ***\n\n\nsystem 2\n--------\nsites: H(1.23, 0, 0), H(-1.23, 0, 0), O(0, 0.33, 0)\ncell: (0, 0, 0), (1, 0, 0), (1, 1, 0)\nenergy: 1.29372\n</code></pre> <p>At the top there is some general information such as date, name of the code (<code>super_code</code>) and its version (<code>v2</code>). Then is information for two systems (<code>system 1</code> and <code>system 2</code>), separated with a string containing a code-specific value <code>magic source</code>. Both system sections contain the quantities <code>sites</code> and <code>energy</code>, but each have a unique quantity as well, <code>latice</code> and <code>cell</code>, respectively.</p> <p>In order to convert the information from this file into the NOMAD archive, we first have to parse the necessary quantities. The <code>nomad-lab</code> Python package provides a <code>text_parser</code> module for declarative (i.e., semi-automated) parsing of text files. You can define text file parsers as follows:</p> <pre><code>def str_to_sites(string):\n    sym, pos = string.split('(')\n    pos = np.array(pos.split(')')[0].split(',')[:3], dtype=float)\n    return sym, pos\n\n\ncalculation_parser = TextParser(\n    quantities=[\n        Quantity(\n            'sites',\n            r'([A-Z]\\([\\d\\.\\, \\-]+\\))',\n            str_operation=str_to_sites,\n            repeats=True,\n        ),\n        Quantity(\n            Model.lattice,\n            r'(?:latice|cell): \\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*\\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*\\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*',\n            repeats=False,\n        ),\n        Quantity('energy', r'energy: (\\d\\.\\d+)'),\n        Quantity(\n            'magic_source',\n            r'done with magic source\\s*\\*{3}\\s*\\*{3}\\s*[^\\d]*(\\d+)',\n            repeats=False,\n        ),\n    ]\n)\n\nmainfile_parser = TextParser(\n    quantities=[\n        Quantity('date', r'(\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d)', repeats=False),\n        Quantity('program_version', r'super\\_code\\s*v(\\d+)\\s*', repeats=False),\n        Quantity(\n            'calculation',\n            r'\\s*system \\d+([\\s\\S]+?energy: [\\d\\.]+)([\\s\\S]+\\*\\*\\*)*',\n            sub_parser=calculation_parser,\n            repeats=True,\n        ),\n    ]\n)\n</code></pre> <p>The quantities to be parsed can be specified as a list of <code>Quantity</code> objects in <code>TextParser</code>. Each quantity should have a name and a regular expression (re) pattern to match the value. The matched value should be enclosed in a group(s) denoted by <code>(...)</code>. In addition, we can specify the following arguments:</p> <ul> <li><code>findall (default=True)</code>: Switches simultaneous matching of all quantities using <code>re.findall</code>. In this case, overlap between matches is not tolerated, i.e. two quantities cannot share the same block in the file. If this cannot be avoided, set <code>findall=False</code> switching to<code>re.finditer</code>. This will perform matching one quantity at a time which is slower but with the benefit that matching is done independently of other quantities.</li> <li><code>repeats (default=False)</code>: Switches finding multiple matches for a quantity. By default, only the first match is returned.</li> <li><code>str_operation (default=None)</code>: An external function to be applied on the matched value to perform more specific string operations. In the above example, we defined <code>str_to_sites</code> to convert the parsed value of the atomic sites.</li> <li><code>sub_parser (default=None)</code>: A nested parser to be applied on the matched block. This can also be a <code>TextParser</code> object with a list of quantities to be parsed or other <code>FileParser</code> objects.</li> <li><code>dtype (default=None)</code>: The data type of the parsed value.</li> <li><code>shape (default=None)</code>: The shape of the parsed data.</li> <li><code>unit (default=None)</code>: The pint unit of the parsed data.</li> <li><code>flatten (default=True)</code>: Switches splitting the parsed string into a flat list.</li> <li><code>convert (default=True)</code>: Switches automatic conversion of parsed value.</li> <li><code>comment (default=None)</code>: String preceding a line to ignore.</li> </ul> <p>A <code>metainfo.Quantity</code> object can also be passed as first argument in place of name in order to define the data type, shape, and unit for the quantity. <code>TextParser</code> returns a dictionary of key-value pairs, where the key is defined by the name of the quantities and the value is based on the matched re pattern.</p> <p>To parse a file, simply do: To parse a file, specify the path to such file and call the <code>parse()</code> function of <code>TextParser</code>:</p> <pre><code>mainfile_parser.mainfile = mainfile\nmainfile_parser.parse()\n</code></pre> <p>This will populate the <code>mainfile_parser</code> object with parsed data and it can be accessed like a Python dict with quantity names as keys or directly as attributes:</p> <pre><code>mainfile_parser.get('date')\n'2020/05/15'\n\nmainfile_parser.calculation\n[TextParser(example.out) --&gt; 4 parsed quantities (sites, lattice_vectors, energy, magic_source), TextParser(example.out) --&gt; 3 parsed quantities (sites, lattice_vectors, energy)]\n</code></pre> <p>The next step is to write the parsed data into the NOMAD archive. We can use one of the predefined plugins containing schema packages in NOMAD. However, to better illustrate the connection between a parser and a schema we will define our own schema in this example (See How to write a schema in python for additional information on this topic). We define a root section called <code>Simulation</code> containing two subsections, <code>Model</code> and <code>Output</code>. The definitions are found in <code>exampleparser/metainfo/example.py</code>:</p> <p><pre><code>class Model(ArchiveSection):\n    m_def = Section()\n\n    n_atoms = Quantity(\n        type=np.int32, description=\"\"\"Number of atoms in the model system.\"\"\"\n    )\n\n    labels = Quantity(\n        type=str, shape=['n_atoms'], description=\"\"\"Labels of the atoms.\"\"\"\n    )\n\n    positions = Quantity(\n        type=np.float64, shape=['n_atoms'], description=\"\"\"Positions of the atoms.\"\"\"\n    )\n\n    lattice = Quantity(\n        type=np.float64,\n        shape=[3, 3],\n        description=\"\"\"Lattice vectors of the model system.\"\"\",\n    )\n\nclass Output(ArchiveSection):\n    m_def = Section()\n\n    model = Quantity(\n        type=Reference(Model), description=\"\"\"Reference to the model system.\"\"\"\n    )\n\n    energy = Quantity(\n        type=np.float64,\n        unit='eV',\n        description=\"\"\"Value of the total energy of the system.\"\"\",\n    )\n\n\nclass Simulation(ArchiveSection):\n    m_def = Section()\n\n    code_name = Quantity(\n        type=str, description=\"\"\"Name of the code used for the simulation.\"\"\"\n    )\n\n    code_version = Quantity(type=str, description=\"\"\"Version of the code.\"\"\")\n\n    date = Quantity(type=Datetime, description=\"\"\"Execution date of the simulation.\"\"\")\n\n    model = SubSection(sub_section=Model, repeats=True)\n\n    output = SubSection(sub_section=Output, repeats=True)\n</code></pre> Each of the classes inherit from the base class <code>ArchiveSection</code>. This is the abstract class used in NOMAD to define sections and subsections in a schema. The <code>Model</code> section is used to store the <code>sites</code> and <code>lattice/cell</code> information, while the <code>Output</code> section is used to store the <code>energy</code> quantity. Each of the classes that we defined is a sub-class of <code>ArchiveSection</code>. This is required in order to assign these sections to the <code>data</code> section of the NOMAD archive.</p> <p>The following is the implementation of the <code>parse</code> function of <code>ExampleParser</code> to write the parsed quantities from our mainfile parser into the archive:</p> <p><pre><code>def parse(self, mainfile: str, archive: EntryArchive, logger):\n    simulation = Simulation(\n        code_name='super_code', code_version=mainfile_parser.get('program_version')\n    )\n    date = datetime.datetime.strptime(mainfile_parser.date, '%Y/%m/%d')\n    simulation.date = date\n\n    for calculation in mainfile_parser.get('calculation', []):\n        model = Model()\n        model.lattice = calculation.get('lattice_vectors')\n        sites = calculation.get('sites')\n        model.labels = [site[0] for site in sites]\n        model.positions = [site[1] for site in sites]\n        simulation.model.append(model)\n\n        output = Output()\n        output.model = model\n        output.energy = calculation.get('energy') * units.eV\n        magic_source = calculation.get('magic_source')\n        if magic_source is not None:\n            archive.workflow2 = Workflow(x_example_magic_value=magic_source)\n        simulation.output.append(output)\n    # put the simulation section into archive data\n    archive.data = simulation\n</code></pre> We first assign the code name and version as well as the date that the simulation was performed. For each of the parsed calculations, we create a model and an output section to which we write the corresponding parsed quantities. Finally, we assign the simulation section to the archive data subsection.</p> <p>Now, run the parser again and check that the new archive stores the intended quantities from <code>tests/data/example.out</code>.</p> <p>Additionally, the standard normalizers will be applied as well. This is run automatically during parsing, one can skip these by passing the argument <code>skip-normalizers</code>.</p>"},{"location":"howto/plugins/parsers.html#extending-the-metainfo","title":"Extending the Metainfo","text":"<p>There are several built-in schemas NOMAD (<code>nomad.datamodel.metainfo</code>).</p> <p>In the example above, we have made use of the base section for workflow and extended it to include a code-specific quantity <code>x_example_magic_value</code>. <pre><code># We extend the existing common definition of section Workflow\nclass ExampleWorkflow(Workflow):\n    # We alter the default base class behavior to add all definitions to the existing\n    # base class instead of inheriting from the base class\n    m_def = Section(extends_base_section=True)\n\n    # We define an additional example quantity. Use the prefix x_&lt;parsername&gt;_ to denote\n    # non common quantities.\n    x_example_magic_value = Quantity(\n        type=int, description='The magic value from a magic source.'\n    )\n</code></pre></p> <p>This is the approach for domain-specific schemas such as for simulation workflows. Refer to how to extend schemas.</p>"},{"location":"howto/plugins/parsers.html#other-fileparser-classes","title":"Other FileParser classes","text":"<p>Aside from <code>TextParser</code>, other <code>FileParser</code> classes are also defined. These include:</p> <ul> <li> <p><code>DataTextParser</code>: in addition to matching strings as in <code>TextParser</code>, this parser uses the <code>numpy.loadtxt</code> function to load structured data files. The loaded <code>numpy.array</code> data can then be accessed from the property data.</p> </li> <li> <p><code>XMLParser</code>: uses the ElementTree module to parse an XML file. The <code>parse</code> method of the parser takes in an XPath-style key to access individual quantities. By default, automatic data type conversion is performed, which can be switched off by setting <code>convert=False</code>.</p> </li> </ul>"},{"location":"howto/plugins/parsers.html#parsers-developed-by-fairmat","title":"Parsers developed by FAIRmat","text":"<p>The following is a list of plugins containin parsers developed by FAIRmat:</p> Description Project url electronic structure codes https://github.com/nomad-coe/electronic-parsers.git atomistic codes https://github.com/nomad-coe/atomistic-parsers.git workflow engines https://github.com/nomad-coe/workflow-parsers.git databases https://github.com/nomad-coe/database-parsers.git <p>To refine an existing parser, you should install the parser via the <code>nomad-lab</code> package:</p> <pre><code>pip install nomad-lab\n</code></pre> <p>Clone the parser project:</p> <pre><code>git clone &lt;parser-project-url&gt;\ncd &lt;parser-dir&gt;\n</code></pre> <p>Either remove the installed parser and <code>pip install</code> the cloned version:</p> <pre><code>rm -rf &lt;path-to-your-python-env&gt;/lib/python3.11/site-packages/&lt;parser-module-name&gt;\npip install -e .\n</code></pre> <p>Or set <code>PYTHONPATH</code> so that the cloned code takes precedence over the installed code:</p> <pre><code>PYTHONPATH=. nomad parse &lt;path-to-example-file&gt;\n</code></pre> <p>Alternatively, you can also do a full developer setup of the NOMAD infrastructure and enhance the parser there.</p>"},{"location":"howto/plugins/plugins.html","title":"Get started with plugins","text":"<p>The main way to customize a NOMAD installation is through the use of plugins. A NOMAD Plugin is a Git repository that contains a Python package that an administrator can install into a NOMAD deployment to add custom features. This page contains the basics of how to create, develop and publish a NOMAD Plugin.</p>"},{"location":"howto/plugins/plugins.html#plugin-anatomy","title":"Plugin anatomy","text":"<p>Tip</p> <p>We provide a template repository which you can use to create the initial plugin layout for you.</p> <p>Plugin Git repositories should roughly follow this layout:</p> <pre><code>\u251c\u2500\u2500 nomad-example\n\u2502   \u251c\u2500\u2500 src\n|   \u2502   \u251c\u2500\u2500 nomad_example\n|   |   \u2502   \u251c\u2500\u2500 apps\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n|   |   \u2502   \u251c\u2500\u2500 normalizers\n|   |   \u2502   \u2502   \u251c\u2500\u2500 mynormalizer.py\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n|   |   \u2502   \u251c\u2500\u2500 schema_packages\n|   |   \u2502   \u2502   \u251c\u2500\u2500 mypackage.py\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n|   |   \u2502   \u251c\u2500\u2500 parsers\n|   |   \u2502   \u2502   \u251c\u2500\u2500 myparser.py\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u251c\u2500\u2500 LICENSE.txt\n\u2502   \u251c\u2500\u2500 README.md\n</code></pre> <p>We suggest using the following convention for naming the repository name and the plugin package:</p> <ul> <li>repository name: <code>nomad-&lt;plugin name&gt;</code></li> <li>package name: <code>nomad_&lt;plugin name&gt;</code></li> </ul> <p>In the folder structure you can see that a single plugin can contain multiple types of customizations: apps, parsers, schema packages and normalizers. These are called a plugin entry points and you will learn more about them next.</p>"},{"location":"howto/plugins/plugins.html#plugin-entry-points","title":"Plugin entry points","text":"<p>Plugin entry points represent different types of customizations that can be added to a NOMAD installation. The following plugin entry point types are currently supported:</p> <ul> <li>Apps</li> <li>Example uploads</li> <li>Normalizers</li> <li>Parsers</li> <li>Schema packages</li> </ul> <p>Entry points contain configuration, but also a separate resource, which should live in a separate Python module. This split enables lazy-loading: the configuration can be loaded immediately, while the resource is loaded later when/if it is required. This can significantly improve startup times, as long as all time-consuming initializations are performed only when loading the resource. This split also helps to avoid cyclical imports between the plugin code and the <code>nomad-lab</code> package.</p> <p>For example the entry point instance for a parser is contained in <code>.../parsers/__init__.py</code> and it contains e.g. the name, version and any additional entry point-specific parameters that control its behaviour. The entry point has a <code>load</code> method than can be called lazily to return the resource, which is a <code>Parser</code> instance defined in <code>.../parsers/myparser.py</code>.</p> <p>In <code>pyproject.toml</code> you can expose plugin entry points for automatic discovery. E.g. to expose an app and a package, you would add the following to <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyapp = \"nomad_example.parsers:myapp\"\nmypackage = \"nomad_example.schema_packages:mypackage\"\n</code></pre> <p>Here it is important to use the <code>nomad.plugin</code> group name in the <code>project.entry-points</code> header. The value on the right side (<code>\"nomad_example.schema_packages:mypackage\"</code>) must be a path pointing to a plugin entry point instance inside the python code. This unique key will be used to identify the plugin entry point when e.g. accessing it to read some of it's configuration values. The name on the left side (<code>mypackage</code>) can be set freely.</p> <p>You can read more about how to write different types of entry points in their dedicated documentation pages or learn more about the Python entry point mechanism.</p>"},{"location":"howto/plugins/plugins.html#controlling-loading-of-plugin-entry-points","title":"Controlling loading of plugin entry points","text":"<p>By default, plugin entry points are automatically loaded, and as an administrator you only need to install the Python package. You can, however, control which entry points to load by explicitly including/excluding them in your <code>nomad.yaml</code>. For example, if a plugin has the following <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyparser = \"nomad_example.parsers:myparser\"\n</code></pre> <p>You could disable the parser entry point in your <code>nomad.yaml</code> with:</p> <pre><code>plugins:\n  entry_points:\n    exclude: [\"nomad_plugin.parsers:myparser\"]\n</code></pre>"},{"location":"howto/plugins/plugins.html#extending-and-using-the-entry-point","title":"Extending and using the entry point","text":"<p>The plugin entry point is an instance of a <code>pydantic</code> model. This base model may already contain entry point-specific fields (such as the file extensions that a parser plugin will match) but it is also possible to extend this model to define additional fields that control your plugin behaviour.</p> <p>To specify new configuration options, you can add new <code>pydantic</code> fields to the subclass. For example, if we wanted to add a new configuration option for a parser, we could do the following:</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import ParserEntryPoint\n\n\nclass MyParserEntryPoint(ParserEntryPoint):\n    parameter: int = Field(0, description='Config parameter for this parser.')\n</code></pre> <p>where we have defined a new subclass of <code>ParserEntryPoint</code> and added a new configuration field <code>parameter</code>. The plugin users can then control these settings in their <code>nomad.yaml</code> using <code>plugins.entry_points.options</code>:</p> <pre><code>plugins:\n  entry_points:\n    options:\n      \"nomad_example.parsers:myparser\":\n        parameter: 47\n</code></pre> <p>Note that the model will also validate the values coming from <code>nomad.yaml</code>, and you should utilize the validation mechanisms of <code>pydantic</code> to provide users with helpful messages about invalid configuration.</p> <p>In your code, you can then access the whole entry point by loading it with <code>config.get_plugin_entry_point</code>:</p> <pre><code>from nomad.config import config\n\nconfiguration = config.get_plugin_entry_point('nomad_example.parsers:myparser')\nprint(f'The parser parameter is: {configuration.parameter}')\n</code></pre>"},{"location":"howto/plugins/plugins.html#plugin-development-guidelines","title":"Plugin development guidelines","text":""},{"location":"howto/plugins/plugins.html#linting-and-formatting","title":"Linting and formatting","text":"<p>While developing NOMAD plugins, we highly recommend using a Python linter, such as Ruff, to analyze and enforce coding standards in your plugin projects. This also ensures smoother integration and collaboration. If you have used our template repository, you will automatically have <code>ruff</code> defined as a development dependency with suitable defaults set in <code>pyproject.toml</code> together with a GitHub actions that runs the linting and formatting checks on each push to the Git repository.</p>"},{"location":"howto/plugins/plugins.html#testing","title":"Testing","text":"<p>For testing, you should use pytest, and a folder structure that mimics the package layout with test modules named after the tested module. For example, if you are developing a parser in <code>myparser.py</code>, the test folder structure should look like this:</p> <pre><code>\u251c\u2500\u2500 nomad-example-plugin\n\u2502   \u251c\u2500\u2500 src\n|   \u2502   \u251c\u2500\u2500 nomad_example\n|   |   \u2502   \u251c\u2500\u2500 parsers\n|   |   \u2502   \u2502   \u251c\u2500\u2500 myparser.py\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 tests\n|   \u2502   \u251c\u2500\u2500 parsers\n|   |   \u2502   \u251c\u2500\u2500 test_myparser.py\n|   |   \u2502   \u251c\u2500\u2500 conftest.py\n|   \u2502   \u251c\u2500\u2500 conftest.py\n</code></pre> <p>Any shared test utilities (such as <code>pytest</code> fixtures) should live in <code>conftest.py</code> modules placed at the appropriate level in the folder hierarchy, i.e. utilities dealing with parsers would live in <code>tests/parsers/conftest.py</code>, while root level utilities would live in <code>tests/conftest.py</code>. If you have used our template repository, you will automatically have an initial test folder structure, <code>pytest</code> defined as a development dependency in <code>pyproject.toml</code> and a GitHub action that runs the test suite on each push to the Git repository.</p> <p>In the <code>pytest</code> framework, test cases are created by defining functions with the <code>test_</code> prefix, which perform assertions. A typical test case could look like this:</p> <pre><code>def test_parse_file():\n    parser = MyParser()\n    archive = EntryArchive()\n    parser.parse('tests/data/example.out', archive, logging)\n\n    sim = archive.data\n    assert len(sim.model) == 2\n    assert len(sim.output) == 2\n    assert archive.workflow2.x_example_magic_value == 42\n</code></pre> <p>You can run all the tests in the <code>tests/</code> directory with:</p> <pre><code>python -m pytest -svx tests\n</code></pre>"},{"location":"howto/plugins/plugins.html#documentation","title":"Documentation","text":"<p>As your plugin matures, you should also think about documenting its usage. We recommend using <code>mkdocs</code> to create your documentation as a set of markdown files. If you have used our template repository, you will automatically have an initial documentation folder structure, <code>mkdocs</code> defined as a development dependency in <code>pyproject.toml</code> and a GitHub action that builds the docs to a separate <code>gh-pages</code> branch each push to the Git repository. Note that if you wish to host the documentation using GitHub pages, you need to enable this in the repository settings.</p>"},{"location":"howto/plugins/plugins.html#publishing-a-plugin","title":"Publishing a plugin","text":"<p>Attention</p> <p>The standard processes for publishing plugins and using plugins from other developers are still being worked out. The \"best\" practices mentioned in the following are preliminary. We aim to set up a dedicated plugin registry that allows you to publish your plugin and find plugins from others.</p>"},{"location":"howto/plugins/plugins.html#github-repository","title":"GitHub repository","text":"<p>The simplest way to publish a plugin is to have it live in a publicly shared Git repository. The package can then be installed with:</p> <pre><code>pip install git+https://&lt;repository_url&gt;\n</code></pre> <p>Note</p> <p>If you develop a plugin in the context of FAIRmat or the NOMAD CoE, put your plugin repositories in the corresponding GitHub organization.</p>"},{"location":"howto/plugins/plugins.html#pypipip-package","title":"PyPI/pip package","text":"<p>You may additionally publish the plugin package in PyPI. Learn from the PyPI documentation how to create a package for PyPI. We recommend to use the <code>pyproject.toml</code>-based approach.</p> <p>The PyPI documentation provides further information about how to publish a package to PyPI. If you have access to the MPCDF GitLab and NOMAD's presence there, you can also use the <code>nomad-FAIR</code> package registry:</p> <pre><code>pip install twine\ntwine upload \\\n    -u &lt;username&gt; -p &lt;password&gt; \\\n    --repository-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi \\\n    dist/nomad-example-plugin-*.tar.gz\n</code></pre>"},{"location":"howto/plugins/plugins.html#installing-a-plugin","title":"Installing a plugin","text":"<p>See our documentation on How to install plugins into a NOMAD Oasis.</p>"},{"location":"howto/plugins/schema_packages.html","title":"How to write a schema package","text":"<p>Schema packages are used to define and distribute custom data definitions that can be used within NOMAD. These schema packages typically contain schemas that users can select to instantiate manually filled entries using our ELN functionality, or that parsers when organizing data they extract from files. Schema packages may also contain more abstract base classes that other schema packages use.</p> <p>This documentation shows you how to write a plugin entry point for a schema package. You should read the documentation on getting started with plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/schema_packages.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing a schema package. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 schema_packages\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mypackage.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/schema_packages.html#schema-package-entry-point","title":"Schema package entry point","text":"<p>The entry point defines basic information about your schema package and is used to automatically load it into a NOMAD distribution. It is an instance of a <code>SchemaPackageEntryPoint</code> or its subclass and it contains a <code>load</code> method which returns a <code>nomad.metainfo.SchemaPackage</code> instance that contains section and schema definitions. You will learn more about the <code>SchemaPackage</code> class in the next sections. The entry point should be defined in <code>*/schema_packages/__init__.py</code> like this:</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import SchemaPackageEntryPoint\n\n\nclass MySchemaPackageEntryPoint(SchemaPackageEntryPoint):\n\n    def load(self):\n        from nomad_example.schema_packages.mypackage import m_package\n\n        return m_package\n\n\nmypackage = MySchemaPackageEntryPoint(\n    name = 'MyPackage',\n    description = 'My custom schema package.',\n)\n</code></pre> <p>Here you can see that a new subclass of <code>SchemaPackageEntryPoint</code> was defined. In this new class you can override the <code>load</code> method to determine how the <code>SchemaPackage</code> class is loaded, but you can also extend the <code>SchemaPackageEntryPoint</code> model to add new configurable parameters for this schema package as explained here.</p> <p>We also instantiate an object <code>mypackage</code> from the new subclass. This is the final entry point instance in which you specify the default parameterization and other details about the schema package. In the reference you can see all of the available configuration options for a <code>SchemaPackageEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for it to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmypackage = \"nomad_example.schema_packages:mypackage\"\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#schemapackage-class","title":"<code>SchemaPackage</code> class","text":"<p>The <code>load</code>-method of a schema package entry point returns an instance of a <code>nomad.metainfo.SchemaPackage</code> class. This definition should be contained in a separate file (e.g. <code>*/schema_packages/mypackage.py</code>) and could look like this:</p> <pre><code>from nomad.datamodel.data import Schema\nfrom nomad.datamodel.metainfo.annotations import ELNAnnotation, ELNComponentEnum\nfrom nomad.metainfo import SchemaPackage, Quantity, MSection\n\nm_package = SchemaPackage()\n\n\nclass System(MSection):\n    '''\n    A system section includes all quantities that describe a single simulated\n    system (a.k.a. geometry).\n    '''\n\n    n_atoms = Quantity(\n        type=int, description='''\n        Defines the number of atoms in the system.\n        ''')\n\n    atom_labels = Quantity(\n        type=MEnum(ase.data.chemical_symbols), shape['n_atoms'])\n    atom_positions = Quantity(type=float, shape=['n_atoms', 3], unit='angstrom')\n    simulation_cell = Quantity(type=float, shape=[3, 3], unit='angstrom')\n    pbc = Quantity(type=bool, shape=[3])\n\n\nclass Simulation(Schema):\n    system = SubSection(sub_section=System, repeats=True)\n\nm_package.__init_metainfo__()\n</code></pre> <p>Schema packages typically contain one or several schema definitions, that can the be used to manually create new entries through the ELN functionality, or also by parsers to create instances of this schema fully automatically. All of the definitions contained in the package should be placed between the contructor call (<code>m_package = SchemaPackage()</code>) and the initialization (<code>m_package.__init_metainfo__()</code>).</p> <p>In this basic example we defined two sections: <code>System</code> and <code>Simulation</code>. <code>System</code> inherits from most primitive type of section - <code>MSection</code> - whereas <code>Simulation</code> is defined as a subclass of <code>Schema</code> which makes it possible to use this as the root section of an entry. Each section can have two types of properties: quantities and subsections. Sections and their properties are defined with Python classes and their attributes. Each quantity defines a piece of data. Basic quantity attributes are <code>type</code>, <code>shape</code>, <code>unit</code>, and <code>description</code>.</p> <p>Subsections allow the placement of sections within each other, forming containment hierarchies. Basic subsection attributes are <code>sub_section</code>\u2014a reference to the section definition of the subsection\u2014and <code>repeats</code>\u2014determines whether a subsection can be included once or multiple times.</p> <p>To use the above-defined schema and create actual data, we have to instantiate the classes:</p> <pre><code>run = Run()\nsystem = run.m_create(System)\nsystem.n_atoms = 3\nsystem.atom_labels = ['H', 'H', 'O']\n\nprint(system.atom_labels)\nprint(n_atoms = 3)\n</code></pre> <p>Section instances can be used like regular Python objects: quantities and subsections can be set and accessed like any other Python attribute. Special metainfo methods, starting with <code>m_</code> allow us to realize more complex semantics. For example <code>m_create</code> will instantiate a subsection and add it to the parent section in one step.</p> <p>Another example for an <code>m_</code>-method is:</p> <pre><code>run.m_to_json(indent=2)\n</code></pre> <p>This will convert the data into JSON:</p> <pre><code>{\n    \"m_def\" = \"Run\",\n    \"systems\": [\n        {\n            \"n_atoms\" = 3,\n            \"atom_labels\" = [\n                \"H\",\n                \"H\",\n                \"O\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#schema-packages-python-vs-yaml","title":"Schema packages: Python vs. YAML","text":"<p>In this guide, we explain how to write and upload schema packages in the <code>.archive.yaml</code> format. Writing and uploading such YAML schema packages is a good way for NOMAD users to start exploring schemas, but it has limitations. As a NOMAD developer or Oasis administrator you can add Python schema packages to NOMAD. All built-in NOMAD schemas (e.g. for electronic structure code data) are written in Python and are part of the NOMAD sources (<code>nomad.datamodel.metainfo.*</code>).</p> <p>There is a 1-1 translation between the structure in Python schema packages (written in classes) and YAML (or JSON) schema packages (written in objects). Both use the same fundamental concepts, like section, quantity, or subsection, introduced in YAML schemas. The main benefit of Python schema packages is the ability to define custom <code>normalize</code>-functions.</p> <p><code>normalize</code>-functions are attached to sections and are are called when instances of these sections are processed. All files are processed when they are uploaded or changed. To add a <code>normalize</code> function, your section has to inherit from <code>Schema</code> or <code>ArchiveSection</code> which provides the base for this functionality. Here is an example:</p> <pre><code>from nomad.datamodel import Schema, ArchiveSection\nfrom nomad.metainfo.metainfo import Quantity, Datetime, SubSection\n\n\nclass Sample(ArchiveSection):\n    added_date = Quantity(type=Datetime)\n    formula = Quantity(type=str)\n\n    sample_id = Quantity(type=str)\n\n    def normalize(self, archive, logger):\n        super(Sample, self).normalize(archive, logger)\n\n        if self.sample_id is None:\n            self.sample_id = f'{self.added_date}--{self.formula}'\n\n\nclass SampleDatabase(Schema):\n    samples = SubSection(section=Sample, repeats=True)\n</code></pre> <p>Make sure to call the <code>super</code> implementation properly to support multiple inheritance. In order to control the order by which the <code>normalize</code> calls are executed, one can define <code>normalizer_level</code> which is set to 0 by default. The normalize functions are always called for any sub section before the parent section. However, the order for any sections on the same level will be from low values of <code>normalizer_level</code> to high.</p> <p>If we parse an archive like this:</p> <pre><code>data:\n  m_def: 'examples.archive.custom_schema.SampleDatabase'\n  samples:\n    - formula: NaCl\n      added_date: '2022-06-18'\n</code></pre> <p>we will get a final normalized archive that contains our data like this:</p> <pre><code>{\n  \"data\": {\n    \"m_def\": \"examples.archive.custom_schema.SampleDatabase\",\n    \"samples\": [\n      {\n        \"added_date\": \"2022-06-18T00:00:00+00:00\",\n        \"formula\": \"NaCl\",\n        \"sample_id\": \"2022-06-18 00:00:00+00:00--NaCl\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#migration-guide","title":"Migration guide","text":"<p>By default, schema packages are identified by the full qualified path to the Python module that contains the definitions. An example of a full qualified path could be <code>nomad_example.schema_packages.mypackage</code>, where the first part is the Python package name, second part is a subpackage, and the last part is a Python module containing the definitions. This is the easiest way to prevent conflicts between different schema packages: python package names are unique (prevents clashes between packages) and paths inside a package must point to a single python module (prevents clashes within package). This does, however, mean that if you move your schema definition in the plugin source code, any references to the old definition will break. This becomes problematic in installations that have lot of old data processed with the old definition location, as those entries will still refer to the old location and will not work correctly.</p> <p>As it might not be possible, or even wise to prevent changes in the source code layout, and reprocessing all old entries might be impractical, we do provide an alias mechanism to help with migration tasks. Imagine your schema package was contained in <code>nomad_example.schema_packages.mypackage</code>, and in a newer version of your plugin you want to move it to <code>nomad_example.schema_packages.mynewpackage</code>. The way to do this without completely breaking the old entries is to add an alias in the schema package definition:</p> <pre><code>m_package = SchemaPackage(aliases=['nomad_example.schema_packages.mypackage'])\n</code></pre> <p>Note that this will only help in scenarious where you have moved the definition and not removed or modified any of them.</p>"},{"location":"howto/plugins/schema_packages.html#definitions","title":"Definitions","text":"<p>The following describes in detail the schema language for the NOMAD Metainfo and how it is expressed in Python.</p>"},{"location":"howto/plugins/schema_packages.html#common-attributes-of-metainfo-definitions","title":"Common attributes of Metainfo Definitions","text":"<p>In the example, you have already seen the basic Python interface to the Metainfo. Sections are represented in Python as objects. To define a section, you write a Python class that inherits from <code>MSection</code>. To define subsections and quantities you use Python properties. The definitions themselves are also objects derived from classes. For subsections and quantities, you directly instantiate <code>:class:SubSection</code> and <code>:class:Quantity</code>. For sections there is a generated object derived from <code>:class:Section</code> and available via <code>m_def</code> from each section class and section instance.</p> <p>These Python classes, used to represent metainfo definitions, form an inheritance hierarchy to share common properties</p> <ul> <li><code>name</code>: each definition has a name. This is typically defined by the corresponding Python property. For example, a section class name becomes the section name; a quantity gets the name from the variable name used in its Python definition, etc.</li> <li><code>description</code>: each definition should have one. Either set it directly or use doc strings</li> <li><code>links</code>: a list of useful internet references.</li> <li><code>more</code>: a dictionary of custom information. Any additional <code>kwargs</code> set when creating a definition     are added to <code>more</code>.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#sections","title":"Sections","text":"<p>Sections are defined with Python classes that extend <code>MSection</code> (or other section classes).</p> <ul> <li><code>base_sections</code>: automatically taken from the base classes of the Python class.</li> <li><code>extends_base_section</code>: a boolean that determines the inheritance. If this is <code>False</code>, normal Python inheritance implies and this section will inherit all properties (subsections, quantities) from all base classes. If <code>True</code>, all definitions in this section will be added to the properties of the base class section. This allows the extension of existing sections with additional properties.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#quantities","title":"Quantities","text":"<p>Quantity definitions are the main building block of metainfo schemas. Each quantity represents a single piece of data. Quantities can be defined with the following attributes:</p> <ul> <li><code>type</code>: can be a primitive Python type (<code>str</code>, <code>int</code>, <code>bool</code>), a numpy data type (<code>np.dtype('float64')</code>), an <code>MEnum('item1', ..., 'itemN')</code>, a predefined metainfo type (<code>Datetime</code>, <code>JSON</code>, <code>File</code>, ...), or another section or quantity to define a reference type.</li> <li><code>shape</code>: defines the dimensionality of the quantity. Examples are: <code>[]</code> (number), <code>['*']</code> (list), <code>[3, 3]</code> (3 by 3 matrix), <code>['n_elements']</code> (a vector of length defined by another quantity <code>n_elements</code>).</li> <li><code>unit</code>: a physical unit. We use Pint here. You can use unit strings that are parsed by Pint, e.g. <code>meter</code>, <code>m</code>, <code>m/s^2</code>. As a convention the NOMAD Metainfo uses only SI units.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#subsection","title":"SubSection","text":"<p>A subsection defines a named property of a section that refers to another section. It allows to define that a section that contains another section.</p> <ul> <li><code>sub_section</code>: (aliases <code>section_def</code>, <code>sub_section_def</code>) defines the section that can be contained.</li> <li><code>repeats</code>: a boolean that determines whether the subsection relationship allows multiple sections or only one.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#references-and-proxies","title":"References and Proxies","text":"<p>Besides creating hierarchies with subsections (e.g. tree structures), the metainfo also allows one to create a reference within a section that points to either another section or a quantity value:</p> <pre><code>class Calculation(MSection):\n    system = Quantity(type=System.m_def)\n    atom_labels = Quantity(type=System.atom_labels)\n\ncalc = Calculation()\ncalc.system = run.systems[-1]\ncalc.atom_labels = run.systems[-1]\n</code></pre> <p>To define a reference, define a normal quantity and simply use the section or quantity you want to refer to as type. Then you can assign respective section instances as values.</p> <p>In Python memory, quantity values that reference other sections simply contain a Python reference to the respective section instance. However, upon serializing/storing metainfo data, these references have to be represented differently.</p> <p>Value references work a little differently. When you read a value reference, it behaves like the reference value. Internally, we do not store the values, but instead a reference to the section that holds the referenced quantity is stored. Therefore, when you want to assign a value reference, use the section with the quantity and not the value itself.</p> <p>References are serialized as URLs. There are different types of reference URLs:</p> <ul> <li><code>#/run/0/calculation/1</code>: a reference in the same Archive</li> <li><code>/run/0/calculation/1</code>: a reference in the same archive (legacy version)</li> <li><code>../upload/archive/mainfile/{mainfile}#/run/0</code>: a reference into an Archive of the same upload</li> <li><code>/entries/{entry_id}/archive#/run/0/calculation/1</code>: a reference into the Archive of a different entry on the same NOMAD installation</li> <li><code>/uploads/{upload_id}/archive/{entry_id}#/run/0/calculation/1</code>: similar to the previous one but based on uploads</li> <li><code>https://myoasis.de/api/v1/uploads/{upload_id}/archive/{entry_id}#/run/0/calculation/1</code>: a global reference towards a different NOMAD installation (Oasis)</li> </ul> <p>The host and path parts of URLs correspond with the NOMAD API. The anchors are paths from the root section of an Archive, over its subsections, to the referenced section or quantity value. Each path segment is the name of the subsection or an index in a repeatable subsection: <code>/system/0</code> or <code>/system/0/atom_labels</code>.</p> <p>References are automatically serialized by <code>:py:meth:MSection.m_to_dict</code>. When de-serializing data with <code>:py:meth:MSection.m_from_dict</code> these references are not resolved right away, because the reference section might not yet be available. Instead references are stored as <code>:class:MProxy</code> instances. These objects are automatically replaced by the referenced object when a respective quantity is accessed.</p> <p>If you want to define references, it might not be possible to define the referenced section or quantity beforehand, due to the way Python definitions and imports work. In these cases, you can use a proxy to reference the reference type. There is a special proxy implementation for sections:</p> <pre><code>class Calculation(MSection):\n    system = Quantity(type=SectionProxy('System')\n</code></pre> <p>The strings given to <code>SectionProxy</code> are paths within the available definitions. The above example works, if <code>System</code> is eventually defined in the same package.</p>"},{"location":"howto/plugins/schema_packages.html#categories","title":"Categories","text":"<p>In the old metainfo this was known as abstract types.</p> <p>Categories are defined with Python classes that have <code>:class:MCategory</code> as base class. Their name and description are taken from the name and docstring of the class. An example category looks like this:</p> <pre><code>class CategoryName(MCategory):\n    ''' Category description '''\n    m_def = Category(links=['http://further.explanation.eu'], categories=[ParentCategory])\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#adding-python-schemas-to-nomad","title":"Adding Python schemas to NOMAD","text":"<p>The following describes how to integrate new schema modules into the existing code according to best practices.</p>"},{"location":"howto/plugins/schema_packages.html#schema-super-structure","title":"Schema super structure","text":"<p>You should follow the basic developer's getting started to setup a development environment. This will give you all the necessary libraries and allows you to place your modules into the NOMAD code.</p> <p>The <code>EntryArchive</code> section definition sets the root of the archive for each entry in NOMAD. It therefore defines the top level sections:</p> <ul> <li><code>metadata</code>: all \"administrative\" metadata (ids, permissions, publish state, uploads, user metadata, etc.)</li> <li><code>results</code>: a summary with copies and references to data from method specific sections. This also presents the searchable metadata.</li> <li><code>workflows</code>: all workflow metadata</li> <li>Method-specific subsections: e.g. <code>run</code>. This is were all parsers are supposed to add the parsed data.</li> </ul> <p>The main NOMAD Python project includes Metainfo definitions in the following modules:</p> <ul> <li><code>nomad.metainfo</code>: defines the Metainfo itself. This includes a self-referencing schema. E.g. there is a section <code>Section</code>, etc.</li> <li><code>nomad.datamodel</code>: defines the section <code>metadata</code> that contains all \"administrative\" metadata. It also contains the root section <code>EntryArchive</code>.</li> <li><code>nomad.datamodel.metainfo</code>: defines all the central, method specific (but not parser specific) definitions. For example the section <code>run</code> with all the simulation definitions (computational material science definitions) that are shared among the respective parsers.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#extending-existing-sections","title":"Extending existing sections","text":"<p>Parsers can provide their own definitions. By convention, these are placed into a <code>metainfo</code> sub-module of the parser Python module. The definitions here can add properties to existing sections (e.g. from <code>nomad.datamodel.metainfo</code>). By convention, use a <code>x_mycode_</code> prefix. This is done with the <code>extends_base_section</code> Section property. Here is an example:</p> <pre><code>from nomad.metainfo import Section\nfrom nomad.datamodel.metainfo.workflow import Workflow\n\nclass MyCodeRun(Workflow)\n    m_def = Section(extends_base_section=True)\n    x_mycode_execution_mode = Quantity(\n        type=MEnum('hpc', 'parallel', 'single'), description='...')\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#schema-conventions","title":"Schema conventions","text":"<ul> <li>Use lower snake case for section properties; use upper camel case for section definitions.</li> <li>Use a <code>_ref</code> suffix for references.</li> <li>Use subsections rather than inheritance to add specific quantities to a general section. E.g. the section <code>workflow</code> contains a section <code>geometry_optimization</code> for all geometry optimization specific workflow quantities.</li> <li>Prefix parser-specific and user-defined definitions with <code>x_name_</code>, where <code>name</code> is the short handle of a code name or other special method prefix.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#use-python-schemas-to-work-with-data","title":"Use Python schemas to work with data","text":""},{"location":"howto/plugins/schema_packages.html#access-structured-data-via-api","title":"Access structured data via API","text":"<p>The API section demonstrates how to access an Archive, i.e. retrieve the processed data from a NOMAD entry. This API will give you JSON data likes this:</p> https://nomad-lab.eu/prod/v1/api/v1/entries/--dLZstNvL_x05wDg2djQmlU_oKn/archive<pre><code>{\n    \"run\": [\n        {\n            \"program\": {...},\n            \"method\": [...],\n            \"system\": [\n                {...},\n                {...},\n                {...},\n                {...},\n                {\n                    \"type\": \"bulk\",\n                    \"configuration_raw_gid\": \"-ZnDK8gT9P3_xtArfKlCrDOt9gba\",\n                    \"is_representative\": true,\n                    \"chemical_composition\": \"KKKGaGaGaGaGaGaGaGaGa\",\n                    \"chemical_composition_hill\": \"Ga9K3\",\n                    \"chemical_composition_reduced\": \"K3Ga9\",\n                    \"atoms\": {...},\n                    \"springer_material\": [...],\n                    \"symmetry\": [...]\n                }\n            ]\n            \"calculation\": [...],\n        }\n    ],\n    \"workflow\": [...],\n    \"metadata\": {...},\n    \"results\":{\n        \"material\": {...},\n        \"method\": {...},\n        \"properties\": {...},\n    }\n}\n</code></pre> <p>This will show you the Archive as a hierarchy of JSON objects (each object is a section), where each key is a property (e.g. a quantity or subsection). Of course you can use this data in this JSON form. You can expect that the same keys (each item has a formal definition) always provides the same type of data. However, not all keys are present in every archive, and not all lists might have the same number of objects. This depends on the data. For example, some runs contain many systems (e.g. geometry optimizations), others don't; typically bulk systems will have symmetry data, non bulk systems might not. To learn what each key means, you need to look up its definition in the Metainfo.</p> <p>You can browse the NOMAD metainfo schema or the archive of each entry (e.g. a VASP example) in the web-interface.</p>"},{"location":"howto/plugins/schema_packages.html#wrap-data-with-python-schema-classes","title":"Wrap data with Python schema classes","text":"<p>In Python, JSON data is typically represented as nested combinations of dictionaries and lists. Of course, you could work with this right away. To make it easier for Python programmers, the NOMAD Python package allows you to use this JSON data with a higher level interface, which provides the following advantages:</p> <ul> <li>code completion in dynamic coding environments like Jupyter notebooks</li> <li>a cleaner syntax that uses attributes instead of dictionary access</li> <li>all higher dimensional numerical data is represented as numpy arrays</li> <li>allows to navigate through references</li> <li>numerical data has a Pint unit attached to it</li> </ul> <p>For each section the Python package contains a Python class that corresponds to its definition in the metainfo. You can use these classes to access <code>json_data</code> downloaded via API: <pre><code>from nomad.datamodel import EntryArchive\n\narchive = EntryArchive.m_from_dict(json_data)\ncalc = archive.run[0].calculation[-1]\ntotal_energy_in_ev = calc.energy.total.value.to(units.eV).m\nformula = calc.system_ref.chemical_formula_reduced\n</code></pre></p> <p>Archive data can also be serialized into JSON again: <pre><code>import json\n\nprint(json.dumps(calc.m_to_dict(), indent=2))\n</code></pre></p>"},{"location":"howto/plugins/schema_packages.html#access-structured-data-via-the-nomad-python-package","title":"Access structured data via the NOMAD Python package","text":"<p>The NOMAD Python package provides utilities to query large amounts of archive data. This uses the built-in Python schema classes as an interface to the data.</p>"},{"location":"howto/plugins/schema_packages.html#schema-packages-developed-by-fairmat","title":"Schema packages developed by FAIRmat","text":"<p>The following is a list of plugins containing schema packages developed by FAIRmat:</p> Description Project url simulation run https://github.com/nomad-coe/nomad-schema-plugin-run.git simulation data https://github.com/nomad-coe/nomad-schema-plugin-simulation-data.git simulation workflow https://github.com/nomad-coe/nomad-schema-plugin-simulation-workflow.git NEXUS https://github.com/FAIRmat-NFDI/pynxtools.git synthesis https://github.com/FAIRmat-NFDI/AreaA-data_modeling_and_schemas.git material processing https://github.com/FAIRmat-NFDI/nomad-material-processing.git measurements https://github.com/FAIRmat-NFDI/nomad-measurements.git"},{"location":"howto/programmatic/api.html","title":"How to use the API","text":"<p>This guide is about using NOMAD's REST APIs directly, e.g. via Python's request.</p> <p>To access the processed data with our client library <code>nomad-lab</code> follow How to access processed data. You can also watch our video tutorial on the API.</p>"},{"location":"howto/programmatic/api.html#different-options-to-use-the-api","title":"Different options to use the API","text":"<p>NOMAD offers all its functionality through application programming interfaces (APIs). More specifically RESTful HTTP APIs that allows you to use NOMAD as a set of resources (think data) that can be uploaded, accessed, downloaded, searched for, etc. via HTTP requests.</p> <p>You can get an overview on all NOMAD APIs on the API page. We will focus here on NOMAD's main API (v1). In fact, this API is also used by the web interface and should provide everything you need.</p> <p>There are different tools and libraries to use the NOMAD API that come with different trade-offs between expressiveness, learning curve, and convenience.</p> You can use your browser <p>For example to see the metadata for all entries with elements Ti and O go here: https://nomad-lab.eu/prod/v1/api/v1/entries?elements=Ti&amp;elements=O</p> Use *curl* or *wget* <p>REST API's use resources located via URLs. You access URLs with curl or wget. Same Ti, O example as before: <pre><code>curl \"https://nomad-lab.eu/prod/v1/api/v1/entries?results.material.elements=Ti&amp;results.material.elements=O\" | python -m json.tool\n</code></pre></p>  Use Python and requests <p>Requests is a popular Python library to use the internets HTTP protocol that is used to communicate with REST APIs. Install with <code>pip install requests</code>. See the initial example.</p> Use our dashboard <p>The NOMAD API has an OpenAPI dashboard. This is an interactive documentation of all API functions that allows you to try these functions in the browser.</p> Use NOMAD's Python package <p>Install the NOMAD Python client library and use it's <code>ArchiveQuery</code> functionality for a more convenient query based access of archive data following the How-to access the processed data guide.</p>"},{"location":"howto/programmatic/api.html#using-request","title":"Using request","text":"<p>If you are comfortable with REST APIs and using Pythons <code>requests</code> library, this example demonstrates the basic concepts of NOMAD's main API. You can get more documentation and details on all functions from the API dashboard.</p> <p>The following issues a search query for all entries that have both Ti and O among the elements of their respective materials. It restricts the results to one entry and only returns the <code>entry_id</code>.</p> <pre><code>import requests\nimport json\n\nbase_url = 'http://nomad-lab.eu/prod/v1/api/v1'\n\nresponse = requests.post(\n    f'{base_url}/entries/query',\n    json={\n        'query': {\n            'results.material.elements': {\n                'all': ['Ti', 'O']\n            }\n        },\n        'pagination': {\n            'page_size': 1\n        },\n        'required': {\n            'include': ['entry_id']\n        }\n    })\nresponse_json = response.json()\nprint(json.dumps(response.json(), indent=2))\n</code></pre> <p>This will give you something like this: <pre><code>{\n  \"owner\": \"public\",\n  \"query\": {\n    \"name\": \"results.material.elements\",\n    \"value\": {\n      \"all\": [\n        \"Ti\",\n        \"O\"\n      ]\n    }\n  },\n  \"pagination\": {\n    \"page_size\": 1,\n    \"order_by\": \"entry_id\",\n    \"order\": \"asc\",\n    \"total\": 17957,\n    \"next_page_after_value\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\"\n  },\n  \"required\": {\n    \"include\": [\n      \"entry_id\"\n    ]\n  },\n  \"data\": [\n    {\n      \"entry_id\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\"\n    }\n  ]\n}\n</code></pre></p> <p>The <code>entry_id</code> is a unique identifier for, well, entries. You can use it to access other entry data. For example, you want to access the entry's archive. More precisely, you want to gather the formula and energies from the main workflow result. The following requests the archive based on the <code>entry_id</code> and only requires some archive sections.</p> <pre><code>first_entry_id = response_json['data'][0]['entry_id']\nresponse = requests.post(\n    f'{base_url}/entries/{first_entry_id}/archive/query',\n    json={\n        'required': {\n            'workflow': {\n                'calculation_result_ref': {\n                    'energy': '*',\n                    'system_ref': {\n                        'chemical_composition': '*'\n                    }\n                }\n            }\n        }\n    })\nresponse_json = response.json()\nprint(json.dumps(response_json, indent=2))\n</code></pre> <p>The result will look like this: <pre><code>{\n  \"required\": {\n    \"workflow\": {\n      \"calculation_result_ref\": {\n        \"energy\": \"*\",\n        \"system_ref\": {\n          \"chemical_composition\": \"*\"\n        }\n      }\n    }\n  },\n  \"entry_id\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\",\n  \"data\": {\n    \"entry_id\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\",\n    \"upload_id\": \"YXUIZpw5RJyV3LAsFI2MmQ\",\n    \"parser_name\": \"parsers/fhi-aims\",\n    \"archive\": {\n      \"run\": [\n        {\n          \"system\": [\n            {\n              \"chemical_composition\": \"OOSrTiOOOSrTiOOOSrTiOFF\"\n            }\n          ],\n          \"calculation\": [\n            {\n              \"energy\": {\n                \"fermi\": -1.1363378335891879e-18,\n                \"total\": {\n                  \"value\": -5.697771591896252e-14\n                },\n                \"correlation\": {\n                  \"value\": -5.070133798617076e-17\n                },\n                \"exchange\": {\n                  \"value\": -2.3099755059272454e-15\n                },\n                \"xc\": {\n                  \"value\": -2.360676843913416e-15\n                },\n                \"xc_potential\": {\n                  \"value\": 3.063766944960246e-15\n                },\n                \"free\": {\n                  \"value\": -5.697771595558439e-14\n                },\n                \"sum_eigenvalues\": {\n                  \"value\": -3.3841806795825544e-14\n                },\n                \"total_t0\": {\n                  \"value\": -5.697771593727346e-14\n                },\n                \"correction_entropy\": {\n                  \"value\": -1.8310927833270112e-23\n                },\n                \"correction_hartree\": {\n                  \"value\": -4.363790430157292e-17\n                },\n                \"correction_xc\": {\n                  \"value\": -2.3606768439090564e-15\n                }\n              },\n              \"system_ref\": \"/run/0/system/0\"\n            }\n          ]\n        }\n      ],\n      \"workflow\": [\n        {\n          \"calculation_result_ref\": \"/run/0/calculation/0\"\n        }\n      ]\n    }\n  }\n}\n</code></pre></p> <p>You can work with the results in the given JSON (or respective Python dict/list) data already. If you have NOMAD's Python library installed , you can take the archive data and use the Python interface. The Python interface will help with code-completion (e.g. in notebook environments), resolve archive references (e.g. from workflow to calculation to system), and allow unit conversion: <pre><code>from nomad.datamodel import EntryArchive\nfrom nomad.metainfo import units\n\narchive = EntryArchive.m_from_dict(response_json['data']['archive'])\nresult = archive.workflow[0].calculation_result_ref\nprint(result.system_ref.chemical_composition)\nprint(result.energy.total.value.to(units('eV')))\n</code></pre></p> <p>This will give you an output like this: <pre><code>OOSrTiOOOSrTiOOOSrTiOFF\n-355626.93095025205 electron_volt\n</code></pre></p>"},{"location":"howto/programmatic/api.html#different-kinds-of-data","title":"Different kinds of data","text":"<p>We distinguish between different kinds of NOMAD data and there are different functions in the API:</p> <ul> <li>Entry metadata, a summary of extracted data for an entry.</li> <li>Raw files, the files as they were uploaded to NOMAD.</li> <li>Archive data, all of the extracted data for an entry.</li> </ul> <p>There are also different entities (see also Datamodel) with different functions in the API:</p> <ul> <li>Entries</li> <li>Uploads</li> <li>Datasets</li> <li>Users</li> </ul> <p>The API URLs typically start with the entity, followed by the kind of data. Examples are:</p> <ul> <li><code>entries/query</code> - Query entries for metadata</li> <li><code>entries/archive/query</code> - Query entries for archive data</li> <li><code>entries/{entry-id}/raw</code> - Download raw data for a specific entry</li> <li><code>uploads/{upload-id}/raw/path/to/file</code> - Download a specific file of an upload</li> </ul>"},{"location":"howto/programmatic/api.html#common-concepts","title":"Common concepts","text":"<p>The initial example above, showed how to execute a basic search. This includes some fundamental concepts that can be applied to many parts of the API. Let's discuss some of the common concepts.</p>"},{"location":"howto/programmatic/api.html#response-layout","title":"Response layout","text":"<p>Functions that have a JSON response, will have a common layout. First, the response will contain all keys and values of the request. The request is not repeated verbatim, but in a normalized form. Abbreviations in search queries might be expanded, default values for optional parameters are added, or additional response specific information is included. Second, the response will contain the results under the key <code>data</code>.</p>"},{"location":"howto/programmatic/api.html#owner","title":"Owner","text":"<p>All functions that allow a query will also allow to specify the <code>owner</code>. Depending on the API function, its default value will be mostly <code>visible</code>. Some values are only available if you are logged in.</p> <p>The <code>owner</code> allows to limit the scope of the search based on entry ownership. This is useful if you only want to search among all publicly downloadable entries or only among your own entries, etc.</p> <p>These are the possible owner values and their meaning:</p> <ul> <li><code>admin</code>: No restriction. Only usable by an admin user.</li> <li><code>all</code>: Published entries (with or without embargo), or entries that belong to you     or are shared with you.</li> <li><code>public</code>: Published entries without embargo.</li> <li><code>shared</code>: Entries that belong to you or are shared with you.</li> <li><code>staging</code>: Unpublished entries that belong to you or are shared with you.</li> <li><code>user</code>: Entries that belong to you.</li> <li><code>visible</code>: Published entries without embargo, or unpublished entries that belong to     you or are shared with you.</li> </ul>"},{"location":"howto/programmatic/api.html#queries","title":"Queries","text":"<p>A query can be a very simple list of parameters. Different parameters or values of the same parameter are combined with a logical and. The following query would search for all entries that are VASP calculations, contain Na and Cl, and are authored by Stefano Curtarolo and Chris Wolverton. <pre><code>{\n    \"results.material.elements\": [\"Na\", \"Cl\"],\n    \"results.method.simulation.program_name\": \"VASP\",\n    \"authors\": [\"Stefano Curtarolo\", \"Chris Wolverton\"]\n}\n</code></pre></p> <p>A short cut to change the logical combination of values in a list, is to add a suffix to the quantity <code>:any</code>: <pre><code>{\n    \"results.material.elements\": [\"Na\", \"Cl\"],\n    \"results.method.simulation.program_name\": \"VASP\",\n    \"authors:any\": [\"Stefano Curtarolo\", \"Chris Wolverton\"]\n}\n</code></pre></p> <p>Otherwise, you can also write complex logical combinations of parameters like this: <pre><code>{\n    \"and\": [\n        {\n            \"or\": [\n                {\n                    \"results.material.elements\": [\"Cl\", \"Na\"]\n                },\n                {\n                    \"results.material.elements\": [\"H\", \"O\"]\n                }\n            ]\n        },\n        {\n            \"not\": {\n                \"results.material.symmetry.crystal_system\": \"cubic\"\n            }\n        }\n    ]\n}\n</code></pre> Other short-cut prefixes are <code>none:</code> and <code>any:</code> (the default).</p> <p>By default all quantity values have to equal the given values to match. For some values you can also use comparison operators like this: <pre><code>{\n    \"upload_create_time\": {\n        \"gt\": \"2020-01-01\",\n        \"lt\": \"2020-08-01\"\n    },\n    \"results.properties.geometry_optimization.final_energy_difference\": {\n        \"lte\": 1.23e-18\n    }\n}\n</code></pre></p> <p>or shorter with suffixes: <pre><code>{\n    \"upload_create_time:gt\": \"2020-01-01\",\n    \"upload_create_time:lt\": \"2020-08-01\",\n    \"results.properties.geometry_optimization.final_energy_difference:lte\": 1.23e-18\n}\n</code></pre></p> <p>The searchable quantities are a subset of the NOMAD Archive quantities defined in the NOMAD Metainfo. The searchable quantities also depend on the API endpoint.</p> <p>There is also an additional query parameter that you can use to formulate queries based on the optimade filter language: <pre><code>{\n    \"optimade_filter\": \"nelements &gt;= 2 AND elements HAS ALL 'Ti', 'O'\"\n}\n</code></pre></p>"},{"location":"howto/programmatic/api.html#pagination","title":"Pagination","text":"<p>When you issue a query, usually not all results can be returned. Instead, an API returns only one page. This behavior is controlled through pagination parameters, like <code>page_site</code>, <code>page</code>, <code>page_offset</code>, or <code>page_after_value</code>.</p> <p>Let's consider a search for entries as an example. <pre><code>response = requests.post(\n    f'{base_url}/entries/query',\n    json={\n        'query': {\n            'results.material.elements': {\n                'all': ['Ti', 'O']\n            }\n        },\n        'pagination': {\n            'page_size': 10\n        }\n    }\n)\n</code></pre></p> <p>This will only result in a response with a maximum of 10 entries. The response will contain a <code>pagination</code> object like this: <pre><code>{\n    \"page_size\": 10,\n    \"order_by\": \"entry_id\",\n    \"order\": \"asc\",\n    \"total\": 17957,\n    \"next_page_after_value\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\"\n}\n</code></pre></p> <p>In this case, the pagination is based on after values. This means that the search can be continued with a follow up request at a certain point characterized by the <code>next_page_after_value</code>. If you follow up with:</p> <p><pre><code>response = requests.post(\n    f'{base_url}/entries/query',\n    json={\n        'query': {\n            'results.material.elements': {\n                'all': ['Ti', 'O']\n            }\n        },\n        'pagination': {\n            'page_size': 10,\n            'page_after_value': '--SZVYOxA2jTu_L-mSxefSQFmeyF'\n        }\n    }\n)\n</code></pre> You will get the next 10 results.</p> <p>Here is a full example that collects the first 100 formulas from entries that match a certain query by paginating.</p> <pre><code>import requests\n\nbase_url = 'http://nomad-lab.eu/prod/v1/api/v1'\njson_body = {\n    'query': {\n        'results.material.elements': {\n            'all': ['Ti', 'O']\n        }\n    },\n    'pagination': {\n        'page_size': 10\n    },\n    'required': {\n        'include': ['results.material.chemical_formula_hill']\n    }\n}\n\nformulas = set()\n\nwhile len(formulas) &lt; 100:\n    response = requests.post(f'{base_url}/entries/query', json=json_body)\n    response_json = response.json()\n\n    for data in response_json['data']:\n        formulas.add(data['results']['material']['chemical_formula_hill'])\n\n    next_value = response_json['pagination'].get('next_page_after_value')\n    if not next_value:\n        break\n    json_body['pagination']['page_after_value'] = next_value\n\nprint(formulas)\n</code></pre>"},{"location":"howto/programmatic/api.html#authentication","title":"Authentication","text":"<p>Most of the API operations do not require any authorization and can be freely used without a user or credentials. However, to upload, edit, or view your own and potentially unpublished data, the API needs to authenticate you.</p> <p>The NOMAD API uses OAuth and tokens to authenticate users. We provide simple operations that allow you to acquire an access token via username and password:</p> <pre><code>import requests\n\nresponse = requests.get(\n    'https://nomad-lab.eu/prod/v1/api/v1/auth/token', params=dict(username='myname', password='mypassword'))\ntoken = response.json()['access_token']\n\nresponse = requests.get(\n    'https://nomad-lab.eu/prod/v1/api/v1/uploads',\n    headers={'Authorization': f'Bearer {token}'})\nuploads = response.json()['data']\n</code></pre> <p>If you have the NOMAD Python package installed. You can use its <code>Auth</code> implementation:</p> <pre><code>import requests\nfrom nomad.client import Auth\n\nresponse = requests.get(\n    'https://nomad-lab.eu/prod/v1/api/v1/uploads',\n    auth=Auth(user='myname or email', password='mypassword'))\nuploads = response.json()['data']\n</code></pre> <p>To use authentication in the dashboard, simply use the Authorize button. The dashboard GUI will manage the access token and use it while you try out the various operations.</p>"},{"location":"howto/programmatic/api.html#app-token","title":"App token","text":"<p>If the short-term expiration of the default access token does not suit your needs, you can request an app token with a user-defined expiration. For example, you can send the GET request <code>/auth/app_token?expires_in=86400</code> together with some way of authentication, e.g. header <code>Authorization: Bearer &lt;access token&gt;</code>. The API will return an app token, which is valid for 24 hours in subsequent request headers with the format <code>Authorization: Bearer &lt;app token&gt;</code>. The request will be declined if the expiration is larger than the maximum expiration defined by the API config.</p> <p>Warning</p> <p>Despite the name, the app token is used to impersonate the user who requested it. It does not discern between different uses and will only become invalid once it expires (or when the API's secret is changed).</p>"},{"location":"howto/programmatic/api.html#search-for-entries","title":"Search for entries","text":"<p>See using requests for a typical search example. Combine the different concepts above to create the queries that you need.</p> <p>Searching for entries is typically just an initial step. Once you know what entries exist you'll probably want to do one of the following things.</p>"},{"location":"howto/programmatic/api.html#download-raw-files","title":"Download raw files","text":"<p>You can use queries to download raw files, but typically you don't want to download file-by-file or entry-by-entry. Therefore, we allow to download a large set of files in one big zip-file. Here, you might want to use a program like curl to download directly from the shell:</p> <pre><code>curl \"https://nomad-lab.eu/prod/v1/api/v1/entries/raw?results.material.elements=Ti&amp;results.material.elements=O\" -o download.zip\n</code></pre>"},{"location":"howto/programmatic/api.html#access-processed-data-archives","title":"Access processed data (archives)","text":"<p>Above under using requests, you've already learned how to access archive data. A special feature of the archive API functions is that you can define what is <code>required</code> from the archives.</p> <pre><code>response = requests.post(\n    f'{base_url}/entries/archive/query',\n    json={\n        'query': ...,\n        'pagination': ...,\n        'required': {\n            'workflow': {\n                'calculation_result_ref': {\n                    'energy': '*',\n                    'system_ref': {\n                        'chemical_composition': '*'\n                    }\n                }\n            }\n        }\n    })\n</code></pre> <p>The <code>required</code> part allows you to specify what parts of the requested archives should be returned. The NOMAD Archive is a hierarchical data format and you can require certain branches (i.e. sections) in the hierarchy. By specifying certain sections with specific contents or all contents (via the directive <code>\"*\"</code>), you can determine what sections and what quantities should be returned. The default is the whole archive, i.e., <code>\"*\"</code>.</p> <p>For example to specify that you are only interested in the <code>metadata</code> use:</p> <pre><code>{\n    \"metadata\": \"*\"\n}\n</code></pre> <p>Or to only get the <code>energy_total</code> from each individual entry, use: <pre><code>{\n    \"run\": {\n        \"configuration\": {\n            \"energy\": \"*\"\n        }\n    }\n}\n</code></pre></p> <p>You can also request certain parts of a list, e.g. the last calculation: <pre><code>{\n    \"run\": {\n        \"calculation[-1]\": \"*\"\n    }\n}\n</code></pre></p> <p>These required specifications are also very useful to get workflow results. This works because we can use references (e.g. workflow to final result calculation) and the API will resolve these references and return the respective data. For example just the total energy value and reduced formula from the resulting calculation: <pre><code>{\n    \"workflow\": {\n        \"calculation_result_ref\": {\n            \"energy\": \"*\",\n            \"system_ref\": {\n                \"value\": {\n                    \"chemical_composition\": \"*\"\n                }\n            }\n        }\n    }\n}\n</code></pre></p> <p>You can also resolve all references in a branch with the <code>include-resolved</code> directive. This will resolve all references in the branch, and also all references in referenced sections: <pre><code>{\n    \"workflow\":\n        \"calculation_result_ref\": \"include-resolved\"\n    }\n}\n</code></pre></p> <p>By default, the targets of \"resolved\" references are added to the archive at their original hierarchy positions. This means, all references are still references, but they are resolvable within the returned data, since they targets are now part of the data. Another option is to add <code>\"resolve-inplace\": true</code> to the root of required. Here, the reference targets will replace the references: <pre><code>{\n    \"resolve-inplace\": true,\n    \"workflow\":\n        \"calculation_result_ref\": \"include-resolved\"\n    }\n}\n</code></pre></p> <p>You can browse the NOMAD metainfo schema or the archive of each entry (e.g. a VASP example) in the web-interface.</p>"},{"location":"howto/programmatic/api.html#limits","title":"Limits","text":"<p>The API allows you to ask many requests in parallel and to put a lot of load on NOMAD servers. Since this can accidentally or deliberately reduce the service quality for other, we have to enforce a few limits.</p> <ul> <li>rate limit: you can only run a certain amount of requests at the same time</li> <li>rate limit: you can only run a certain amount of requests per second</li> <li>api limit: many API endpoints will enforce a maximum page size</li> </ul> <p>If you get responses with an HTTP code 503 Service Unavailable, you are hitting a rate limit and you cannot use the service until you fall back into our limits. Consider, to ask fewer requests in a larger time frame.</p> <p>Rate limits are enforced based on your IP address. Please note that when you or your colleagues are sharing a single external IPs from within a local network, e.g. via NAT, you are also sharing the rate limits. Depending on the NOMAD installation, these limits can be as low as 30 requests per second or 10 concurrent requests.</p> <p>Consider to use endpoints that allow you to retrieve full pages of resources, instead of endpoints that force you to access resources one at a time. See also the sections on types of data and pagination.</p> <p>However, pagination also has its limits and you might ask for pages that are too large. If you get responses in the 400 range, e.g. 422 Unprocessable Content or 400 Bad request, you might hit an api limit. Those responses are typically accompanied by an error message in the response body that will inform you about the limit, e.g. the maximum allowed page size.</p>"},{"location":"howto/programmatic/archive_query.html","title":"How to access processed data","text":"<p>The <code>ArchiveQuery</code> allows you to search for entries and access their parsed and processed archive data at the same time. Furthermore, all data is accessible through a convenient Python interface based on the schema rather than plain JSON. See also this guide on using NOMAD's Python schemas to work with processed data.</p> <p>As a requirement, you have to install the <code>nomad-lab</code> Python package. Follow the How to install nomad-lab guide.</p>"},{"location":"howto/programmatic/archive_query.html#getting-started","title":"Getting started","text":"<p>To define a query, one can, for example, write</p> <pre><code>from nomad.client.archive import ArchiveQuery\n\nquery = ArchiveQuery(query={}, required='*', page_size=10, results_max=10000)\n</code></pre> <p>Although the above query object has an empty query.</p> <p>The query object is constructed only. To access the desired data, users need to perform two operations manually. Two interfaces that can be used in different environments are provided.</p>"},{"location":"howto/programmatic/archive_query.html#synchronous-interface","title":"Synchronous Interface","text":""},{"location":"howto/programmatic/archive_query.html#fetch","title":"Fetch","text":"<p>The fetch process is carried out synchronously. Users can call the following to fetch up to <code>results_max</code> entries.</p> <pre><code># number_of_entries = query.fetch(1000) # fetch 1000 entries\nnumber_of_entries = query.fetch()  # fetch at most results_max entries\n</code></pre> <p>An indicative number <code>n</code> can be provided <code>fetch(n)</code> to fetch <code>n</code> entries at once. The fetch process may submit multiple requests to the server, each request asks for <code>page_size</code> entries. The number of qualified entries will be returned. Meanwhile, the qualified entry list would be populated with their IDs. To check all qualified upload IDs, one can call <code>entry_list()</code> method to return the full list.</p> <pre><code>print(query.entry_list())\n</code></pre> <p>If applicable, it is possible to fetch a large number of entries first and then perform a second fetch by using some entry ID in the first fetch result as the <code>after</code> argument so that some middle segment can be downloaded.</p>"},{"location":"howto/programmatic/archive_query.html#download","title":"Download","text":"<p>After fetching the qualified entries, the desired data can be downloaded asynchronously. One can call</p> <pre><code># results = query.download(1000) # download 1000 entries\nresults = query.download()  # download all fetched entries if fetched otherwise fetch and download up to `results_max` entries\n</code></pre> <p>to download up to <code>results_max</code> entries. The downloaded results are returned as a list. Alternatively, it is possible to just download a portion of previously fetched entries at a single time. For example,</p> <pre><code># previously fetched for example 1000 entries\n# but only download the first 100 (approx.) entries\nresults = query.download(100)\n</code></pre> <p>The same <code>download(n)</code> method can be called repeatedly. If there are no sufficient entries, new entries will be automatically fetched. If there are no more entries, the returned result list is empty. For example,</p> <pre><code>total_results = []\nwhile True:\n    result = query.download(100)\n    if len(result) == 0:\n        break\n    total_results.extend(result)\n</code></pre> <p>There is no retry mechanism in the download process. If any entries fail to be downloaded due to server error, it is kept in the list otherwise removed.</p>"},{"location":"howto/programmatic/archive_query.html#pandas-dataframe","title":"Pandas Dataframe","text":"<p>You can also convert the downloaded results to pandas dataframe directly by calling <code>entries_to_dataframe</code> method on the <code>query</code> object. In order to filter the final dataframe to contain only specific keys/column_names, you can use the option <code>keys_to_filter</code> with a list of relevant keys. For example:</p> <pre><code>results = query.entries_to_dataframe(keys_to_filter=[])\n</code></pre> <p>The option <code>from_query</code> can be used to control the formatting of the dataframe(s). By setting this option to <code>False</code>, all entries with their entire contents are flattened and returned in one single (and potentially huge) dataframe, and by setting it to <code>True</code>, it returns a python dictionary with each key denoting a separate distinct nested path in the <code>required</code> and each value specifying the corresponding dataframe.</p>"},{"location":"howto/programmatic/archive_query.html#asynchronous-interface","title":"Asynchronous Interface","text":"<p>Some applications, such as Jupyter Notebook, may run a global/top level event loop. To query data in those environments, one can use the asynchronous interface.</p> <pre><code>number_of_entries = await query.async_fetch()  # indicative number n applies: async_fetch(n)\nresults = await query.async_download()  # indicative number n applies: async_download(n)\n</code></pre> <p>Alternatively, if one wants to use the asynchronous interface, it is necessary to patch the global event loop to allow nested loops.</p> <p>To do so, one can add the following at the beginning of the notebook.</p> <pre><code>import nest_asyncio\n\nnest_asyncio.apply()\n</code></pre>"},{"location":"howto/programmatic/archive_query.html#a-complete-rundown","title":"A Complete Rundown","text":"<p>Here we show a valid query and acquire data from server.</p> <p>We first define the desired query and construct the object. We limit the maximum number of entries to be 10000 and 10 entries per page.</p> <pre><code>from nomad.client.archive import ArchiveQuery\n\nrequired = {\n    'workflow': {\n        'calculation_result_ref': {\n            'energy': '*',\n            'system_ref': {\n                'chemical_composition_reduced': '*'\n            }\n        }\n    }\n}\n\nquery = {\n    'results.method.simulation.program_name': 'VASP',\n    'results.material.elements': ['Ti']\n}\n\nquery = ArchiveQuery(query=query, required=required, page_size=10, results_max=10000)\n</code></pre> <p>Let's fetch some entries.</p> <pre><code>query.fetch(10)\nprint(query.entry_list())\n</code></pre> <p>If we print the entry list, it would be</p> <pre><code>[('---CU_ejqV7yjEFteUAH0rG0SKIS', 'aimE2ajMQnOKruRbQjzpCA'), ('---Dz9vL-eyErWEk7-1tX4zLVmwo', 'Vyb6k1OTRSuyXfvsRk4CPQ'), ('---pRcX7NG_XDx_4ufUaeEnZnmrO', 'IL_YBCD8TSyLlsqzIcBgYw'), ('--0RSTtl4mvX3Nd0JjhL_V1YV1ip', 'tF8R9nmZTyyfnv2zWADI0A'), ('--0SDuSOM_gpweM3PDb0WOFgYDyv', 'mLO6o1GBShWrtXfoSJHgfw'), ('--0jLz1eNRwtR_oRxPpgVC9U437y', 'HVheHWfxTpe28HhbHpcO1A'), ('--1cY4hzXaXdThxsw7saN3nd3xyt', 'h79v1yw_Qf-kOVTa0WYfdg'), ('--2nakQLLxyI_vsEOIbwzHMgWWPQ', 'iIGUoyaiT5i4b4UynPlnSQ'), ('--3-SQGOswGzwaEo5QiQNCcZQhi8', '1lQ90kNaSWyJXBQ_kK91Tg'), ('--3Km0GSVTHRkNCJHHjQdHqxwVfR', 'dHAJP-NvQw22FoBeDXiAIg')]\n</code></pre> <p>Each is a tuple of two strings. The first is the entry ID while the second is the upload ID.</p> <p>Now data can be downloaded.</p> <pre><code>result = query.download(8)\nprint(f'Downloaded {len(result)} entries.')  # Downloaded 100 entries.\n</code></pre> <p>The first eight entries will be downloaded. If one prints the list again, only the last two are present.</p> <pre><code>print(query.entry_list())\n</code></pre> <pre><code>[('--3-SQGOswGzwaEo5QiQNCcZQhi8', '1lQ90kNaSWyJXBQ_kK91Tg'), ('--3Km0GSVTHRkNCJHHjQdHqxwVfR', 'dHAJP-NvQw22FoBeDXiAIg')]\n</code></pre> <p>It is possible to download more data. We perform one more download call to illustrate that the fetch process will be automatically triggered.</p> <pre><code>result = query.download(5)\nprint(f'Downloaded {len(result)} entries.')  # Downloaded 200 entries.\n</code></pre> <p>In the above, we request five entries. However, the list contains only two entries, the fetch process will be called to fetch extra three entries from server. But since the page size is 10, the server will return 10 entries. You will see the following message in the terminal.</p> <pre><code>Fetching remote uploads...\n10 entries are qualified and added to the download list.\nDownloading 5 entries...  [####################################]  100%\n</code></pre> <p>Now that we have downloaded a few entries, we can convert them into pandas dataframe.</p> <pre><code>dataframes = query.entries_to_dataframe(keys_to_filter=['workflow2.results.calculation_result_ref'])\nprint(dataframes)\n</code></pre> <p>By setting <code>keys_to_filter</code> to <code>['workflow2.results.calculation_result_ref']</code>, we create a dataframe representing the content that exists in the response tree under the section <code>calculation_result_ref</code>. Below, you can see the final dataframe printed in the Python console. You can also try setting this option to an empty list (or simply removing the option) to see the results containing the entire response tree in dataframe format. Furthermore, since we have converted our data into a pandas dataframe, we can proceed to export CSV files, obtain statistics, create plots, and more.</p> <pre><code>    energy.fermi  ...  system_ref.chemical_composition_reduced\n0            NaN  ...                                     O3Ti\n1            NaN  ...                                   LiO3Ti\n2            NaN  ...                                     O3Ti\n3            NaN  ...                                     O3Ti\n4            NaN  ...                                   LiO3Ti\n5            NaN  ...                                     O3Ti\n6            NaN  ...                                  Cu44OTi\n7  -1.602177e-18  ...                                  O51Ti26\n8  -1.602177e-18  ...                                  O51Ti26\n9            NaN  ...                                     O3Ti\n10 -1.602177e-18  ...                                  O51Ti26\n11 -1.602177e-18  ...                                  O51Ti26\n12 -1.602177e-18  ...                                  O51Ti26\n13           NaN  ...                                     O3Ti\n14           NaN  ...                                    KO3Ti\n15           NaN  ...                                    KO3Ti\n16           NaN  ...                                     O3Ti\n17           NaN  ...                                   AlO3Ti\n18           NaN  ...                                   O3TiZn\n\n[19 rows x 7 columns]\n</code></pre>"},{"location":"howto/programmatic/archive_query.html#argument-list","title":"Argument List","text":"<p>The following arguments are acceptable for <code>ArchiveQuery</code>.</p> <ul> <li><code>owner</code> : <code>str</code> The scope of data to access. Default: <code>'visible'</code></li> <li><code>query</code> : <code>dict</code> The API query. There are no validations of any means carried out by the class, users shall make sure   the provided query is valid. Otherwise, server would return error messages.</li> <li><code>required</code> : <code>dict</code> The required quantities.</li> <li><code>url</code> : <code>str</code> The database url. It can be the one of your local database. The official NOMAD database is used be   default if no valid one defined. Default: <code>http://nomad-lab.eu/prod/v1/api</code></li> <li><code>after</code> : <code>str</code> It can be understood that the data is stored in a sequential list. Each upload has a unique ID,   if <code>after</code> is not provided, the query always starts from the first upload. One can choose to query the uploads in the   middle of storage by assigning a proper value of <code>after</code>.</li> <li><code>results_max</code> : <code>int</code> Determine how many entries to download. Note each upload may have multiple entries.</li> <li><code>page_size</code> : <code>int</code> Page size.</li> <li><code>username</code> : <code>str</code> Username for authentication.</li> <li><code>password</code> : <code>str</code> Password for authentication.</li> <li><code>retry</code> : <code>int</code> In the case of server errors, the fetch process is automatically retried every <code>sleep_time</code> seconds.   This argument limits the maximum times of retry.</li> <li><code>sleep_time</code> : <code>float</code> The interval of fetch retry.</li> </ul>"},{"location":"howto/programmatic/archive_query.html#the-complete-example","title":"The complete example","text":"<p>Attention</p> <p>This examples uses the new <code>workflow2</code> workflow system. This is still under development and this example might not yet produce results on the public nomad data.</p> <pre><code>--8 &lt; -- \"examples/archive/archive_query.py\"\n</code></pre>"},{"location":"howto/programmatic/download.html","title":"Download data","text":"<p>A common use-case for the NOMAD API is to download large amounts of NOMAD data. In this how-to guide, we use curl and API endpoints that stream .zip files to download many resources with a single request directly from the command line.</p>"},{"location":"howto/programmatic/download.html#prerequisites","title":"Prerequisites","text":"<p>Here is some background information to understand the examples better.</p>"},{"location":"howto/programmatic/download.html#curl","title":"curl","text":"<p>To download resources from a REST API using curl, you can utilize the powerful command-line tool to send HTTP requests and retrieve the desired data. Curl provides a simple and efficient way to interact with RESTful APIs, allowing you to specify the necessary headers, parameters, and authentication details. Whether you need to download files, retrieve JSON data, or access other resources, curl offers a flexible and widely supported solution for programmatically fetching data from REST APIs.</p>"},{"location":"howto/programmatic/download.html#raw-files-vs-processed-data","title":"Raw files vs processed data","text":"<p>We are covering two types of resources: raw files and processed data. The former is organized into uploads and sub directory. The organization depends on how the author was providing the files. The later is organized by entries. Each NOMAD entry has corresponding structured data.</p> <p>Endpoints that target raw files typically contain <code>raw</code>, e.g. <code>uploads/&lt;id&gt;/raw</code> or <code>entries/raw/query</code>. Endpoints that target processed data contain <code>archive</code> (because we call the entirety of all processed data the NOMAD Archive), e.g. <code>entries/&lt;id&gt;/archive</code> or <code>entries/archive/query</code>.</p>"},{"location":"howto/programmatic/download.html#entry-vs-upload","title":"Entry vs upload","text":"<p>API endpoints for data download either target entries or uploads. For both types of entities, endpoints for raw files and processed data (as well as searchable metadata) exist. API endpoint paths start with the entity, e.g. <code>uploads/&lt;id&gt;/raw</code> or <code>entries/&lt;id&gt;/raw</code>.</p>"},{"location":"howto/programmatic/download.html#download-a-whole-upload","title":"Download a whole upload","text":"<p>Let's assume you want to download an entire upload. In this example the upload id is <code>wW45wJKiREOYTY0ARuknkA</code>.</p> <pre><code>curl -X GET \"https://nomad-lab.eu/prod/v1/api/v1/uploads/wW45wJKiREOYTY0ARuknkA/raw\" -o download.zip\n</code></pre> <p>This will create a <code>download.zip</code> file in the current folder. The zip file will contain the raw file directory of the upload.</p> <p>The used <code>uploads/&lt;id&gt;/raw</code> endpoint is only available for published uploads. For those, all raw files have already been packed into a zip file and this endpoint simply lets you download it. This is the simplest and most reliable download implementation.</p> <p>Alternatively, you can download specific files or sub-directories. This method is available for all uploads. Including un-published uploads.</p> <pre><code>curl -X GET \"https://nomad-lab.eu/prod/v1/api/v1/uploads/wW45wJKiREOYTY0ARuknkA/raw/?compress=true\" -o download.zip\n</code></pre> <p>This endpoint looks very similar, but is implemented very differently. Note that we put an empty path <code>/</code> to the end of the URL, plus a query parameter <code>compress=true</code>. The path can be replaced with any directory or file path in the upload; <code>/</code> would denote the whole upload. The query parameter says that we want to download the whole directory as a zip file, instead of an individual file. This traverses through all files and creates a zip file on the fly.</p>"},{"location":"howto/programmatic/download.html#download-a-whole-dataset","title":"Download a whole dataset","text":"<p>Now let's assume that you want to download all raw files that are associated with all the entries of an entire dataset. In this example the dataset DOI is <code>10.17172/NOMAD/2023.11.17-2</code>.</p> <pre><code>curl -X POST \"https://nomad-lab.eu/prod/v1/api/v1/entries/raw/query\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"query\": {\n        \"datasets.doi\": \"10.17172/NOMAD/2023.11.17-2\"\n    }\n}' \\\n-o download.zip\n</code></pre> <p>This time, we use the <code>entries/raw/query</code> endpoint that is based on entries and not on uploads. Here, we select entries with a query. In the example, we query for the dataset DOI, but you can replace this with any NOMAD search query (look out for the <code>&lt;&gt;</code> symbol on the search interface). The zip file will contain all raw files from all the directories that have the mainfile of one of the entries that match the queries.</p> <p>This might not necessarily download all uploaded files. Alternatively, you can use a query to get all upload ids and then use the method from the previous section:</p> <pre><code>curl -X POST \"https://nomad-lab.eu/prod/v1/api/v1/entries/query\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"query\": {\n        \"datasets.doi\": \"10.17172/NOMAD/2023.11.17-2\"\n    },\n    \"pagination\": {\n        \"page_size\": 0\n    },\n    \"aggregations\": {\n        \"upload_ids\": {\n            \"terms\": {\n                \"quantity\": \"upload_id\"\n            }\n        }\n    }\n}'\n</code></pre> <p>The last command will print JSON data that contains all the upload ids. It uses the <code>entries/query</code> endpoint that allows you to query NOMAD's search. It does not return any results (<code>page_size: 0</code>), but performs an aggregation over all search results and collects the upload ids from all entries.</p>"},{"location":"howto/programmatic/download.html#download-some-processed-data-for-a-whole-dataset","title":"Download some processed data for a whole dataset","text":"<p>Similar to raw files, you can also download processed data. This is also an entry based operation based on a query. This time we also specify a <code>required</code> to explain which parts of the processed data, we are interested in:</p> <pre><code>curl -X POST \"https://nomad-lab.eu/prod/v1/api/v1/entries/archive/download/query\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"query\": {\n        \"datasets.doi\": \"10.17172/NOMAD/2023.11.17-2\"\n    },\n    \"required\": {\n        \"metadata\": {\n            \"entry_id\": \"*\",\n            \"mainfile\": \"*\",\n            \"upload_id\": \"*\"\n        },\n        \"results\": {\n            \"material\": \"*\"\n        },\n        \"run\": {\n            \"system[-1]\": {\n                \"atoms\": \"*\"\n            }\n        }\n    }\n}' \\\n-o download.zip\n</code></pre> <p>Here we use the <code>entries/archive/download/query</code> endpoint. The result is a zip file with one json file per entry. There are no directories and the files are named <code>&lt;entry-id&gt;.json</code>. To associate the json files with entries, you should require information that tells you more about the entries, e.g. <code>required.metadata.mainfile</code>.</p> <p>See also the How to access processed data how-to guide.</p>"},{"location":"howto/programmatic/local_parsers.html","title":"How to run a parser","text":"<p>You can find a list of all parsers and supported files in the reference.</p> <p>First you need to have the <code>nomad-lab</code> pypi package installed. You find more detailed instructions here:</p> <pre><code>pip install nomad-lab\n</code></pre>"},{"location":"howto/programmatic/local_parsers.html#from-the-command-line","title":"From the command line","text":"<p>You can run NOMAD parsers from the command line interface (CLI). The parse command will automatically match the right parser to your file and run the parser. There are two output formats:</p> <ul> <li><code>--show-metadata</code> a json representation of the basic metadata</li> <li><code>--show-archive</code> a json representation of the full parse results</li> <li><code>--preview-plots</code>: Optionally previews the generated plots.</li> <li><code>--save-plot-dir &lt;directory&gt;</code>: Specifies a directory to save the plot images.</li> </ul> <pre><code>nomad parse --show-archive &lt;path-to-your-mainfile-code-output-file&gt;\n</code></pre> <p>Note</p> <p>If you run into missing dependency errors, you might want to install additional dependencies via <code>pip install nomad-lab[parsing]</code>. Only a few parsers require extra dependencies. Please refer to the parser projects for more details.</p> <p>To skip the parser matching, i.e. the process that determined which parser fits to the given file, and state the parser directly, you can use the <code>--parser</code> argument to provide a parser name.</p> <pre><code>nomad parse --parser parsers/vasp &lt;path-to-your-mainfile-code-output-file&gt;\n</code></pre> <p>To skip the potentially error-prone and depending on your use-case unnecessary normalization, you can use the <code>--skip-normalizers</code> argument:</p> <pre><code>nomad parse --skip-normalizers &lt;path-to-your-mainfile-code-output-file&gt;\n</code></pre>"},{"location":"howto/programmatic/local_parsers.html#from-a-python-program","title":"From a python program","text":"<p>You can also use the NOMAD parsers within Python, as shown below. This will give you the parse results as metainfo objects to conveniently analyze the results in Python. See metainfo for more details on how to use the metainfo in Python.</p> <pre><code># requires: nomad-lab\nimport sys\nfrom nomad.client import parse, normalize_all\n\n# match and run the parser\narchives = parse('path/to/you/file')\n# run all normalizers\nfor archive in archives:\n    normalize_all(archive)\n\n    # get the 'main section' section_run as a metainfo object\n    section_run = archive.run[0]\n\n    # get the same data as JSON serializable Python dict\n    python_dict = section_run.m_to_dict()\n</code></pre>"},{"location":"howto/programmatic/publish_python.html","title":"How to publish data using python","text":""},{"location":"howto/programmatic/publish_python.html#uploading-changing-metadata-and-publishing-via-python-api","title":"Uploading, changing metadata, and publishing via python API","text":"<p>The NOMAD API allows uploading, publishing, etc. using a local python environment, as an alternative to the NOMAD GUI. An overview of all API functionalities is provided in How to use the API</p> <p>We have prepare some simple python functions to facilitate use of this API. For use as demonstrated below, copy the following code into a file called NOMAD_API.py:</p> <pre><code>import requests\n\ndef get_authentication_token(nomad_url, username, password):\n    '''Get the token for accessing your NOMAD unpublished uploads remotely'''\n    try:\n        response = requests.get(\n            nomad_url + 'auth/token', params=dict(username=username, password=password), timeout=10)\n        token = response.json().get('access_token')\n        if token:\n            return token\n\n        print('response is missing token: ')\n        print(response.json())\n        return\n    except Exception:\n        print('something went wrong trying to get authentication token')\n        return\n\n\ndef create_dataset(nomad_url, token, dataset_name):\n    '''Create a dataset to group a series of NOMAD entries'''\n    try:\n        response = requests.post(\n            nomad_url + 'datasets/',\n            headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n            json={\"dataset_name\": dataset_name},\n            timeout=10\n            )\n        dataset_id = response.json().get('dataset_id')\n        if dataset_id:\n            return dataset_id\n\n        print('response is missing dataset_id: ')\n        print(response.json())\n        return\n    except Exception:\n        print('something went wrong trying to create a dataset')\n        return\n\ndef upload_to_NOMAD(nomad_url, token, upload_file):\n    '''Upload a single file for NOMAD upload, e.g., zip format'''\n    with open(upload_file, 'rb') as f:\n        try:\n            response = requests.post(\n                nomad_url + 'uploads',\n                headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n                data=f, timeout=30)\n            upload_id = response.json().get('upload_id')\n            if upload_id:\n                return upload_id\n\n            print('response is missing upload_id: ')\n            print(response.json())\n            return\n        except Exception:\n            print('something went wrong uploading to NOMAD')\n            return\n\ndef check_upload_status(nomad_url, token, upload_id):\n    '''\n    # upload success =&gt; returns 'Process publish_upload completed successfully'\n    # publish success =&gt; 'Process publish_upload completed successfully'\n    '''\n    try:\n        response = requests.get(\n            nomad_url + 'uploads/' + upload_id,\n            headers={'Authorization': f'Bearer {token}'}, timeout=30)\n        status_message = response.json().get('data').get('last_status_message')\n        if status_message:\n            return status_message\n\n        print('response is missing status_message: ')\n        print(response.json())\n        return\n    except Exception:\n        print('something went wrong trying to check the status of upload' + upload_id)\n        # upload gets deleted from the upload staging area once published...or in this case something went wrong\n        return\n\ndef edit_upload_metadata(nomad_url, token, upload_id, metadata):\n    '''\n    Example of new metadata:\n    upload_name = 'Test_Upload_Name'\n    metadata = {\n        \"metadata\": {\n        \"upload_name\": upload_name,\n        \"references\": [\"https://doi.org/xx.xxxx/xxxxxx\"],\n        \"datasets\": dataset_id,\n        \"embargo_length\": 0,\n        \"coauthors\": [\"coauthor@affiliation.de\"],\n        \"comment\": 'This is a test upload...'\n        },\n    }\n    '''\n\n    try:\n        response = requests.post(\n            nomad_url+'uploads/' + upload_id + '/edit',\n            headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n            json=metadata, timeout=30)\n        return response\n    except Exception:\n        print('something went wrong trying to add metadata to upload' + upload_id)\n        return\n\ndef publish_upload(nomad_url, token, upload_id):\n    '''Publish an upload'''\n    try:\n        response = requests.post(\n            nomad_url+'uploads/' + upload_id + '/action/publish',\n            headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n            timeout=30)\n        return response\n    except Exception:\n        print('something went wrong trying to publish upload: ' + upload_id)\n        return\n</code></pre> <p>Now, we will demonstrate how to use these functions. Within a notebook or python script, import the above functions:</p> <pre><code>from Nomad_API import *`\n</code></pre> <p>Define the following user information: <pre><code>username = 'nomad_email@affiliation.edu'\npassword = 'password'\n</code></pre></p> <p>Define the NOMAD API endpoint: <pre><code># nomad_url = 'https://nomad-lab.eu/prod/v1/api/v1/'  # production nomad\nnomad_url = 'https://nomad-lab.eu/prod/v1/test/api/v1/'  # test nomad (deleted occassionally)\n</code></pre></p> <p>Get a token for accessing your unpublished uploads:</p> <pre><code>token = get_authentication_token(nomad_url, username, password)\n</code></pre> <p>Create a dataset for grouping uploads that belong to, e.g., a publication:</p> <pre><code>dataset_id = create_dataset(nomad_url, token, 'Test_Dataset')\n</code></pre> <p>Upload some test data to NOMAD:</p> <pre><code>upload_id = upload_to_NOMAD(nomad_url, token, 'test_data.zip')\n</code></pre> <p>Check the status to make sure the upload was processed correctly:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <p>The immediate result may be:</p> <pre><code>'Waiting for results (level 0)'\n</code></pre> <p>After some time you will get:</p> <pre><code>'Process process_upload completed successfully'\n</code></pre> Tip <p>Some data, e.g., large systems or molecular dynamics trajectories, take some time to process. In this case, you can call the above function intermittantly, e.g., in a while loop with a sleep call in between, waiting for <code>last_status_message</code> to be \"Process process_upload completed successfully\"</p> <p>Now that the upload processing is complete, we can add coauthors, references, and other comments, as well as link to a dataset and provide a proper name for the upload:</p> <pre><code>metadata = {\n    \"metadata\": {\n    \"upload_name\": 'Test_Upload',\n    \"references\": [\"https://doi.org/xx.xxxx/x.xxxx\"],\n    \"datasets\": dataset_id,\n    \"embargo_length\": 0,\n    \"coauthors\": [\"coauthor@affiliation.de\"],\n    \"comment\": 'This is a test upload...',\n},\n}\nresponse = edit_upload_metadata(nomad_url, token, upload_id, metadata)\n</code></pre> <p>Check the upload again to make sure that the metadata was changed:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <pre><code>'Process edit_upload_metadata completed successfully'\n</code></pre> <p>Now, we are ready to publish:</p> <pre><code>response = publish_upload(nomad_url, token, upload_id)\n</code></pre> <p>Once again check the status:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <pre><code>'Process publish_upload completed successfully'\n</code></pre>"},{"location":"howto/programmatic/pythonlib.html","title":"How to install nomad-lab","text":"<p>We provide a Python package called <code>nomad-lab</code>. The package can be used to run certain NOMAD features within local Python programming environments. It includes the NOMAD parsers and normalizers, or convenience functions to query the processed data on NOMAD.</p> <p>Released version of the package are hosted on pypi and you can install it with pip (or conda).</p> <p>To install the newest pypi release, simply use pip: <pre><code>pip install nomad-lab\n</code></pre></p> <p>Attention</p> <p>The latest develop versions might still be considered beta and might not be published to pypi. If you require specific new features you might need to install <code>nomad-lab</code> from our GitLab package registry. To use features of a specific commit or branch, consider to clone and build the project yourself.</p> <p>To install the latest release developer releases from our GitLab use: <pre><code>pip install nomad-lab --extra-index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre></p> <p>To install an older version of NOMAD (e.g. v0.10.x), you can of use reference the respective version on pypy: <pre><code>pip install nomad-lab==1.0.10\n</code></pre></p> <p>Certain functionality might require more dependencies. The basic install above, installs the dependencies for accessing the NOMAD Archive or running most of the NOMAD parsers.</p> <p>Other functions, e.g. running the NOMAD infrastructure, require additional dependencies. You can use the <code>[extra]</code> notation to install these extra requirements:</p> <p><pre><code>pip install nomad-lab[parsing]\npip install nomad-lab[infrastructure]\npip install nomad-lab[dev]\n</code></pre> The various extras have the following meaning:</p> <ul> <li>parsing, run all parsers, incl. parsers based on HDF5, netCDF, or asr</li> <li>infrastructure, everything to run NOMAD services, see also Oasis documentation</li> <li>dev, necessary to run development and build tools, e.g. pytest, pylint, mypy</li> </ul>"},{"location":"reference/annotations.html","title":"Schema annotations","text":"<p>Definitions in a schema can have annotations. These annotations provide additional information that NOMAD can use to alter its behavior around these definitions. Annotations are named blocks of key-value pairs:</p> <pre><code>definitions:\n  sections:\n    MyAnnotatedSection:\n      m_annotations:\n        annotation_name:\n          key1: value\n          key2: value\n</code></pre> <p>Many annotations control the representation of data in the GUI. This can be for plots or data entry/editing capabilities.</p>"},{"location":"reference/annotations.html#eln-annotations","title":"ELN annotations","text":"<p>These annotations control how data can be entered and edited. Use the key <code>eln</code> to add this annotations. For example:</p> <pre><code>class Sample(EntryData):\n    sample_id = Quantity(type=str, a_eln=dict(component='StringEditQuantity'))`)\n</code></pre> <p>or in YAML schemas: <pre><code>Sample:\n  quantities:\n    sample_id:\n      type: str\n      m_annotations:\n        eln:\n          component: StringEditQuantity\n</code></pre></p> <p>An <code>eln</code> annotation can be added to section and quantity definitions to different effects. In both cases, it controls how sections and quantities are represented in the GUI with different parameters; see below.</p> <p>The UI gives an overview about all ELN edit annotations and components here.</p> name type component <code>str</code> The form field component that is used to make the annotated quantity editable. If no component is given, the quantity won't be editable. This can be used on quantities only.The supported values are:<code>StringEditQuantity</code>: For editing simple short string values. <code>URLEditQuantity</code>: For editing strings that are validated to be URLs. <code>EnumEditQuantity</code>: For Editing enum values. Uses a dropdown list with enum values. This component may be used for short enumerates. <code>RadioEnumEditQuantity</code>: For Editing enum values. Uses radio buttons. <code>AutocompleteEditQuantity</code>: For editing enum values. Uses an autocomplete form with dropdown list. This component may be used for longer enumerates. <code>FileEditQuantity</code>: For editing a reference to a file. Will allow to choose a file or upload a file. <code>BoolEditQuantity</code>: For editing boolean choices. <code>NumberEditQuantity</code>: For editing numbers with our without unit. <code>SliderEditQuantity</code>: For editing numbers with a horizontal slider widget. <code>DateTimeEditQuantity</code>: For editing datetimes. <code>RichTextEditQuantity</code>: For editing long styled text with a rich text editor. <code>ReferenceEditQuantity</code>: For editing references to other sections. <code>UserEditQuantity</code>: For entering user information. Lets you choose a nomad user or enter information manually. <code>AuthorEditQuantity</code>: For entering author information manually.options: - <code>StringEditQuantity</code> - <code>URLEditQuantity</code> - <code>EnumEditQuantity</code> - <code>RadioEnumEditQuantity</code> - <code>AutocompleteEditQuantity</code> - <code>FileEditQuantity</code> - <code>BoolEditQuantity</code> - <code>NumberEditQuantity</code> - <code>SliderEditQuantity</code> - <code>DateTimeEditQuantity</code> - <code>DateEditQuantity</code> - <code>TimeEditQuantity</code> - <code>RichTextEditQuantity</code> - <code>ReferenceEditQuantity</code> - <code>UserEditQuantity</code> - <code>AuthorEditQuantity</code> - <code>QueryEditQuantity</code> label <code>str</code> [Deprecated] ELN label annotation has been deprecated and it is advised to utilize display annotation instead. Custom label for the quantity shown on the form field. It is recommended to adhere to the convention of using lowercase letters for the label, except for abbreviations which could be capitalized. props <code>Dict[str, Any]</code> A dictionary with additional props that are passed to the edit component. default <code>Any</code> Prefills any set form field component with the given value. This is different from the quantities <code>default</code> property. The quantities default is not stored in the data; the default value is assumed if no other value is given. The ELN form field default value will be stored, even if not changed. defaultDisplayUnit <code>str</code> This attribute is deprecated, use the <code>unit</code> attribute of <code>display</code> annotation instead. Allows to define a default unit to initialize a <code>NumberEditQuantity</code> with. The unit has to be compatible with the unit of the annotation quantity and the annotated quantity must have a unit. Only applies to quantities and with <code>component=NumberEditQuantity</code>.deprecated minValue <code>Union[int, float]</code> Allows to specify a minimum value for quantity annotations with number type. Will show an error, if outside numbers are entered. Only works on quantities and in conjunction with <code>component=NumberEditQuantity</code>. maxValue <code>Union[int, float]</code> Allows to specify a maximum value for quantity annotations with number type. Will show an error, if outside numbers are entered. Only works on quantities and in conjunction with <code>component=NumberEditQuantity</code>. showSectionLabel <code>int</code> To customize the ReferenceEditQuantity behaviour. If true the section label will be shown instead of referenced file name and the path to the section. hide <code>List[str]</code> This attribute is deprecated. Use <code>visible</code> attribute of <code>display</code> annotation instead. Allows you to hide certain quantities from a section editor. Give a list of quantity names. Quantities must exist in the section that this annotation is added to. Can only be used in section annotations.deprecated overview <code>int</code> Shows the annotation section on the entry's overview page. Can only be used on section annotations. lane_width <code>Union[str, int]</code> Value to overwrite the css width of the lane used to render the annotation section and its editor. properties <code>SectionProperties</code> The value to customize the quantities and sub sections of the annotation section. The supported keys: <code>visible</code>: To determine the visible quantities and sub sections by their names <code>editable</code>: To render things visible but not editable, e.g. in inheritance situations <code>order</code>: # To order things, properties listed in that order first, then the rest"},{"location":"reference/annotations.html#sectionproperties","title":"SectionProperties","text":"<p>The display settings for quantities and subsections. (Deprecated)</p> name type visible <code>Filter</code> Defines the visible quantities and subsections. (Deprecated)default: <code>1</code> editable <code>Filter</code> Defines the editable quantities and subsections. (Deprecated) order <code>List[str]</code> To customize the order of the quantities and subsections. (Deprecated)"},{"location":"reference/annotations.html#filter","title":"Filter","text":"<p>A filter defined by an include list or and exclude list of the quantities or subsections.</p> name type include <code>List[str]</code> The list of quantity or subsection names to be included. exclude <code>List[str]</code> The list of quantity or subsection names to be excluded."},{"location":"reference/annotations.html#browser","title":"Browser","text":"<p>The <code>browser</code> annotation allows to specify if the processed data browser needs to display a quantity differently. It can be applied to quantities. For example</p> <pre><code>    class Experiment(EntryData):\n        description = Quantity(type=str, a_browser=dict(render_value='HtmlValue'))\n</code></pre> <p>or in yaml</p> <pre><code>Experiment:\n  quantities:\n    description:\n      type: str\n      m_annotations:\n        browser:\n          render_value: HtmlValue\n</code></pre> name type adaptor <code>str</code> Allows to change the Adaptor implementation that is used to render the lane for this quantity. Possible values are:<code>RawFileAdaptor</code>: An adopter that is used to show files, including all file actions, like file preview.options: - <code>RawFileAdaptor</code> render_value <code>str</code> Allows to change the Component used to render the value of the quantity. Possible values are:<code>HtmlValue</code>: Renders a string as HTML. <code>JsonValue</code>: Renders a dict or list in a collapsable tree.options: - <code>JsonValue</code> - <code>HtmlValue</code>"},{"location":"reference/annotations.html#display-annotations","title":"Display annotations","text":""},{"location":"reference/annotations.html#display-annotation-for-quantities","title":"Display annotation for quantities","text":"<p>This annotations control how quantities are displayed in the GUI.  Use the key <code>display</code> to add this annotation. For example in Python:</p> <pre><code>class Example(EntryData):\n    sample_id = Quantity(type=str, a_display={'visible': False})\n</code></pre> <p>or in YAML: <pre><code>definitions:\n  Example:\n    quantities:\n      sample_id:\n        type: str\n        m_annotations:\n          display:\n            visible: false\n</code></pre></p> name type visible <code>Filter</code> Defines the visible quantities and subsections.default: <code>1</code> editable <code>Filter</code> Defines the editable quantities and subsections. unit <code>str</code> To determine the default display unit for quantity."},{"location":"reference/annotations.html#display-annotation-for-sections","title":"Display annotation for sections","text":"<p>This annotations control how sections are displayed in the GUI. Use the key <code>display</code> to add this annotation. For example in Python:</p> <pre><code>class Example(MSection):\n    m_def = Section(a_display={\n        'visible': False\n    })\n</code></pre> <p>or in YAML: <pre><code>definitions:\n  sections:\n    Example:\n      m_annotations:\n        display:\n          visible: false\n</code></pre></p> name type visible <code>Filter</code> Defines the visible quantities and subsections.default: <code>1</code> editable <code>Filter</code> Defines the editable quantities and subsections. order <code>List[str]</code> To customize the order of the quantities and subsections."},{"location":"reference/annotations.html#label_quantity","title":"<code>label_quantity</code>","text":"<p>This annotation goes in the section that we want to be filled with tabular data, not in the single quantities. It is used to give a name to the instances that might be created by the parser. If it is not provided, the name of the section itself will be used as name. Many times it is useful because, i. e., one might want to create a bundle of instances of, say, a \"Substrate\" class, each instance filename not being \"Substrate_1\", \"Substrate_2\", etc., but being named after a quantity contained in the class that is, for example, the specific ID of that sample.</p> <pre><code>MySection:\n  more:\n    label_quantity: my_quantity\n  quantities:\n    my_quantity:\n      type: np.float64\n      shape: ['*']\n      description: \"my quantity to be filled from the tabular data file\"\n      unit: K\n      m_annotations:\n        tabular:\n          name: \"Sheet1/my header\"\n        plot:\n          x: timestamp\n          y: ./my_quantity\n</code></pre> <p>Important</p> <p>The quantity designated as <code>label_quantity</code> should not be an array but a integer, float or string, to be set as the name of a file. If an array quantity is chosen, the parser would fall back to the use of the section as name.</p>"},{"location":"reference/annotations.html#tabular-data","title":"Tabular data","text":""},{"location":"reference/annotations.html#tabular","title":"<code>tabular</code>","text":"<p>Allows to map a quantity to a row or a column of a spreadsheet data-file. Should only be used in conjunction with <code>tabular_parser</code>.</p> name type name <code>str</code> The column name that should be mapped to the annotation quantity. Has to be the same string that is used in the header, i.e. first <code>.csv</code> line or first excel file <code>row</code>. For excel files with multiple sheets, the name can have the form <code>&lt;sheet name&gt;/&lt;column name&gt;</code>. Otherwise, only the first sheets is used. Has to be applied to the quantity that a column should be mapped to. unit <code>str</code> The unit of the value in the file. Has to be compatible with the annotated quantity's unit. Will be used to automatically convert the value. If this is not defined, the values will not be converted. Has to be applied to the quantity that a column should be mapped to. <p>Each and every quantity to be filled with data from tabular data files should be annotated as the following example. A practical example is provided in How To section.</p> <pre><code>my_quantity:\n  type: np.float64\n  shape: ['*']\n  description: \"my quantity to be filled from the tabular data file\"\n  unit: K\n  m_annotations:\n    tabular:\n      name: \"Sheet1/my header\"\n    plot:\n      x: timestamp\n      y: ./my_quantity\n</code></pre>"},{"location":"reference/annotations.html#tabular_parser","title":"<code>tabular_parser</code>","text":"<p>One special quantity will be dedicated to host the tabular data file. In the following examples it is called <code>data_file</code>, it contains the <code>tabular_parser</code> annotation, as shown below.</p> <p>Instructs NOMAD to treat a string valued scalar quantity as a file path and interprets the contents of this file as tabular data. Supports both <code>.csv</code> and Excel files.</p> name type parsing_options <code>TabularParsingOptions</code> Options on how to extract the data from csv/xlsx file. Under the hood, NOMAD uses pandas <code>Dataframe</code> to parse the data from tabular files. These are the available options that can be passed down to the parser.The supported values are:<code>skiprows</code>: Number of rows to be skipped while reading the file. <code>sep</code>: The character used to separate cells (specific to csv files). <code>comment</code>: The character denoting the commented lines. <code>separator</code>: An alias for <code>sep</code>.default: Complex object, default value not displayed. mapping_options <code>List[TabularMappingOptions]</code> A list of directives on how to map the extracted data from the csv/xlsx file to NOMAD. Each directive is a distinct directive, which allows for more modular definition of your tabular parser schema. If no item is provided, the entire schema is treated to be parsed under column mode.The supported values in each item of this list are:<code>mapping_mode</code>: A <code>list</code> of paths to the repeating sub-sections where the tabular quantities are to be filled from     individual rows of the excel/csv file (i.e. in the row mode). Each path is a <code>/</code> separated list of     nested sub-sections. The targeted sub-sections, will be considered when mapping table rows to quantities.     Has to be used to annotate the quantity that holds the path to the <code>.csv</code> or excel file. <code>file_mode</code>: The character used to separate cells (specific to csv files). <code>sections</code>: The character denoting the commented lines.default: <code>[]</code>"},{"location":"reference/annotations.html#tabularmappingoptions","title":"TabularMappingOptions","text":"name type mapping_mode <code>str</code> This controls the behaviour of mapping of the extracted data onto NOMAD schema.The supported values are:<code>row</code>: A <code>list</code> of paths to the repeating sub-sections where the tabular quantities are to be filled from     individual rows of the excel/csv file (i.e. in the row mode). Each path is a <code>/</code> separated list of     nested sub-sections. The targeted sub-sections, will be considered when mapping table rows to quantities.     Has to be used to annotate the quantity that holds the path to the <code>.csv</code> or excel file. <code>column</code>: A <code>list</code> of paths to the sub-sections where the tabular quantities are to be filled from the     entire column of the excel/csv file (i.e. in the column mode). Each path is a <code>/</code>     separated list of nested sub-sections. The targeted sub-sections, will be     considered when mapping table columns to quantities. Has to be used to annotate the quantity that     holds the path to the <code>.csv</code> or excel file. <code>enrty</code>: A <code>list</code> of paths to the (sub)sections where the tabular quantities are to be filled from individual rows     of the excel/csv file, to create distinct entries. Each path is a     <code>/</code> separated list of nested sub-sections. The targeted (sub)sections, will be     considered when mapping table rows to quantities. The schema of the resultant entry follows the     (sub)section's schema. In order to parse the entire schema using entry mode, then set the     first item in this list to <code>root</code>.     Has to be used to annotate the quantity that     holds the path to the <code>.csv</code> or excel file.default: <code>TabularMode.column</code>options: - <code>row</code> - <code>column</code> file_mode <code>str</code> This controls the behaviour of the parser towards working physical files in file system.The supported values are:<code>current_entry</code>: Processing the data into the same NOMAD entry. <code>single_new_entry</code>: Creating a new entry and processing the data into this new NOMAD entry. <code>multiple_new_entries</code>: Creating many new entries and processing the data into these new NOMAD entries.options: - <code>current_entry</code> - <code>single_new_entry</code> - <code>multiple_new_entries</code> sections <code>List[str]</code> A <code>list</code> of paths to the (sub)sections where the tabular quantities are to be filled from the data extracted from the tabular file."},{"location":"reference/annotations.html#tabularparsingoptions","title":"TabularParsingOptions","text":"name type skiprows <code>Union[List[int], int]</code> Number of rows to skip sep <code>str</code> Character identifier of a separator comment <code>str</code> Character identifier of a commented line separator <code>str</code> Alias for <code>sep</code>"},{"location":"reference/annotations.html#available-combinations","title":"Available Combinations","text":"Tutorial ref. <code>file_mode</code> <code>mapping_mode</code> <code>sections</code> How to ref. 1 <code>current_entry</code> <code>column</code> <code>root</code> HowTo 2 <code>current_entry</code> <code>column</code> my path HowTo np1 <code>current_entry</code> <code>row</code> <code>root</code> Not possible 3 <code>current_entry</code> <code>row</code> my path HowTo np2 <code>single_new_entry</code> <code>column</code> <code>root</code> Not possible 4 <code>single_new_entry</code> <code>column</code> my path HowTo np3 <code>single_new_entry</code> <code>row</code> <code>root</code> Not possible 5 <code>single_new_entry</code> <code>row</code> my path HowTo np4 <code>multiple_new_entries</code> <code>column</code> <code>root</code> Not possible np5 <code>multiple_new_entries</code> <code>column</code> my path Not possible 6 <code>multiple_new_entries</code> <code>row</code> <code>root</code> HowTo 7 <code>multiple_new_entries</code> <code>row</code> my path HowTo <pre><code>data_file:\n  type: str\n  description: \"the tabular data file containing data\"\n  m_annotations:\n    tabular_parser:\n      parsing_options:\n        comment: '#'\n      mapping_options:\n      - mapping_mode: column\n        file_mode: single_new_entry\n        sections:\n        - my_section/my_quantity\n</code></pre>"},{"location":"reference/annotations.html#plot","title":"Plot","text":"<p>The PlotSection base section serves as an additional functionality to your sections. This base section is designed to simplify the process of creating various types of plots, making it easy to use Plotly Express, Plotly Subplot, and the general Plotly graph objects.</p> <p>Features:</p> <ul> <li>Plotly Express: Create simple and quick plots with a high-level, expressive API.</li> <li>Plotly Subplot: Organize multiple plots into subplots for more complex visualizations.</li> <li>General Plotly Graph Objects: Fine-tune your plots by working directly with Plotly's graph objects.</li> </ul> <p>Usage:</p> <ul> <li>Inherit from this base section to leverage its plot functionality.</li> <li>Customize your plots using the annotations plotly-express, plotly-subplots, or/and plotly-graph-object.</li> </ul> <p>The PlotSection class makes it possible to define plots that are shown alongside your data. Underneath, we use the Plotly Open Source Graphing Libraries to control the creation of the plots, and you can find many useful examples in their documentation.</p> <p>In Python schemas, the PlotSection class gives you full freedom to define plots programmatically. For example, you could use plotly.express and plotly.graph_objs to define plots like this:</p> <pre><code>from nomad.datamodel.metainfo.plot import PlotSection, PlotlyFigure\nfrom nomad.datamodel.data import EntryData\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\nclass CustomSection(PlotSection, EntryData):\n    m_def = Section()\n    time = Quantity(type=float, shape=['*'], unit='s', a_eln=dict(component='NumberEditQuantity'))\n    substrate_temperature = Quantity(type=float, shape=['*'], unit='K', a_eln=dict(component='NumberEditQuantity'))\n    chamber_pressure = Quantity(type=float, shape=['*'], unit='Pa', a_eln=dict(component='NumberEditQuantity'))\n\n    def normalize(self, archive, logger):\n        super(CustomSection, self).normalize(archive, logger)\n\n        first_line = px.scatter(x=self.time, y=self.substrate_temperature)\n        second_line = px.scatter(x=self.time, y=self.chamber_pressure)\n        figure1 = make_subplots(rows=1, cols=2, shared_yaxes=True)\n        figure1.add_trace(first_line.data[0], row=1, col=1)\n        figure1.add_trace(second_line.data[0], row=1, col=2)\n        figure1.update_layout(height=400, width=716, title_text=\"Creating Subplots in Plotly\")\n        self.figures.append(PlotlyFigure(label='figure 1', figure=figure1.to_plotly_json()))\n\n        figure2 = px.scatter(x=self.substrate_temperature, y=self.chamber_pressure, color=self.chamber_pressure, title=\"Chamber as a function of Temperature\")\n        self.figures.append(PlotlyFigure(label='figure 2', index=1, figure=figure2.to_plotly_json()))\n\n        heatmap_data = [[None, None, None, 12, 13, 14, 15, 16],\n             [None, 1, None, 11, None, None, None, 17],\n             [None, 2, 6, 7, None, None, None, 18],\n             [None, 3, None, 8, None, None, None, 19],\n             [5, 4, 10, 9, None, None, None, 20],\n             [None, None, None, 27, None, None, None, 21],\n             [None, None, None, 26, 25, 24, 23, 22]]\n\n        heatmap = go.Heatmap(z=heatmap_data, showscale=False, connectgaps=True, zsmooth='best')\n        figure3 = go.Figure(data=heatmap)\n        figure_json = figure3.to_plotly_json()\n        figure_json['config'] = {'staticPlot': True}\n        self.figures.append(PlotlyFigure(label='figure 3', index=0, figure=figure_json)\n</code></pre> <p>To customize the plot configuration in python one can add the config to the generated json by to_plotly_json().</p> <pre><code>figure_json['config'] = {'staticPlot': True}\n</code></pre> <p>In YAML schemas, plots can be defined by using the PlotSection as a base class, and additionally utilizing different flavours of plot annotations. The different annotation options are described below.</p>"},{"location":"reference/annotations.html#plotlygraphobjectannotation","title":"PlotlyGraphObjectAnnotation","text":"<p>Allows to plot figures using plotly graph object.</p> <pre><code>    Example:\n      base_sections:\n        - 'nomad.datamodel.metainfo.plot.PlotSection'\n      m_annotations:\n        plotly_graph_object:\n        - data:\n            x: '#xArr'\n            y: '#xArr'\n          layout:\n            title:\n              text: 'Plotly Graph Object'\n          label: 'Plotly Graph Object'\n          index: 1\n</code></pre> name type label <code>str</code> Figure label data <code>Dict</code> Plotly data layout <code>Dict</code> Plotly layout config <code>Dict</code> Plotly config"},{"location":"reference/annotations.html#plotlyexpressannotation","title":"PlotlyExpressAnnotation","text":"<p>Allows to plot multi trace figures using plotly Express.</p> <pre><code>  sections:\n    Example:\n      base_sections:\n        - 'nomad.datamodel.metainfo.plot.PlotSection'\n      m_annotations:\n        plotly_express:\n          method: scatter\n          x: '#xArr'\n          y: '#yArr'\n          label: 'Example Express Plot'\n          index: 0\n          layout:\n            title:\n              text: 'Example Express Plot'\n            xaxis:\n              title:\n                text: 'x axis'\n            yaxis:\n              title:\n                text: 'y axis'\n          traces:\n            - method: scatter\n              x: '#xArr'\n              y: '#zArr'\n</code></pre> name type method <code>str</code> Plotly express plot method layout <code>Dict</code> Plotly layout x <code>Union[List[float], List[str], str]</code> Plotly express x y <code>Union[List[float], List[str], str]</code> Plotly express y z <code>Union[List[float], List[str], str]</code> Plotly express z color <code>Union[List[float], List[str], str]</code> Plotly express color symbol <code>str</code> Plotly express symbol title <code>str</code> Plotly express title label <code>str</code> Figure label traces <code>List[PlotlyExpressTraceAnnotation]</code> List of traces added to the main trace defined by plotly_express methoddefault: <code>[]</code>"},{"location":"reference/annotations.html#plotlyexpresstraceannotation","title":"PlotlyExpressTraceAnnotation","text":"<p>Allows to plot figures using plotly Express.</p> name type method <code>str</code> Plotly express plot method layout <code>Dict</code> Plotly layout x <code>Union[List[float], List[str], str]</code> Plotly express x y <code>Union[List[float], List[str], str]</code> Plotly express y z <code>Union[List[float], List[str], str]</code> Plotly express z color <code>Union[List[float], List[str], str]</code> Plotly express color symbol <code>str</code> Plotly express symbol title <code>str</code> Plotly express title"},{"location":"reference/annotations.html#plotlysubplotsannotation","title":"PlotlySubplotsAnnotation","text":"<p>Allows to plot figures in subplots.</p> <pre><code>    Example:\n      base_sections:\n        - 'nomad.datamodel.metainfo.plot.PlotSection'\n      m_annotations:\n        plotly_subplots:\n          parameters:\n            rows: 2\n            cols: 2\n          layout:\n            title:\n              text: 'All plots'\n          plotly_express:\n            - method: scatter\n              x: '#xArr'\n              y: '#yArr'\n              title: 'subplot 1'\n            - method: scatter\n              x: '#xArr'\n              y: '#zArr'\n              title: 'subplot 2'\n            - method: scatter\n              x: '#zArr'\n              y: '#xArr'\n              title: 'subplot 3'\n            - method: scatter\n              x: '#zArr'\n              y: '#yArr'\n              title: 'subplot 4'\n</code></pre> name type label <code>str</code> Figure label layout <code>Dict</code> Plotly layout parameters <code>Dict</code> plotly.subplots.make_subplots parameters i.e. rows, cols, shared_xaxes, shared_xaxes, horizontal_spacing , ... See plotly make_subplots documentation for more information. plotly_express <code>List[PlotlyExpressAnnotation]</code> List of subplots defined by plotly_express methoddefault: <code>[]</code>"},{"location":"reference/annotations.html#plot-annotations-in-python","title":"plot annotations in python","text":"<p>For simple plots in Python schema one could use the annotations without normalizer:</p> <pre><code>from nomad.datamodel.metainfo.plot import PlotSection\nfrom nomad.metainfo import Quantity, Section\nfrom nomad.datamodel.data import EntryData\n\nclass CustomSection(PlotSection, EntryData):\n    m_def = Section(\n        a_plotly_graph_object=[\n            {\n                'label': 'graph object 1',\n                'data': {'x': '#time', 'y': '#chamber_pressure'},\n                'layout': {\n                    'title': {\n                        'text': 'Plot in section level'\n                    },\n                    'xaxis': {\n                        'title': {\n                            'text': 'x data'\n                        }\n                    },\n                    'yaxis': {\n                        'title': {\n                            'text': 'y data'\n                        }\n                    }\n                }\n            }, {\n                'label': 'graph object 2',\n                'data': {'x': '#time', 'y': '#substrate_temperature'}\n            }\n        ],\n        a_plotly_express={\n            'label': 'fig 2',\n            'index': 2,\n            'method': 'scatter',\n            'x': '#substrate_temperature',\n            'y': '#chamber_pressure',\n            'color': '#chamber_pressure'\n        },\n        a_plotly_subplots={\n            'label': 'fig 1',\n            'index': 1,\n            'parameters': {'rows': 2, 'cols': 2},\n            'layout': {\n                'title': {\n                    'text': 'All plots'\n                }\n            },\n            'plotly_express': [\n                {\n                    'method': 'scatter',\n                    'x': '#time',\n                    'y': '#chamber_pressure',\n                    'color': '#chamber_pressure'\n                },\n                {\n                    'method': 'scatter',\n                    'x': '#time',\n                    'y': '#substrate_temperature',\n                    'color': '#substrate_temperature'\n                },\n                {\n                    'method': 'scatter',\n                    'x': '#substrate_temperature',\n                    'y': '#chamber_pressure',\n                    'color': '#chamber_pressure'\n                },\n                {\n                    'method': 'scatter',\n                    'x': '#substrate_temperature',\n                    'y': '#chamber_pressure',\n                    'color': '#substrate_temperature'\n                }\n            ]\n        }\n    )\n    time = Quantity(type=float, shape=['*'], unit='s', a_eln=dict(component='NumberEditQuantity'))\n    substrate_temperature = Quantity(type=float, shape=['*'], unit='K', a_eln=dict(component='NumberEditQuantity'))\n    chamber_pressure = Quantity(type=float, shape=['*'], unit='Pa', a_eln=dict(component='NumberEditQuantity'))\n</code></pre>"},{"location":"reference/annotations.html#plotannotation-deprecated","title":"PlotAnnotation (Deprecated)","text":"<p>The <code>PlotAnnotation</code> is now deprecated and will be removed in future releases. We recommend transitioning to the use of <code>PlotSection</code> and <code>PlotlyGraphObjectAnnotation</code> for your plotting needs.</p> <p>This annotation can be used to add a plot to a section or quantity. Example:</p> <pre><code>class Evaporation(MSection):\n    m_def = Section(a_plot={\n        'label': 'Temperature and Pressure',\n        'x': 'process_time',\n        'y': ['./substrate_temperature', './chamber_pressure'],\n        'config': {\n            'editable': True,\n            'scrollZoom': False\n        }\n    })\n    time = Quantity(type=float, shape=['*'], unit='s')\n    substrate_temperature = Quantity(type=float, shape=['*'], unit='K')\n    chamber_pressure = Quantity(type=float, shape=['*'], unit='Pa')\n</code></pre> <p>You can create multi-line plots by using lists of the properties <code>y</code> (and <code>x</code>). You either have multiple sets of <code>y</code>-values over a single set of <code>x</code>-values. Or you have pairs of <code>x</code> and <code>y</code> values. For this purpose the annotation properties <code>x</code> and <code>y</code> can reference a single quantity or a list of quantities. For repeating sub sections, the section instance can be selected with an index, e.g. \"sub_section_name/2/parameter_name\" or with a slice notation <code>start:stop</code> where negative values index from the end of the array, e.g. \"sub_section_name/1:-5/parameter_name\".</p> <p>The interactive examples of the plot annotations can be found here.</p> name type label <code>str</code> Is passed to plotly to define the label of the plot. x <code>Union[List[str], str]</code> A path or list of paths to the x-axes values. Each path is a <code>/</code> separated list of sub-section and quantity names that leads from the annotation section to the quantity. Repeating sub sections are indexed between two <code>/</code>s with an integer or a slice <code>start:stop</code>. y <code>Union[List[str], str]</code> A path or list of paths to the y-axes values. list of sub-section and quantity names that leads from the annotation section to the quantity. Repeating sub sections are indexed between two <code>/</code>s with an integer or a slice <code>start:stop</code>. lines <code>List[dict]</code> A list of dicts passed as <code>traces</code> to plotly to configure the lines of the plot. See https://plotly.com/javascript/reference/scatter/ for details. layout <code>dict</code> A dict passed as <code>layout</code> to plotly to configure the plot layout. See https://plotly.com/javascript/reference/layout/ for details. config <code>dict</code> A dict passed as <code>config</code> to plotly to configure the plot functionality. See https://plotly.com/javascript/configuration-options/ for details."},{"location":"reference/cli.html","title":"Command Line Interface (CLI)","text":""},{"location":"reference/cli.html#nomad","title":"nomad","text":"<p>This is the entry point to nomad's command line interface CLI. It uses a sub-command structure similar to the git command.</p> <p>Usage:</p> <pre><code>nomad [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -v, --verbose     sets log level to info\n  --debug           sets log level to debug\n  --log-label TEXT  Label applied to logg entries.\n  --help            Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>admin: The nomad admin commands to do nasty stuff directly on the databases.</li> <li>clean: Cleanse the given path by removing empty folders.</li> <li>client: Commands that use the nomad API to do useful things</li> <li>dev: Commands related to the nomad source code.</li> <li>parse: Run parsing and normalizing locally.</li> </ul>"},{"location":"reference/cli.html#nomad-admin","title":"nomad admin","text":"<p>The nomad admin commands to do nasty stuff directly on the databases. Remember: With great power comes great responsibility!</p> <p>Usage:</p> <pre><code>nomad admin [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>clean: Checks consistency of files and es vs mongo and deletes orphan entries.</li> <li>entries: Entry related commands</li> <li>lift-embargo: Check and lift embargo of data with expired embargo period.</li> <li>ops: Generate scripts and commands for nomad operation.</li> <li>reset: Reset/remove all databases.</li> <li>reset-processing: Reset all uploads and entries \"stuck\" in processing using level mongodb operations.</li> <li>rewrite-doi-urls: </li> <li>run: Run a nomad service locally (outside docker).</li> <li>upgrade: Commands for upgrading to a newer NOMAD version</li> <li>uploads: Upload related commands</li> <li>users: Add, import, export users.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-clean","title":"nomad admin clean","text":"<p>Checks consistency of files and es vs mongo and deletes orphan entries.</p> <p>Usage:</p> <pre><code>nomad admin clean [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --dry           Do not delete anything, just check.\n  --skip-entries  Skip cleaning entries with missing uploads.\n  --skip-fs       Skip cleaning the filesystem.\n  --skip-es       Skip cleaning the es index.\n  --staging-too   Also clean published entries in staging, make sure these\n                  files are not due to reprocessing\n  --force         Do not ask for confirmation.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-entries","title":"nomad admin entries","text":"<p>Entry related commands</p> <p>Usage:</p> <pre><code>nomad admin entries [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>rm: Delete selected entries from mongo and elastic</li> </ul>"},{"location":"reference/cli.html#nomad-admin-entries-rm","title":"nomad admin entries rm","text":"<p>Delete selected entries from mongo and elastic</p> <p>Usage:</p> <pre><code>nomad admin entries rm [OPTIONS] [ENTRIES]...\n</code></pre> <p>Options:</p> <pre><code>  --skip-es     Keep the elastic index version of the data.\n  --skip-mongo  Keep uploads and entries in mongo.\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-lift-embargo","title":"nomad admin lift-embargo","text":"<p>Check and lift embargo of data with expired embargo period.</p> <p>Usage:</p> <pre><code>nomad admin lift-embargo [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --dry               Do not lift the embargo, just show what needs to be\n                      done.\n  --parallel INTEGER  Use the given amount of parallel processes. Default is\n                      1.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops","title":"nomad admin ops","text":"<p>Generate scripts and commands for nomad operation.</p> <p>Usage:</p> <pre><code>nomad admin ops [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>dump: Dump the mongo db.</li> <li>nginx-conf: Generate an nginx.conf to serve the GUI and proxy pass to API container.</li> <li>prototypes-update: Updates the AFLOW prototype information using the latest online version and writes the results to a python module in the given FILEPATH.</li> <li>restore: Restore the mongo db.</li> <li>springer-update: Updates the springer database in nomad.config.normalize.springer_db_path.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-ops-dump","title":"nomad admin ops dump","text":"<p>Dump the mongo db.</p> <p>Usage:</p> <pre><code>nomad admin ops dump [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --restore  Do not dump, but restore.\n  --help     Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops-nginx-conf","title":"nomad admin ops nginx-conf","text":"<p>Generate an nginx.conf to serve the GUI and proxy pass to API container.</p> <p>Usage:</p> <pre><code>nomad admin ops nginx-conf [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --prefix TEXT           Alter the url path prefix.\n  --host TEXT             Alter the NOMAD app host.\n  --port TEXT             Alter the NOMAD port host.\n  --server / --no-server  Control writing of the outer server {} block. Useful\n                          when conf file is included within another\n                          nginx.conf.\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops-prototypes-update","title":"nomad admin ops prototypes-update","text":"<p>Updates the AFLOW prototype information using the latest online version and writes the results to a python module in the given FILEPATH.</p> <p>Usage:</p> <pre><code>nomad admin ops prototypes-update [OPTIONS] FILEPATH\n</code></pre> <p>Options:</p> <pre><code>  --matches-only  Only update the match information that depends on the\n                  symmetry analysis settings. Will not perform an online\n                  update.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops-restore","title":"nomad admin ops restore","text":"<p>Restore the mongo db.</p> <p>Usage:</p> <pre><code>nomad admin ops restore [OPTIONS] PATH_TO_DUMP\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops-springer-update","title":"nomad admin ops springer-update","text":"<p>Updates the springer database in nomad.config.normalize.springer_db_path.</p> <p>Usage:</p> <pre><code>nomad admin ops springer-update [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --max-n-query INTEGER  Number of unsuccessful springer request before\n                         returning an error. Default is 10.\n  --retry-time INTEGER   Time in seconds to retry after unsuccessful request.\n                         Default is 120.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-reset","title":"nomad admin reset","text":"<p>Reset/remove all databases.</p> <p>Usage:</p> <pre><code>nomad admin reset [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --remove            Do not just reset all dbs, but also remove them.\n  --i-am-really-sure  Must be set for the command to to anything.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-reset-processing","title":"nomad admin reset-processing","text":"<p>Reset all uploads and entries \"stuck\" in processing using level mongodb operations.</p> <p>Usage:</p> <pre><code>nomad admin reset-processing [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --zero-complete-time  Sets the complete time to epoch zero.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-rewrite-doi-urls","title":"nomad admin rewrite-doi-urls","text":"<p>Rewrites the existing dataset URLs in existing DOI records with freshly generated dataset URLs. This is useful, if the URL layout has changed.</p> <p>Usage:</p> <pre><code>nomad admin rewrite-doi-urls [OPTIONS] [DOIS]...\n</code></pre> <p>Options:</p> <pre><code>  --dry                         Just test if DOI exists and print is current\n                                URL.\n  --save-existing-records TEXT  A filename to store the existing DOI records\n                                in.\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-run","title":"nomad admin run","text":"<p>Run a nomad service locally (outside docker).</p> <p>Usage:</p> <pre><code>nomad admin run [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>app: Run the nomad development app with all apis.</li> <li>appworker: Run both app and worker.</li> <li>hub: Run the jupyter hub.</li> <li>worker: Run the nomad development worker.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-run-app","title":"nomad admin run app","text":"<p>Run the nomad development app with all apis.</p> <p>Usage:</p> <pre><code>nomad admin run app [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --with-gui         The app will configure the gui for production and service\n                     it.\n  --host TEXT        Passed as host parameter.\n  --port INTEGER     Passed as port parameter.\n  --log-config TEXT  Passed as log-config parameter.\n  --gunicorn         Run app with gunicorn instead of uvicorn.\n  --workers INTEGER  Passed to uvicorn workers parameter.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-run-appworker","title":"nomad admin run appworker","text":"<p>Run both app and worker.</p> <p>Usage:</p> <pre><code>nomad admin run appworker [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --app-host TEXT            Passed as app host parameter.\n  --app-port INTEGER         Passed as app port parameter.\n  --fastapi-workers INTEGER  Number of FastAPI workers.\n  --celery-workers INTEGER   Number of Celery workers.\n  --dev                      Use one worker (for dev. env.).\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-run-hub","title":"nomad admin run hub","text":"<p>Run the jupyter hub.</p> <p>Usage:</p> <pre><code>nomad admin run hub [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-run-worker","title":"nomad admin run worker","text":"<p>Run the nomad development worker.</p> <p>Usage:</p> <pre><code>nomad admin run worker [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --workers INTEGER  Number of celery workers.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-upgrade","title":"nomad admin upgrade","text":"<p>Commands for upgrading to a newer NOMAD version</p> <p>Usage:</p> <pre><code>nomad admin upgrade [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>migrate-mongo: Converts (upgrades) records from one mongodb and migrates to another.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-upgrade-migrate-mongo","title":"nomad admin upgrade migrate-mongo","text":"<p>Converts (upgrades) records from one mongodb and migrates to another. Note, it is strongly recommended to run this command with loglevel verbose, i.e.</p> <pre><code>nomad -v upgrade migrate-mongo ...\n</code></pre> <p>Usage:</p> <pre><code>nomad admin upgrade migrate-mongo [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --host TEXT                     The mongodb host. By default same as the\n                                  configured db.\n  --port INTEGER                  The mongodb port. By default same as the\n                                  configured db.\n  --src-db-name TEXT              The name of the source database.  [required]\n  --dst-db-name TEXT              The name of the destination database. By\n                                  default same as the configured db.\n  --upload-query TEXT             An mongo upload query. All uploads matching\n                                  the query will be included in the migration.\n  --entry-query TEXT              An mongo entry query. All uploads with an\n                                  entry matching the query will be included in\n                                  the migration.\n  --ids-from-file TEXT            Reads upload IDs from the specified file.\n                                  Cannot be used together with the --upload-\n                                  query or --entry-query options. This can for\n                                  example be used to retry just the uploads\n                                  that has previously failed (as these ids can\n                                  be exported to file using --failed-ids-to-\n                                  file). You can specify both --ids-from-file\n                                  and --failed-ids-to-file at the same time\n                                  with the same file name.\n  --failed-ids-to-file TEXT       Write the IDs of failed and skipped uploads\n                                  to the specified file. This can for example\n                                  be used to subsequently retry just the\n                                  uploads that failed (as these ids can be\n                                  loaded from file using --ids-from-file). You\n                                  can specify both --ids-from-file and\n                                  --failed-ids-to-file at the same time with\n                                  the same file name.\n  --upload-update TEXT            json with updates to apply to all converted\n                                  uploads\n  --entry-update TEXT             json with updates to apply to all converted\n                                  entries\n  --overwrite [always|if-newer|never]\n                                  If an upload already exists in the\n                                  destination db, this option determines\n                                  whether it and its child records should be\n                                  overwritten with the data from the source\n                                  db. Possible values are \"always\", \"if-\n                                  newer\", \"never\". Selecting \"always\" always\n                                  overwrites, \"never\" never overwrites, and\n                                  \"if-newer\" overwrites if the upload either\n                                  doesn't exist in the destination, or it\n                                  exists but its complete_time (i.e. last time\n                                  it was processed) is older than in the\n                                  source db.\n  --fix-problems                  If a minor, fixable problem is encountered,\n                                  fixes it automaticall; otherwise fail.\n  --dry                           Dry run (not writing anything to the\n                                  destination database).\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads","title":"nomad admin uploads","text":"<p>Upload related commands</p> <p>Usage:</p> <pre><code>nomad admin uploads [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --uploads-mongo-query TEXT      A query\n  --entries-mongo-query TEXT      A query\n  --entries-es-query TEXT         A query\n  --unpublished                   Select only uploads in staging\n  --published                     Select only uploads that are publised\n  --outdated                      Select published uploads with older nomad\n                                  version\n  --processing                    Select only processing uploads\n  --processing-failure-uploads    Select uploads with failed processing\n  --processing-failure-entries    Select uploads with entries with failed\n                                  processing\n  --processing-failure            Select uploads where the upload or any entry\n                                  has failed processing\n  --processing-incomplete-uploads\n                                  Select uploads that have not yet been\n                                  processed\n  --processing-incomplete-entries\n                                  Select uploads where any entry has net yot\n                                  been processed\n  --processing-incomplete         Select uploads where the upload or any entry\n                                  has not yet been processed\n  --processing-necessary          Select uploads where the upload or any entry\n                                  has either not been processed or processing\n                                  has failed in the past\n  --unindexed                     Select uploads that have no entries in the\n                                  elastic search index.\n  --help                          Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>chown: Change the owner of the upload and all its entries.</li> <li>convert-archive: Convert selected uploads to the new format.</li> <li>export: List selected uploads</li> <li>export-bundle: Export one or more uploads as bundles.</li> <li>import-bundle: Import one or more uploads from bundles. Unless specified by the user,</li> <li>index: (Re-)index all entries of the given uploads.</li> <li>integrity: Check certain integrity criteria and return a list of upload IDs.</li> <li>ls: List selected uploads</li> <li>process: Reprocess selected uploads.</li> <li>publish: Publish selected uploads.</li> <li>re-pack: Repack selected uploads.</li> <li>reset: Reset the processing state.</li> <li>rm: Delete selected upload</li> <li>stop: Attempt to abort the processing of uploads.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-uploads-chown","title":"nomad admin uploads chown","text":"<p>Change the owner of the upload and all its entries.</p> <p>Usage:</p> <pre><code>nomad admin uploads chown [OPTIONS] USERNAME [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-convert-archive","title":"nomad admin uploads convert-archive","text":"<p>Convert selected uploads to the new format.</p> <p>Usage:</p> <pre><code>nomad admin uploads convert-archive [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  -o, --overwrite           Overwrite existing target files.\n  -d, --delete-old          Delete old archives once successfully converted.\n  -m, --migrate             Only convert v1 archive files to v1.2 archive\n                            files.\n  -f, --force-repack        Force repacking existing archives that are already\n                            in the new format\n  -p, --parallel INTEGER    Number of processes to use for conversion. Default\n                            is os.cpu_count().\n  -s, --size-limit INTEGER  Only handle archives under limited size in GB.\n                            Default is -1 (no limit).\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-export","title":"nomad admin uploads export","text":"<p>List selected uploads</p> <p>Usage:</p> <pre><code>nomad admin uploads export [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --required TEXT    The required in JSON format\n  -o, --output TEXT  The file to write data to\n  --help             Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-export-bundle","title":"nomad admin uploads export-bundle","text":"<p>Export one or more uploads as bundles.</p> <p>Usage:</p> <pre><code>nomad admin uploads export-bundle [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --out-dir TEXT       Output folder. Default value is \"./bundles\" (defined in\n                       config)\n  --uncompressed       Specify to export each bundle as an uncompressed\n                       folder, instead of a zip-file.\n  --overwrite          Specify to, for each bundle, overwrite the destination\n                       file/folder if it already exists.\n  -s, --settings TEXT  The export settings, specified as json. Settings not\n                       specified in the dictionary will be set to the default\n                       values.\n  -i, --ignore-errors  Specify to ignore errors on individual uploads, and\n                       continue exporting (the default behaviour is to abort\n                       on first failing upload).\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-import-bundle","title":"nomad admin uploads import-bundle","text":"<p>Import one or more uploads from bundles. Unless specified by the user, the configured default import settings are used.</p> <p>Usage:</p> <pre><code>nomad admin uploads import-bundle [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --in TEXT                     The input path, specifying a bundle or a\n                                folder containing multiple bundles.\n  -m, --multi                   Specify this flag if the input_path is a\n                                folder containing multiple bundles, and all\n                                these should be imported. If this option is\n                                specified without specifying --in, we will\n                                default the input path to ./bundles\n  -s, --settings TEXT           The import settings, specified as json.\n                                Settings not specified in the dictionary will\n                                be set to the default values.\n  -e, --embargo_length INTEGER  The embargo length (0-36 months). 0 means no\n                                embargo. If unspecified, the embargo period\n                                defined in the bundle will be used.\n  -c, --use-celery              If specified, uses celery and the worker pool\n                                to do the main part of the import. NOTE: this\n                                requires that the workers can access the\n                                bundle via the exact same path.\n  -i, --ignore-errors           Specify this flag to ignore errors on\n                                individual bundles, and continue importing\n                                (the default behaviour is to abort on first\n                                failing bundle).\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-index","title":"nomad admin uploads index","text":"<p>(Re-)index all entries of the given uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads index [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --parallel INTEGER        Use the given amount of parallel processes.\n                            Default is 1.\n  --transformer TEXT        Qualified name to a Python function that should be\n                            applied to each EntryMetadata.\n  --skip-materials          Only update the entries index.\n  --print-progress INTEGER  Prints a dot every given seconds. Can be used to\n                            keep terminal open that have an i/o-based timeout.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-integrity","title":"nomad admin uploads integrity","text":"<p>Check certain integrity criteria and return a list of upload IDs.</p> <p>Usage:</p> <pre><code>nomad admin uploads integrity [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --both-storages           Select uploads that have both staging and public\n                            versions.\n  --missing-storage         Select uploads of which the corresponding raw\n                            folder (for staging) or the raw zip archive (for\n                            public) is missing in the file system. This only\n                            checks the existence of folder/archive uploaded by\n                            user. To check the contents, use the --missing-\n                            raw-files flag.\n  --missing-raw-files       Select uploads that any of the files listed in\n                            metadata/files is missing. Use --check-all-entries\n                            to check files for all entries in the upload. It\n                            uses the indexed ES data and does not open the\n                            msgpack archive files.\n  --missing-archive-files   Select uploads that miss archive (msgpack) files.\n  --missing-index           Select uploads of which the ES index information\n                            is missing.\n  --entry-mismatch          Select uploads that have different numbers of\n                            entries in mongo and ES.\n  --nomad-version-mismatch  Select uploads that have different nomad versions\n                            in archive and ES.\n  --old-archive-format      Select uploads that are using the old archive\n                            format (v1).\n  --not-preferred-suffix    Select uploads that are using the preferred\n                            (first) suffix in the configuration.\n  --check-all-entries       Check all entries in the upload, otherwise only\n                            check one entry per upload.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-ls","title":"nomad admin uploads ls","text":"<p>List selected uploads</p> <p>Usage:</p> <pre><code>nomad admin uploads ls [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  -e, --entries   Include details about upload entries in the output.\n  --ids           Only include the upload ids in the output.\n  --json          Output a JSON array instead of a tabulated list.\n  --size INTEGER  Controls the maximum size of returned uploads, use -1 to\n                  return all.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-process","title":"nomad admin uploads process","text":"<p>Reprocess selected uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads process [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --parallel INTEGER        Use the given amount of parallel processes.\n                            Default is 1.\n  --process-running         Also reprocess already running processes.\n  --setting TEXT            key=value to overwrite a default reprocess config\n                            setting.\n  --print-progress INTEGER  Prints a dot every given seconds. Can be used to\n                            keep terminal open that have an i/o-based timeout.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-publish","title":"nomad admin uploads publish","text":"<p>Publish selected uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads publish [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --parallel INTEGER        Use the given amount of parallel processes.\n                            Default is 1.\n  --embargo-length INTEGER  Use an embargo length (months) for the\n                            publication.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-re-pack","title":"nomad admin uploads re-pack","text":"<p>Repack selected uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads re-pack [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-reset","title":"nomad admin uploads reset","text":"<p>Reset the processing state.</p> <p>Usage:</p> <pre><code>nomad admin uploads reset [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --with-entries  Also reset all entries.\n  --success       Set the process status to success instead of pending\n  --failure       Set the process status to failure instead of pending.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-rm","title":"nomad admin uploads rm","text":"<p>Delete selected upload</p> <p>Usage:</p> <pre><code>nomad admin uploads rm [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --skip-es     Keep the elastic index version of the data.\n  --skip-mongo  Keep uploads and entries in mongo.\n  --skip-files  Keep all related files.\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-stop","title":"nomad admin uploads stop","text":"<p>Attempt to abort the processing of uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads stop [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --entries    Only stop entries processing.\n  --kill       Use the kill signal and force task failure.\n  --no-celery  Do not attempt to stop the actual celery tasks\n  --help       Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-users","title":"nomad admin users","text":"<p>Add, import, export users.</p> <p>Usage:</p> <pre><code>nomad admin users [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>import: Import users to keycloak from a JSON file.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-users-import","title":"nomad admin users import","text":"<p>Import users to keycloak from a JSON file.</p> <p>Usage:</p> <pre><code>nomad admin users import [OPTIONS] PATH_TO_USERS_FILE\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-clean","title":"nomad clean","text":"<p>Cleanse the given path by removing empty folders.</p> <p>Usage:</p> <pre><code>nomad clean [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --path TEXT  Cleanse the given path by removing empty folders.\n  --help       Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client","title":"nomad client","text":"<p>Commands that use the nomad API to do useful things</p> <p>Usage:</p> <pre><code>nomad client [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -n, --url TEXT       The URL where nomad is running, default is\n                       \"http://nomad-lab.eu/prod/v1/api\".\n  -u, --user TEXT      the user name to login, default is no login.\n  -w, --password TEXT  the password used to login.\n  --token-via-api      retrieve the access token from the api not keycloak.\n  --no-ssl-verify      disables SSL verificaton when talking to nomad.\n  --help               Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>datatests: Metainfo compatibility tests against data in a NOMAD installation.</li> <li>integrationtests: Runs a few example operations as a test.</li> <li>local: Run processing locally.</li> <li>synchdb: Synchronizes the NOMAD database with the given external database.</li> <li>upload: Upload files to nomad. The given path can be a single file or a directory. For file(s) that are not .zip or .tar files, a temporary .zip file will be created and uploaded.</li> </ul>"},{"location":"reference/cli.html#nomad-client-datatests","title":"nomad client datatests","text":"<p>Metainfo compatibility tests against data in a NOMAD installation.</p> <p>Usage:</p> <pre><code>nomad client datatests [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client-integrationtests","title":"nomad client integrationtests","text":"<p>Runs a few example operations as a test.</p> <p>Usage:</p> <pre><code>nomad client integrationtests [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --skip-parsers  Skip extensive upload and parser tests.\n  --skip-publish  Skip publish the upload. Should not be done on an production\n                  environment.\n  --skip-doi      Skip assigning a doi to a dataset.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client-local","title":"nomad client local","text":"<p>Run processing locally.</p> <p>Usage:</p> <pre><code>nomad client local [OPTIONS] ENTRY_ID\n</code></pre> <p>Options:</p> <pre><code>  --override          Override existing local entry data.\n  --show-archive      Print the archive data.\n  --show-metadata     Print the extracted repo metadata.\n  --skip-normalizers  Do not normalize.\n  --not-strict        Also match artificial parsers.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client-synchdb","title":"nomad client synchdb","text":"<p>Synchronizes the NOMAD database with the given external database.</p> <p>Usage:</p> <pre><code>nomad client synchdb [OPTIONS] DB_NAME ROOT_URL\n</code></pre> <p>Options:</p> <pre><code>  --outfile TEXT      File to read/write files missing in NOMAD database\n  --nomadfile TEXT    File to read/write files in NOMAD database\n  --dbfile TEXT       File to read/write files in given database\n  --local_path TEXT   Directory to which the files will be downloaded\n  --parallel INTEGER  Number of processes to spawn to download/upload files\n  --do-download       Flag to automatically download downloaded files\n  --do-upload         Flag to automatically upload downloaded files\n  --do-publish        Flag to automatically publish upload\n  --cleanup           Flag to clean up downloaded files\n  --help              Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client-upload","title":"nomad client upload","text":"<p>Upload files to nomad. The given path can be a single file or a directory. For file(s) that are not .zip or .tar files, a temporary .zip file will be created and uploaded.</p> <p>Usage:</p> <pre><code>nomad client upload [OPTIONS] PATH...\n</code></pre> <p>Options:</p> <pre><code>  --upload-name TEXT    Optional name for the upload of a single file. Will be\n                        ignored on directories.\n  --local-path          Upload files \"offline\": files will not be uploaded,\n                        but processed were they are. Only works when run on\n                        the nomad host.\n  --publish             Automatically move upload out of the staging area\n                        after successful processing\n  --ignore-path-prefix  Ignores common path prefixes when creating an upload.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev","title":"nomad dev","text":"<p>Commands related to the nomad source code.</p> <p>Usage:</p> <pre><code>nomad dev [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>example-data: Adds a few pieces of data to NOMAD.</li> <li>gui-artifacts: Generates all python-based GUI artifacts into javascript code.</li> <li>gui-config: Generates the GUI development config JS file based on NOMAD config.</li> <li>gui-env: Generates the GUI development .env file based on NOMAD config.</li> <li>gui-qa: Runs tests and linting of the nomad gui source code. Useful before committing code.</li> <li>metainfo: Generates a JSON with all metainfo.</li> <li>parser-metadata: Generates a JSON file that compiles all the parser metadata from each parser project.</li> <li>qa: Runs tests and linting of the nomad python source code. Useful before committing code.</li> <li>search-quantities: Generates a JSON with all search quantities.</li> <li>update-parser-readmes: Updates parser<code>s README files by combining a general template with  a parser</code>s metadata YAML file.</li> </ul>"},{"location":"reference/cli.html#nomad-dev-example-data","title":"nomad dev example-data","text":"<p>Adds a few pieces of data to NOMAD.</p> <p>Usage:</p> <pre><code>nomad dev example-data [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -u, --username TEXT  The main author username.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-gui-artifacts","title":"nomad dev gui-artifacts","text":"<p>Generates all python-based GUI artifacts into javascript code.</p> <p>Usage:</p> <pre><code>nomad dev gui-artifacts [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-gui-config","title":"nomad dev gui-config","text":"<p>Generates the GUI development config JS file based on NOMAD config.</p> <p>Usage:</p> <pre><code>nomad dev gui-config [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-gui-env","title":"nomad dev gui-env","text":"<p>Generates the GUI development .env file based on NOMAD config.</p> <p>Usage:</p> <pre><code>nomad dev gui-env [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-gui-qa","title":"nomad dev gui-qa","text":"<p>Runs tests and linting of the nomad gui source code. Useful before committing code.</p> <p>Usage:</p> <pre><code>nomad dev gui-qa [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --skip-tests  Do no tests, just do code checks.\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-metainfo","title":"nomad dev metainfo","text":"<p>Generates a JSON with all metainfo.</p> <p>Usage:</p> <pre><code>nomad dev metainfo [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-parser-metadata","title":"nomad dev parser-metadata","text":"<p>Generates a JSON file that compiles all the parser metadata from each parser project.</p> <p>Usage:</p> <pre><code>nomad dev parser-metadata [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-qa","title":"nomad dev qa","text":"<p>Runs tests and linting of the nomad python source code. Useful before committing code.</p> <p>Usage:</p> <pre><code>nomad dev qa [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --skip-tests     Do no tests, just do code checks.\n  -x, --exitfirst  Stop testing after first failed test case.\n  --help           Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-search-quantities","title":"nomad dev search-quantities","text":"<p>Generates a JSON with all search quantities.</p> <p>Usage:</p> <pre><code>nomad dev search-quantities [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-update-parser-readmes","title":"nomad dev update-parser-readmes","text":"<p>Updates parser<code>s README files by combining a general template with  a parser</code>s metadata YAML file.</p> <p>Usage:</p> <pre><code>nomad dev update-parser-readmes [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --parser TEXT  Only updated the README of the given parsers subdirctory.\n  --help         Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-parse","title":"nomad parse","text":"<p>Run parsing and normalizing locally.</p> <p>Usage:</p> <pre><code>nomad parse [OPTIONS] MAINFILE\n</code></pre> <p>Options:</p> <pre><code>  --show-archive             Print the archive data.\n  --archive-with-meta        If the archvie is printed, it would include\n                             metadata like m_def and m_annotations.\n  --show-metadata            Print the extracted repo metadata.\n  --preview-plots            Preview generated plots\n  --skip-normalizers         Do not run the normalizer.\n  --not-strict               Do also match artificial parsers.\n  --parser TEXT              Skip matching and use the provided parser\n                             (format: `parsers/&lt;name&gt;`). Additional selection\n                             rules still apply for parsers with multiple main\n                             files.\n  --server-context           Whether to use server context.\n  --username TEXT            Username for authentication.\n  --password TEXT            Password for authentication.\n  --save-plot-dir DIRECTORY  Directory to save the plot\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"reference/code_guidelines.html","title":"Code guidelines","text":"<p>NOMAD has a long history and many people are involved in its development. These guidelines are set out to keep the code quality high and consistent. Please read them carefully.</p>"},{"location":"reference/code_guidelines.html#principles-and-rules","title":"Principles and rules","text":"<ul> <li> <p>simple first, complicated only when necessary</p> </li> <li> <p>search and adopt generic established 3rd-party solutions before implementing specific   solutions</p> </li> <li> <p>only unidirectional dependencies between components/modules, no circles</p> </li> <li> <p>only one language: Python (except GUI of course)</p> </li> </ul> <p>The are some rules or better strong guidelines for writing code. The following applies to all Python code (and where applicable, also to Javascript and other code):</p> <ul> <li> <p>Use an IDE (e.g. VS Code) or otherwise automatically   enforce   code formatting and linting.</p> </li> <li> <p>Use <code>nomad qa</code> before committing. This will run all tests, static type checks, linting,   etc.</p> </li> <li> <p>Test the public interface of each submodule (i.e. Python file).</p> </li> <li> <p>There is a style guide to Python. Write   PEP 8-compliant Python code. An exception   is the line cap at 79, which can be broken but keep it 90-ish.</p> </li> <li> <p>Be Pythonic and watch   this talk about best practices.</p> </li> <li> <p>Add docstrings to the public interface of each submodule (i.e. Python file). This   includes APIs that are exposed to other submodules (i.e. other Python files).</p> </li> <li> <p>The project structure follows   this guide. Keep it!</p> </li> <li> <p>Write tests for all contributions.</p> </li> <li> <p>Adopt Clean Code practices. Here is a good   introductory talk to Clean Code.</p> </li> </ul>"},{"location":"reference/code_guidelines.html#enforcing-rules-with-cicd","title":"Enforcing rules with CI/CD","text":"<p>These guidelines are partially enforced by CI/CD. As part of CI all tests are run on all branches; further we run a linter, PEP 8 checker, and mypy (static type checker). You can run <code>nomad qa</code> to run all these tests and checks before committing.</p> <p>See the contributing guide for more details on how to work with issues, branches, merge requests, and CI/CD.</p>"},{"location":"reference/code_guidelines.html#documenting-code","title":"Documenting code","text":"<p>Write Clean Code that is easy to comprehend.</p> <p>However, you should document the whole publicly exposed interface of a module. For Python this includes most classes and functions that you will write, for React its exported components and their props.</p> <p>For all functionality that is exposed to clients (APIs, CLI, schema base classes and annotations, UI functionality), you must consider to add explanations, tutorials, and examples to the documentation system (i.e. the <code>docs</code> folder). This is built with mkdocs and published as part of each NOMAD installation. Also mind <code>nomad/mkdocs.py</code> and <code>mkdocs.yaml</code> and have a look at used plugins and extra functions, e.g. this includes generation of Markdown from <code>examples</code> or Pydantic models.</p> <p>To document Python functions and classes, use Google docstrings. Use Markdown if you need to add markup but try to reduce this to a minimum. You can use VS Code plugins like autoDocstring to help. Always use single quotes, pad single-line docstrings with spaces and start multi-line ones on a new line. Here are a few examples:</p> <pre><code>def generate_uuid() -&gt; str:\n    '''Generates a base64 encoded Version 4 unique identifier. '''\n\n    return base64.encode(uuid4())\n\ndef add(a: float, b: float) -&gt; float:\n    '''\n    Adds two numbers.\n\n    Args:\n      a (float): One number.\n      b (float): The other number.\n\n    Returns:\n      float: The sum of a and b.\n    '''\n\n    return a + b\n</code></pre> <p>The only reason to comment individual lines is because there is absolutely no way to write it simple enough. The typical scenarios are:</p> <ul> <li> <p>workarounds to known issues with used dependencies</p> </li> <li> <p>complex interactions between seemingly unrelated pieces of code that cannot be resolved   otherwise</p> </li> <li> <p>code that has to be cumbersome due to performance optimizations</p> </li> </ul> <p>Do not comment out code. We have Git for that.</p>"},{"location":"reference/code_guidelines.html#names-and-identifiers","title":"Names and identifiers","text":"<p>There is a certain terminology consistently used in this documentation and the source code. Use this terminology for identifiers.</p> <p>Do not use abbreviations. There are (few) exceptions: <code>proc</code> (processing), <code>exc</code> or <code>e</code> (exception), <code>calc</code> (calculation), <code>repo</code> (repository), <code>utils</code> (utilities), and <code>aux</code> (auxiliary). Other exceptions are <code>f</code> for file-like streams and <code>i</code> for index running variables, although the latter is almost never necessary in Python.</p> <p>Terms:</p> <ul> <li> <p>upload: A logical unit that comprises a collection of files uploaded by a user,   organized in a directory structure.</p> </li> <li> <p>entry: An archive item, created by parsing a mainfile. Each entry belongs to an   upload and is associated with various metadata (an upload may have many entries).</p> </li> <li> <p>child entry: Some parsers generate multiple entries -- a main entry plus some number   of child entries. Child entries are identified by the mainfile plus a mainfile_key   (string value).</p> </li> <li> <p>calculation: denotes the results of either a theoretical computation created by CMS   code, or an experiment.</p> </li> <li> <p>raw file: A user uploaded file, located somewhere in the upload's directory structure.</p> </li> <li> <p>mainfile: A raw file identified as parsable, defining an entry of the upload in   question.</p> </li> <li> <p>aux file: Additional files within an upload.</p> </li> <li> <p>entry metadata: Some quantities of an entry that are searchable in NOMAD.</p> </li> <li> <p>archive data: The normalized data of an entry in NOMAD's Metainfo-based format.</p> </li> </ul> <p>Throughout NOMAD, we use different ids. If something is called id, it is usually a random uuid and has no semantic connection to the entity it identifies. If something is called a hash then it is a hash generated based on the entity it identifies. This means either the whole thing or just some properties of this entities.</p> <ul> <li> <p>The most common hash is the <code>entry_hash</code> based on <code>mainfile</code> and aux file contents.</p> </li> <li> <p>The <code>upload_id</code> is a UUID assigned to the upload on creation. It never changes.</p> </li> <li> <p>The <code>mainfile</code> is a path within an upload that points to a file identified as parsable.   This also uniquely identifies an entry within the upload.</p> </li> <li> <p>The <code>entry_id</code> (previously called <code>calc_id</code>) uniquely identifies an entry. It is a hash   over the <code>mainfile</code> and respective <code>upload_id</code>. NOTE: For backward compatibility,   <code>calc_id</code> is also still supported in the API, but using it is strongly discouraged.</p> </li> <li> <p>We often use pairs of <code>upload_id/entry_id</code>, which in many contexts allow to resolve an   entry-related file on the filesystem without having to ask a database about it.</p> </li> <li> <p>The <code>pid</code> or (<code>coe_calc_id</code>) is a legacy sequential integer id, previously used to   identify entries. We still store the <code>pid</code> on these older entries for historical   purposes.</p> </li> <li> <p>Calculation <code>handle</code> or <code>handle_id</code> are created based on those <code>pid</code>.   To create hashes we use func:<code>nomad.utils.hash</code>.</p> </li> </ul>"},{"location":"reference/code_guidelines.html#logging","title":"Logging","text":"<p>There are three important prerequisites to understand about nomad-FAIRDI's logging:</p> <ul> <li> <p>All log entries are recorded in a central Elasticsearch database. To make this database   useful, log entries must be sensible in size, frequency, meaning, level, and logger   name. Therefore, we need to follow some rules when it comes to logging.</p> </li> <li> <p>We use a structured logging approach. Instead of encoding all kinds of information   in log messages, we use key-value pairs that provide context to a log event. In the   end, all entries are stored as JSON dictionaries with <code>@timestamp</code>, <code>level</code>,   <code>logger_name</code>, <code>event</code> plus custom context data. Keep events very short, most   information goes into the context.</p> </li> <li> <p>We use logging to inform about the state of nomad-FAIRDI, not about user behavior,   input, or data. Do not confuse this when determining the log level for an event.   For example, a user providing an invalid upload file should never be an error.</p> </li> </ul> <p>Please follow the following rules when logging:</p> <ul> <li> <p>If a logger is not already provided, only use func:<code>nomad.utils.get_logger</code> to   acquire a new logger. Never use the built-in logging directly. These loggers work like   the system loggers, but allow you to pass keyword arguments with additional context   data. See also the structlog docs.</p> </li> <li> <p>In many context, a logger is already provided (e.g. API, processing, parser,   normalizer). This provided logger has already context information bounded. So it is   important to use those instead of acquiring your own loggers. Have a look for methods   called <code>get_logger</code> or attributes called <code>logger</code>.</p> </li> <li> <p>Keep events (what usually is called message) very short. Examples are:   file uploaded, extraction failed, etc.</p> </li> <li> <p>Structure the keys for context information. When you analyze logs in ELK, you will   see that the set of all keys over all log entries can be quite large. Structure your   keys to make navigation easier. Use keys like <code>nomad.proc.parser_version</code> instead of   <code>parser_version</code>. Use module names as prefixes.</p> </li> <li> <p>Don't log everything. Try to anticipate how you would use the logs in case of bugs,   error scenarios, etc.</p> </li> <li> <p>Don't log sensitive data.</p> </li> <li> <p>Think before logging data (especially dicts, list, NumPy arrays, etc.).</p> </li> <li> <p>Logs should not be abused as a printf-style debugging tool.</p> </li> </ul> <p>The following keys are used in the final logs that are piped to Logstash. Notice that the key name is automatically formed by a separate formatter and may differ from the one used in the actual log call.</p> <p>Keys that are autogenerated for all logs:</p> <ul> <li><code>@timestamp</code>: Timestamp for the log</li> <li><code>@version</code>: Version of the logger</li> <li><code>host</code>: Host name from which the log originated</li> <li><code>path</code>: Path of the module from which the log was created</li> <li><code>tags</code>: Tags for this log</li> <li><code>type</code>: message_type as set in the LogstashFormatter</li> <li><code>level</code>: Log level: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code></li> <li><code>logger_name</code>: Name of the logger</li> <li><code>nomad.service</code>: Service name as configured in <code>config.py</code></li> <li><code>nomad.release</code>: Release name as configured in <code>config.py</code></li> </ul> <p>Keys that are present for events related to processing an entry:</p> <ul> <li><code>nomad.upload_id</code>: id of the currently processed upload</li> <li><code>nomad.entry_id</code>: id of the currently processed entry</li> <li><code>nomad.mainfile</code>: mainfile of the currently processed entry</li> </ul> <p>Keys that are present for events related to exceptions:</p> <ul> <li> <p><code>exc_info</code>: Stores the full Python exception that was encountered. All uncaught   exceptions will be stored automatically here.</p> </li> <li> <p><code>digest</code>: If an exception was raised, the last 256 characters of the message are stored   automatically into this key. If you wish to search for exceptions in   Kibana, you will want to use this value as it will   be indexed unlike the full exception object.</p> </li> </ul>"},{"location":"reference/code_guidelines.html#copyright-notices","title":"Copyright notices","text":"<p>We follow this recommendation of the Linux Foundation for the copyright notice that is placed on top of each source code file.</p> <p>It is intended to provide a broad generic statement that allows all authors/contributors of the NOMAD project to claim their copyright, independent of their organization or individual ownership.</p> <p>You can simply copy the notice from another file. From time to time we can use a tool like licenseheaders to ensure correct notices. In addition we keep a purely informative AUTHORS file.</p>"},{"location":"reference/code_guidelines.html#git-submodules-and-other-in-house-dependencies","title":"Git submodules and other \"in-house\" dependencies","text":"<p>As the NOMAD ecosystem grows, you might develop libraries that are used by NOMAD instead of being part of its main codebase. The same guidelines should apply. You can use GitHub Actions if your library is hosted on Github to ensure automated linting and tests.</p>"},{"location":"reference/config.html","title":"Configuration","text":""},{"location":"reference/config.html#introduction","title":"Introduction","text":"<p>Many aspects of NOMAD and its operation can be modified through configuration. Most configuration items have reasonable defaults and typically only a small subset has to be overwritten.</p> <p>Configuration items get their value in the following order:</p> <ol> <li>The item is read from the environment. This has the highest priority and will overwrite values in a <code>nomad.yaml</code> file or the NOMAD source-code.</li> <li>The value is given in a <code>nomad.yaml</code> configuration file.</li> <li>There is no custom value, and the value hard-coded in the NOMAD sources will be used.</li> </ol> <p>Configuration items are structured. The configuration is hierarchical and items are aggregated in potentially nested section. For example the configuration item <code>services.api_host</code> denotes the attribute <code>api_host</code> in the configuration section <code>services</code>.</p>"},{"location":"reference/config.html#setting-values-from-the-environment","title":"Setting values from the environment","text":"<p>NOMAD services will look at the environment. All environment variables starting with <code>NOMAD_</code> are considered. The rest of the name is interpreted as a configuration item. Sections and attributes are concatenated with a <code>_</code>. For example, the environment variable <code>NOMAD_SERVICES_API_HOST</code> will set the value for the <code>api_host</code> attribute in the <code>services</code> section.</p>"},{"location":"reference/config.html#setting-values-from-a-nomadyaml","title":"Setting values from a <code>nomad.yaml</code>","text":"<p>NOMAD services will look for a <code>nomad.yaml</code> file. By default, they will look in the current working directory. This location can be overwritten with the <code>NOMAD_CONFIG</code> environment variable.</p> <p>The configuration sections and attributes can be denoted with YAML objects and attributes. Here is an example <code>nomad.yaml</code> file: <pre><code>services:\n  api_host: 'localhost'\n  api_base_path: '/nomad-oasis'\n\noasis:\n  is_oasis: true\n  uses_central_user_management: true\n\nnorth:\n  jupyterhub_crypt_key: '978bfb2e13a8448a253c629d8dd84ff89587f30e635b753153960930cad9d36d'\n\nmeta:\n  deployment: 'oasis'\n  deployment_url: 'https://my-oasis.org/api'\n  maintainer_email: 'me@my-oasis.org'\n\nlogtransfer:\n  enabled: false\n\nmongo:\n    db_name: nomad_oasis_v1\n\nelastic:\n    entries_index: nomad_oasis_entries_v1\n    materials_index: nomad_oasis_materials_v1\n</code></pre></p> <p>When overwriting an object in the configuration, the new value will be merged with the default value. The new merged object will have all of the attributes of the new object in addition to any old attributes that were not overwritten. This allows you to simply change an individual setting without having to provide the entire structure again, which simplifies customization that happens deep in the configuration hierarchy. When overwriting anything else (numbers, strings, lists etc.) the new value completely replaces the old one.</p>"},{"location":"reference/config.html#user-interface-customization","title":"User interface customization","text":"<p>Many of the UI options use a data model that contains the following three fields: <code>include</code>, <code>exclude</code> and <code>options</code>. This structure allows you to easily disable, enable, reorder and modify the UI layout with minimal config rewrite. Here are examples of common customization tasks using the search columns as an example:</p> <p>Disable item: <pre><code>ui:\n  apps:\n    options:\n      entries:\n        columns:\n          exclude: ['upload_create_time']\n</code></pre></p> <p>Explicitly select the shown items and their order <pre><code>ui:\n  apps:\n    options:\n      entries:\n        columns:\n          include: ['entry_id', 'upload_create_time']\n</code></pre></p> <p>Modify existing option <pre><code>ui:\n  apps:\n    options:\n      entries:\n        columns:\n          options:\n            upload_create_time:\n              label: \"Uploaded\"\n</code></pre></p> <p>Add a new item that does not yet exist in options. Note that by default all options are shown in the order they have been declared unless the order is explicitly given in <code>include</code>. <pre><code>ui:\n  apps:\n    options:\n      entries:\n        columns:\n          options:\n            upload_id:\n              label: \"Upload ID\"\n</code></pre></p> <p>The following is a reference of all configuration sections and attributes.</p>"},{"location":"reference/config.html#services","title":"Services","text":""},{"location":"reference/config.html#services_1","title":"services","text":"<p>Contains basic configuration of the NOMAD services (app, worker, north).</p> name type console_log_level <code>Union[int, str]</code> The log level that controls console logging for all NOMAD services (app, worker, north). The level is given in Python <code>logging</code> log level numbers.default: <code>30</code> api_host <code>str</code> The external hostname that clients can use to reach this NOMAD installation.default: <code>localhost</code> api_port <code>int</code> The port used to expose the NOMAD app and api to clients.default: <code>8000</code> api_base_path <code>str</code> The base path prefix for the NOMAD app and api.default: <code>/fairdi/nomad/latest</code> api_secret <code>str</code> A secret that is used to issue download and other tokens.default: <code>defaultApiSecret</code> api_timeout <code>int</code> If the NOMAD app is run with gunicorn as process manager, this timeout (in s) is passed and worker processes will be restarted, if they do not respond in time.default: <code>600</code> https <code>int</code> Set to <code>True</code>, if external clients are using SSL to connect to this installation. Requires to setup a reverse-proxy (e.g. the one used in the docker-compose based installation) that handles the SSL encryption.default: <code>False</code> https_upload <code>int</code> Set to <code>True</code>, if upload curl commands should suggest the use of SSL for file uploads. This can be configured independently of <code>https</code> to suggest large file via regular HTTP.default: <code>False</code> admin_user_id <code>str</code> The admin user <code>user_id</code>. All users are treated the same; there are no particular authorization information attached to user accounts. However, the API will grant the user with the given <code>user_id</code> more rights, e.g. using the <code>admin</code> owner setting in accessing data.default: <code>00000000-0000-0000-0000-000000000000</code> encyclopedia_base <code>str</code> This enables links to the given encyclopedia installation in the UI.default: <code>https://nomad-lab.eu/prod/rae/encyclopedia/#</code> optimade_enabled <code>int</code> If true, the app will serve the optimade API.default: <code>True</code> dcat_enabled <code>int</code> If true the app will serve the DCAT API.default: <code>True</code> h5grove_enabled <code>int</code> If true the app will serve the h5grove API.default: <code>True</code> upload_limit <code>int</code> The maximum allowed unpublished uploads per user. If a user exceeds this amount, the user cannot add more uploads.default: <code>10</code> force_raw_file_decoding <code>int</code> By default, text raw-files are interpreted with utf-8 encoding. If this fails, the actual encoding is guessed. With this setting, we force to assume iso-8859-1 encoding, if a file is not decodable with utf-8.default: <code>False</code> max_entry_download <code>int</code> There is an inherent limit in page-based pagination with Elasticsearch. If you increased this limit with your Elasticsearch, you can also adopt this setting accordingly, changing the maximum amount of entries that can be paginated with page-base pagination.Page-after-value-based pagination is independent and can be used without limitations.default: <code>50000</code> unavailable_value <code>str</code> Value that is used in <code>results</code> section Enum fields (e.g. system type, spacegroup, etc.) to indicate that the value could not be determined.default: <code>unavailable</code> app_token_max_expires_in <code>int</code> Maximum expiration time for an app token in seconds. Requests with a higher value will be declined.default: <code>2592000</code> html_resource_http_max_age <code>int</code> Used for the max_age cache-control directive on statically served html, js, css resources.default: <code>60</code> image_resource_http_max_age <code>int</code> Used for the max_age cache-control directive on statically served image resources.default: <code>2592000</code>"},{"location":"reference/config.html#meta","title":"meta","text":"<p>Metadata about the deployment and how it is presented to clients.</p> name type deployment_url <code>str</code> The NOMAD deployment's url. If not explicitly set, will default to the (api url) read from the configuration. label <code>str</code> An additional log-stash data key-value pair added to all logs. Can be used to differentiate deployments when analyzing logs. beta <code>dict</code> Additional data that describes how the deployment is labeled as a beta-version in the UI.default: Complex object, default value not displayed. version <code>str</code> The NOMAD version string.default: <code>0.0</code> commit <code>str</code> The source-code commit that this installation's NOMAD version is build from.default: \"\" deployment <code>str</code> Human-friendly name of this nomad deployment.default: <code>devel</code> service <code>str</code> Name for the service that is added to all logs. Depending on how NOMAD is installed, services get a name (app, worker, north) automatically.default: <code>unknown nomad service</code> name <code>str</code> Web-site title for the NOMAD UI.default: <code>NOMAD</code>deprecated homepage <code>str</code> Provider homepage.default: <code>https://nomad-lab.eu</code>deprecated source_url <code>str</code> URL of the NOMAD source-code repository.default: <code>https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-FAIR</code>deprecated maintainer_email <code>str</code> Email of the NOMAD deployment maintainer.default: <code>markus.scheidgen@physik.hu-berlin.de</code>"},{"location":"reference/config.html#oasis","title":"oasis","text":"<p>Settings related to the configuration of a NOMAD Oasis deployment.</p> name type allowed_users <code>List[str]</code> A list of usernames or user account emails. These represent a white-list of allowed users. With this, users will need to login right-away and only the listed users might use this deployment. All API requests must have authentication information as well. is_oasis <code>int</code> Set to <code>True</code> to indicate that this deployment is a NOMAD Oasis.default: <code>False</code> uses_central_user_management <code>int</code> Set to True to use the central user-management. Typically the NOMAD backend is using the configured <code>keycloak</code> to access user data. With this, the backend will use the API of the central NOMAD (<code>central_nomad_deployment_url</code>) instead.default: <code>False</code> central_nomad_deployment_url <code>str</code> The URL of the API of the NOMAD deployment that is considered the central NOMAD.default: <code>https://nomad-lab.eu/prod/v1/api</code>"},{"location":"reference/config.html#north","title":"north","text":"<p>Settings related to the operation of the NOMAD remote tools hub service north.</p> name type enabled <code>int</code> Enables or disables the NORTH API and UI views. This is independent of whether you run a jupyter hub or not.default: <code>True</code> hub_connect_ip <code>str</code> Overwrites the default hostname that can be used from within a north container to reach the host system.Typically has to be set for non Linux hosts. Set this to <code>host.docker.internal</code> on windows/macos. hub_connect_url <code>str</code> This setting is forwarded to jupyterhub; refer to the jupyterhub documentation. docker_network <code>str</code> This setting is forwarded to jupyterhub; refer to the jupyterhub documentation. jupyterhub_crypt_key <code>str</code> This setting is forwarded to jupyterhub; refer to the jupyterhub documentation. nomad_host <code>str</code> The NOMAD app host name that spawned containers use. nomad_access_token_expiry_time <code>int</code> All tools are run with an access token for the NOMAD api in the NOMAD_CLIENT_ACCESS_TOKEN environment variable. This token will be automatically used by the nomad-lab Python package, e.g. if you use the ArchiveQuery to access data. This option sets the amount of seconds that this token is valid for.default: <code>86400</code> tools <code>NORTHTools</code> The available north tools. Either the tools definitions as dict or a path to a .json file.default: Complex object, default value not displayed. hub_service_api_token <code>str</code> A secret token shared between NOMAD and the NORTH jupyterhub. This needs to be the token of an admin service.default: <code>secret-token</code> hub_ip <code>str</code> This setting is forwarded to jupyterhub; refer to the jupyterhub documentation.default: <code>0.0.0.0</code> hub_host <code>str</code> The internal host name that NOMAD services use to connect to the jupyterhub API.default: <code>localhost</code> hub_port <code>int</code> The internal port that NOMAD services use to connect to the jupyterhub API.default: <code>9000</code> windows <code>int</code> Enable windows OS hacks.default: <code>True</code>"},{"location":"reference/config.html#northtools","title":"NORTHTools","text":"name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, NORTHTool]</code> The available plugin.default: Complex object, default value not displayed."},{"location":"reference/config.html#northtool","title":"NORTHTool","text":"name type short_description <code>str</code> A short description of the tool, e.g. shown in the NOMAD GUI. description <code>str</code> A description of the tool, e.g. shown in the NOMAD GUI. image <code>str</code> The docker image (incl. tags) to use for the tool. cmd <code>str</code> The container cmd that is passed to the spawner. image_pull_policy <code>str</code> The image pull policy used in k8s deployments.default: <code>Always</code> privileged <code>int</code> Whether the tool needs to run in privileged mode.default: <code>False</code> default_url <code>str</code> An optional path prefix that is added to the container URL to reach the tool, e.g. \"/lab\" for jupyterlab. path_prefix <code>str</code> An optional path prefix that is added to the container URL to reach the files, e.g. \"lab/tree\" for jupyterlab. with_path <code>int</code> Whether the tool supports a path to a file or directory. This also enables tools to be launched from files in the NOMAD UI.default: <code>False</code> file_extensions <code>List[str]</code> The file extensions of files that this tool should be launchable for.default: <code>[]</code> mount_path <code>str</code> The path in the container where uploads and work directories will be mounted, e.g. /home/jovyan for Jupyter containers. icon <code>str</code> A URL to an icon that is used to represent the tool in the NOMAD UI. maintainer <code>List[NORTHToolMaintainer]</code> The maintainers of the tool.default: <code>[]</code> external_mounts <code>List[NORTHExternalMount]</code> Additional mounts to be added to tool containers.default: <code>[]</code>"},{"location":"reference/config.html#northtoolmaintainer","title":"NORTHToolMaintainer","text":"name type name <code>str</code> email <code>str</code>"},{"location":"reference/config.html#northexternalmount","title":"NORTHExternalMount","text":"name type host_path <code>str</code> bind <code>str</code> mode <code>str</code> default: <code>ReadMode.ro</code>options: - <code>ro</code> - <code>rw</code>"},{"location":"reference/config.html#files-databases-external-services","title":"Files, databases, external services","text":""},{"location":"reference/config.html#rabbitmq","title":"rabbitmq","text":"<p>Configures how NOMAD is connecting to RabbitMQ.</p> name type host <code>str</code> The name of the host that runs RabbitMQ.default: <code>localhost</code> user <code>str</code> The RabbitMQ user that is used to connect.default: <code>rabbitmq</code> password <code>str</code> The password that is used to connect.default: <code>rabbitmq</code>"},{"location":"reference/config.html#fs","title":"fs","text":"name type staging_external <code>str</code> public_external <code>str</code> north_home_external <code>str</code> archive_version_suffix <code>Union[str, List[str]]</code> This allows to add an additional segment to the names of archive files and thereby allows different NOMAD installations to work with the same storage directories and raw files, but with separate archives.If this is a list, the first string is used. If the file with the first string does not exist on read, the system will look for the file with the next string, etc.default: <code>['v1.2', 'v1']</code> external_working_directory <code>str</code> tmp <code>str</code> default: <code>.volumes/fs/tmp</code> staging <code>str</code> default: <code>.volumes/fs/staging</code> public <code>str</code> default: <code>.volumes/fs/public</code> north_home <code>str</code> default: <code>.volumes/fs/north/users</code> local_tmp <code>str</code> default: <code>/tmp</code> prefix_size <code>int</code> default: <code>2</code> working_directory <code>str</code> default: <code>/app</code>"},{"location":"reference/config.html#elastic","title":"elastic","text":"name type username <code>str</code> password <code>str</code> host <code>str</code> default: <code>localhost</code> port <code>int</code> default: <code>9200</code> timeout <code>int</code> default: <code>60</code> bulk_timeout <code>int</code> default: <code>600</code> bulk_size <code>int</code> default: <code>1000</code> entries_per_material_cap <code>int</code> default: <code>1000</code> entries_index <code>str</code> default: <code>nomad_entries_v1</code> materials_index <code>str</code> default: <code>nomad_materials_v1</code>"},{"location":"reference/config.html#keycloak","title":"keycloak","text":"name type public_server_url <code>str</code> client_secret <code>str</code> server_url <code>str</code> default: <code>https://nomad-lab.eu/fairdi/keycloak/auth/</code> realm_name <code>str</code> default: <code>fairdi_nomad_prod</code> username <code>str</code> default: <code>admin</code> password <code>str</code> default: <code>password</code> client_id <code>str</code> default: <code>nomad_public</code>"},{"location":"reference/config.html#mongo","title":"mongo","text":"<p>Connection and usage settings for MongoDB.</p> name type host <code>str</code> The name of the host that runs mongodb.default: <code>localhost</code> port <code>int</code> The port to connect with mongodb.default: <code>27017</code> db_name <code>str</code> The used mongodb database name.default: <code>nomad_v1</code> username <code>str</code> password <code>str</code>"},{"location":"reference/config.html#logstash","title":"logstash","text":"name type level <code>Union[int, str]</code> default: <code>10</code> enabled <code>int</code> default: <code>False</code> host <code>str</code> default: <code>localhost</code> tcp_port <code>str</code> default: <code>5000</code>"},{"location":"reference/config.html#mail","title":"mail","text":"name type cc_address <code>str</code> enabled <code>int</code> default: <code>False</code> with_login <code>int</code> default: <code>False</code> host <code>str</code> default: \"\" port <code>int</code> default: <code>8995</code> user <code>str</code> default: \"\" password <code>str</code> default: \"\" from_address <code>str</code> default: <code>support@nomad-lab.eu</code>"},{"location":"reference/config.html#datacite","title":"datacite","text":"name type mds_host <code>str</code> default: <code>https://mds.datacite.org</code> enabled <code>int</code> default: <code>False</code> prefix <code>str</code> default: <code>10.17172</code> user <code>str</code> default: <code>*</code> password <code>str</code> default: <code>*</code>"},{"location":"reference/config.html#rfc3161_timestamp","title":"rfc3161_timestamp","text":"name type cert <code>str</code> Path to the optional rfc3161ng timestamping server certificate. username <code>str</code> password <code>str</code> server <code>str</code> The rfc3161ng timestamping host.default: <code>http://zeitstempel.dfn.de</code> hash_algorithm <code>str</code> Hash algorithm used by the rfc3161ng timestamping server.default: <code>sha256</code>"},{"location":"reference/config.html#processing","title":"Processing","text":""},{"location":"reference/config.html#celery","title":"celery","text":"name type max_memory <code>float</code> default: <code>64000000.0</code> timeout <code>int</code> default: <code>1800</code> acks_late <code>int</code> default: <code>False</code> routing <code>str</code> default: <code>queue</code> priorities <code>dict</code> default: Complex object, default value not displayed."},{"location":"reference/config.html#normalize","title":"normalize","text":"name type normalizers <code>Options</code> default: Complex object, default value not displayed. springer_db_path <code>str</code> default: <code>/app/nomad/config/models/normalizing/data/springer.msg</code> system_classification_with_clusters_threshold <code>int</code> The system size limit for running the dimensionality analysis. For very large systems the dimensionality analysis will get too expensive.default: <code>64</code> clustering_size_limit <code>int</code> The system size limit for running the system clustering. For very large systems the clustering will get too expensive.default: <code>600</code> symmetry_tolerance <code>float</code> Symmetry tolerance controls the precision used by spglib in order to find symmetries. The atoms are allowed to move this much from their symmetry positions in order for spglib to still detect symmetries. The unit is angstroms. The value of 0.1 is used e.g. by Materials Project according to https://pymatgen.org/pymatgen.symmetry.html#pymatgen.symmetry.analyzer.SpacegroupAnalyzerdefault: <code>0.1</code> prototype_symmetry_tolerance <code>float</code> The symmetry tolerance used in aflow prototype matching. Should only be changed before re-running the prototype detection.default: <code>0.1</code> max_2d_single_cell_size <code>int</code> Maximum number of atoms in the single cell of a 2D material for it to be considered valid.default: <code>7</code> cluster_threshold <code>float</code> The distance tolerance between atoms for grouping them into the same cluster. Used in detecting system type.default: <code>2.5</code> angle_rounding <code>float</code> Defines the \"bin size\" for rounding cell angles for the material hash in degree.default: <code>10.0</code> flat_dim_threshold <code>float</code> The threshold for a system to be considered \"flat\". Used e.g. when determining if a 2D structure is purely 2-dimensional to allow extra rigid transformations that are improper in 3D but proper in 2D.default: <code>0.1</code> k_space_precision <code>float</code> The threshold for point equality in k-space. Unit: 1/m.default: <code>150000000.0</code> band_structure_energy_tolerance <code>float</code> The energy threshold for how much a band can be on top or below the fermi level in order to still detect a gap. Unit: Joule.default: <code>8.01088e-21</code>"},{"location":"reference/config.html#options","title":"Options","text":"<p>Common configuration class used for enabling/disabling certain elements and defining the configuration of each element.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, Any]</code> Contains the available options.default: Complex object, default value not displayed."},{"location":"reference/config.html#process","title":"process","text":"name type redirect_stdouts <code>int</code> True will redirect lines to stdout (e.g. print output) that occur during processing (e.g. created by parsers or normalizers) as log entries.default: <code>False</code> store_package_definition_in_mongo <code>int</code> Configures whether to store the corresponding package definition in mongodb.default: <code>False</code> add_definition_id_to_reference <code>int</code> Configures whether to attach definition id to <code>m_def</code>, note it is different from <code>m_def_id</code>. The <code>m_def_id</code> will be exported with the <code>with_def_id=True</code> via <code>m_to_dict</code>.default: <code>False</code> write_definition_id_to_archive <code>int</code> Write <code>m_def_id</code> to the archive.default: <code>False</code> index_materials <code>int</code> default: <code>True</code> reuse_parser <code>int</code> default: <code>True</code> metadata_file_name <code>str</code> default: <code>nomad</code> metadata_file_extensions <code>tuple</code> default: <code>('json', 'yaml', 'yml')</code> auxfile_cutoff <code>int</code> default: <code>100</code> parser_matching_size <code>int</code> default: <code>12000</code> max_upload_size <code>int</code> default: <code>34359738368</code> use_empty_parsers <code>int</code> default: <code>False</code> rfc3161_skip_published <code>int</code> default: <code>False</code>"},{"location":"reference/config.html#reprocess","title":"reprocess","text":"<p>Configures standard behaviour when reprocessing. Note, the settings only matter for published uploads and entries. For uploads in staging, we always reparse, add newfound entries, and delete unmatched entries.</p> name type rematch_published <code>int</code> default: <code>True</code> reprocess_existing_entries <code>int</code> default: <code>True</code> use_original_parser <code>int</code> default: <code>False</code> add_matched_entries_to_published <code>int</code> default: <code>True</code> delete_unmatched_published_entries <code>int</code> default: <code>False</code> index_individual_entries <code>int</code> default: <code>False</code>"},{"location":"reference/config.html#bundle_export","title":"bundle_export","text":"<p>Controls behaviour related to exporting bundles.</p> name type default_cli_bundle_export_path <code>str</code> Default path used when exporting bundles using the CLI command.default: <code>./bundles</code> default_settings <code>BundleExportSettings</code> General default settings.default: Complex object, default value not displayed. default_settings_cli <code>BundleExportSettings</code> Additional default settings, applied when exporting using the CLI. This allows to override some of the settings specified in the general default settings above."},{"location":"reference/config.html#bundleexportsettings","title":"BundleExportSettings","text":"name type include_raw_files <code>int</code> If the raw files should be included in the exportdefault: <code>True</code> include_archive_files <code>int</code> If the parsed archive files should be included in the exportdefault: <code>True</code> include_datasets <code>int</code> If the datasets should be included in the exportdefault: <code>True</code>"},{"location":"reference/config.html#bundle_import","title":"bundle_import","text":"<p>Controls behaviour related to importing bundles.</p> name type required_nomad_version <code>str</code> Minimum  NOMAD version of bundles required for import.default: <code>1.1.2</code> default_cli_bundle_import_path <code>str</code> Default path used when importing bundles using the CLI command.default: <code>./bundles</code> allow_bundles_from_oasis <code>int</code> If oasis admins can \"push\" bundles to this NOMAD deployment.default: <code>False</code> allow_unpublished_bundles_from_oasis <code>int</code> If oasis admins can \"push\" bundles of unpublished uploads.default: <code>False</code> default_settings <code>BundleImportSettings</code> General default settings.default: Complex object, default value not displayed. default_settings_cli <code>BundleImportSettings</code> Additional default settings, applied when importing using the CLI. This allows to override some of the settings specified in the general default settings above.default: Complex object, default value not displayed."},{"location":"reference/config.html#bundleimportsettings","title":"BundleImportSettings","text":"name type include_raw_files <code>int</code> If the raw files should be included in the importdefault: <code>True</code> include_archive_files <code>int</code> If the parsed archive files should be included in the importdefault: <code>True</code> include_datasets <code>int</code> If the datasets should be included in the importdefault: <code>True</code> include_bundle_info <code>int</code> If the bundle_info.json file should be kept (not necessary but may be nice to have.default: <code>True</code> keep_original_timestamps <code>int</code> If all timestamps (create time, publish time etc) should be imported from the bundle.default: <code>False</code> set_from_oasis <code>int</code> If the from_oasis flag and oasis_deployment_url should be set.default: <code>True</code> delete_upload_on_fail <code>int</code> If False, it is just removed from the ES index on failure.default: <code>False</code> delete_bundle_on_fail <code>int</code> Deletes the source bundle if the import fails.default: <code>True</code> delete_bundle_on_success <code>int</code> Deletes the source bundle if the import succeeds.default: <code>True</code> delete_bundle_include_parent_folder <code>int</code> When deleting the bundle, also include parent folder, if empty.default: <code>True</code> trigger_processing <code>int</code> If the upload should be processed when the import is done (not recommended).default: <code>False</code> process_settings <code>Reprocess</code> When trigger_processing is set to True, these settings control the reprocessing behaviour (see the config for <code>reprocess</code> for more info). NOTE: reprocessing is no longer the recommended method to import bundles.default: Complex object, default value not displayed."},{"location":"reference/config.html#reprocess_1","title":"Reprocess","text":"<p>Configures standard behaviour when reprocessing. Note, the settings only matter for published uploads and entries. For uploads in staging, we always reparse, add newfound entries, and delete unmatched entries.</p> name type rematch_published <code>int</code> default: <code>True</code> reprocess_existing_entries <code>int</code> default: <code>True</code> use_original_parser <code>int</code> default: <code>False</code> add_matched_entries_to_published <code>int</code> default: <code>True</code> delete_unmatched_published_entries <code>int</code> default: <code>False</code> index_individual_entries <code>int</code> default: <code>False</code>"},{"location":"reference/config.html#archive","title":"archive","text":"name type block_size <code>int</code> In case of using blocked TOC, this is the size of each block.default: <code>1048576</code> read_buffer_size <code>int</code> GPFS needs at least 256K to achieve decent performance.default: <code>1048576</code> copy_chunk_size <code>int</code> The chunk size of every read of binary data. It is used to copy data from one file to another. A small value will result in more syscalls, a large value will result in higher peak memory usage.default: <code>16777216</code> toc_depth <code>int</code> Depths of table of contents in the archive.default: <code>10</code> use_new_writer <code>int</code> default: <code>True</code> small_obj_optimization_threshold <code>int</code> For any child of lists/dicts whose encoded size is smaller than this value, no TOC will be generated.default: <code>1048576</code> fast_loading <code>int</code> When enabled, this flag determines whether to read the whole dict/list at once when a certain mount of children has been visited. This reduces the number of syscalls although data may be repeatedly read. Otherwise, always read children one by one. This may slow down the loading as more syscalls are needed.default: <code>True</code> fast_loading_threshold <code>float</code> If the fraction of children that have been visited is less than this threshold, fast loading will be used.default: <code>0.6</code> trivial_size <code>int</code> To identify numerical lists.default: <code>20</code>"},{"location":"reference/config.html#user-interface","title":"User Interface","text":"<p>These settings affect the behaviour of the user interface. Note that the preferred way for creating custom apps is by using app plugin entry points.</p>"},{"location":"reference/config.html#ui","title":"ui","text":"<p>Used to customize the user interface.</p> name type app_base <code>str</code> This is automatically set. north_base <code>str</code> This is automatically set. theme <code>Theme</code> Controls the site theme and identity. unit_systems <code>UnitSystems</code> Controls the available unit systems. entry <code>Entry</code> Controls the entry visualization. apps <code>Apps</code> Contains the App definitions. north <code>NORTHUI</code> NORTH (NOMAD Remote Tools Hub) UI configuration.default: Complex object, default value not displayed. example_uploads <code>ExampleUploads</code> Controls the available example uploads.default: Complex object, default value not displayed."},{"location":"reference/config.html#apps","title":"Apps","text":"<p>Contains App definitions and controls their availability.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, App]</code> Contains the available app options."},{"location":"reference/config.html#theme","title":"Theme","text":"<p>Theme and identity settings.</p> name type title <code>str</code> Site name in the browser tab."},{"location":"reference/config.html#unitsystems","title":"UnitSystems","text":"<p>Controls the available unit systems.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, UnitSystem]</code> Contains the available unit systems. selected <code>str</code> Selected option."},{"location":"reference/config.html#unitsystem","title":"UnitSystem","text":"name type label <code>str</code> Short, descriptive label used for this unit system. units <code>Dict[str, UnitSystemUnit]</code> Contains a mapping from each dimension to a unit. If a unit is not        specified for a dimension, the SI equivalent will be used by default.        The following dimensions are available:         - dimensionless - length - mass - time - current - temperature - luminosity - luminous_flux - substance - angle - information - force - energy - power - pressure - charge - resistance - conductance - inductance - magnetic_flux - magnetic_field - frequency - luminance - illuminance - electric_potential - capacitance - activity"},{"location":"reference/config.html#unitsystemunit","title":"UnitSystemUnit","text":"name type definition <code>str</code> The unit definition. Can be a mathematical expression that combines several units, e.g. <code>(kg * m) / s^2</code>. You should only use units that are registered in the NOMAD unit registry (<code>nomad.units.ureg</code>). locked <code>int</code> Whether the unit is locked in the unit system it is defined in.default: <code>False</code>"},{"location":"reference/config.html#northui","title":"NORTHUI","text":"<p>NORTH (NOMAD Remote Tools Hub) UI configuration.</p> name type enabled <code>int</code> Whether the NORTH tools are available in the UI. The default value is read from the root-level NORTH configuration.default: <code>True</code>"},{"location":"reference/config.html#entry","title":"Entry","text":"<p>Controls the entry visualization.</p> name type cards <code>Cards</code> Controls the cards that are displayed on the entry overview page."},{"location":"reference/config.html#cards","title":"Cards","text":"<p>Contains the overview page card definitions and controls their visibility.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, Card]</code> Contains the available card options."},{"location":"reference/config.html#card","title":"Card","text":"<p>Definition for a card shown in the entry overview page.</p> name type error <code>str</code> The error message to show if an error is encountered within the card."},{"location":"reference/config.html#exampleuploads","title":"ExampleUploads","text":"<p>Controls the availability of example uploads.</p> name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include."},{"location":"reference/config.html#others","title":"Others","text":""},{"location":"reference/config.html#logtransfer","title":"logtransfer","text":"<p>Configuration of logtransfer and statistics service.</p> <p>When enabled (enabled) an additional logger will write logs to a log file (log_file). At regular intervals (transfer_interval) a celery task is scheduled. It will log a set of statistics. It will copy the log file (transfer_log_files). Transfer the contents of the copy to the central NOMAD (oasis.central_nomad_deployment_url) and delete the copy. The transfer is only done if the the log file has a certain size (transfer_threshold). Only a maximum amount of logs are transferred (transfer_capacity). Only logs with a certain level (level) are considered. The files will be stored in fs.tmp.</p> name type enabled <code>int</code> If enabled this starts process that frequently generates logs with statistics.default: <code>False</code> transfer_threshold <code>int</code> The minimum size in bytes of stored logs before logs are transferred. 0 means transfer at every transfer interval.default: <code>0</code> transfer_capacity <code>int</code> The maximum number of bytes of stored logs that are transferred. Excess is dropped.default: <code>1000000</code> transfer_interval <code>int</code> Time interval in seconds after which stored logs are potentially transferred.default: <code>600</code> level <code>Union[int, str]</code> The min log level for logs to be transferred.default: <code>20</code> log_file <code>str</code> The log file that is used to store logs for transfer.default: <code>nomad.log</code> transfer_log_file <code>str</code> The log file that is used to copy logs for transfer.default: <code>.transfer.log</code> file_rollover_wait_time <code>float</code> Time in seconds to wait after log file was \"rolled over\" for transfer.default: <code>1</code>"},{"location":"reference/config.html#tests","title":"tests","text":"name type assume_auth_for_username <code>str</code> Will assume that all API calls with no authentication have authentication for the user with the given username. default_timeout <code>int</code> default: <code>60</code>"},{"location":"reference/config.html#resources","title":"resources","text":"name type enabled <code>int</code> default: <code>False</code> db_name <code>str</code> default: <code>nomad_v1_resources</code> max_time_in_mongo <code>float</code> Maxmimum time a resource is stored in mongodb before being updated.default: <code>31536000.0</code> download_retries <code>int</code> Number of retries when downloading resources.default: <code>2</code> download_retry_delay <code>int</code> Delay between retries in secondsdefault: <code>10</code> max_connections <code>int</code> Maximum simultaneous connections used to download resources.default: <code>10</code>"},{"location":"reference/config.html#client","title":"client","text":"name type user <code>str</code> password <code>str</code> access_token <code>str</code> url <code>str</code> default: <code>http://nomad-lab.eu/prod/v1/api</code>"},{"location":"reference/config.html#gitlab","title":"gitlab","text":"name type private_token <code>str</code> default: <code>not set</code>"},{"location":"reference/config.html#plugins","title":"plugins","text":"name type entry_points <code>EntryPoints</code> Used to control plugin entry points. plugin_packages <code>Dict[str, PluginPackage]</code> Contains the installed installed plugin packages with the package name used as a key. This is autogenerated and should not be modified."},{"location":"reference/config.html#pluginpackage","title":"PluginPackage","text":"name type name <code>str</code> Name of the plugin Python package, read from pyproject.toml. description <code>str</code> Package description, read from pyproject.toml. version <code>str</code> Plugin package version, read from pyproject.toml. homepage <code>str</code> Link to the plugin package homepage, read from pyproject.toml. documentation <code>str</code> Link to the plugin package documentation page, read from pyproject.toml. repository <code>str</code> Link to the plugin package source code repository, read from pyproject.toml. entry_points <code>List[str]</code> List of entry point ids contained in this package, read form pyproject.toml"},{"location":"reference/config.html#entrypoints","title":"EntryPoints","text":"name type include <code>List[str]</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>List[str]</code> List of excluded options. Has higher precedence than include. options <code>Dict[str, Union[Schema, Normalizer, Parser, SchemaPackageEntryPoint, ParserEntryPoint, NormalizerEntryPoint, AppEntryPoint, ExampleUploadEntryPoint]]</code> The available plugin entry points.default: Complex object, default value not displayed."},{"location":"reference/config.html#appentrypoint","title":"AppEntryPoint","text":"<p>Base model for app plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Determines the entry point type.default: <code>app</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> A human readable description of the plugin entry point. plugin_package <code>str</code> The plugin package from which this entry points comes from. app <code>App</code> The app configuration."},{"location":"reference/config.html#schemapackageentrypoint","title":"SchemaPackageEntryPoint","text":"<p>Base model for schema package plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Specifies the entry point type.default: <code>schema_package</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> A human readable description of the plugin entry point. plugin_package <code>str</code> The plugin package from which this entry points comes from."},{"location":"reference/config.html#exampleuploadentrypoint","title":"ExampleUploadEntryPoint","text":"<p>Base model for example upload plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Determines the entry point type.default: <code>example_upload</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> Longer description of the example upload. plugin_package <code>str</code> The plugin package from which this entry points comes from. category <code>str</code> Category for the example upload. title <code>str</code> Title of the example upload. path <code>str</code> Path to the example upload contents folder. Should be a path that starts from the package root folder, e.g. 'example_uploads/my_upload'. url <code>str</code> URL that points to an online file. If you use this instead of 'path', the file will be downloaded once upon app startup. local_path <code>str</code> The final path to use when creating the upload. This field will be automatically generated by the 'load' function and is typically not set manually."},{"location":"reference/config.html#parser","title":"Parser","text":"<p>A Parser describes a NOMAD parser that can be loaded as a plugin.</p> <p>The parser itself is referenced via <code>python_name</code>. For Parser instances <code>python_name</code> must refer to a Python class that has a <code>parse</code> function. The other properties are used to create a <code>MatchingParserInterface</code>. This comprises general metadata that allows users to understand what the parser is, and metadata used to decide if a given file \"matches\" the parser.</p> name type plugin_type <code>str</code> The type of the plugin. This has to be the string <code>parser</code> for parser plugins.default: <code>parser</code> id <code>str</code> The unique identifier for this plugin. name <code>str</code> A short descriptive human readable name for the plugin. description <code>str</code> A human readable description of the plugin. plugin_documentation_url <code>str</code> The URL to the plugins main documentation page. plugin_source_code_url <code>str</code> The URL of the plugins main source code repository. python_package <code>str</code> Name of the python package that contains the plugin code and a plugin metadata file called <code>nomad_plugin.yaml</code>. parser_class_name <code>str</code> The fully qualified name of the Python class that implements the parser. This class must have a function <code>def parse(self, mainfile, archive, logger)</code>. parser_as_interface <code>int</code> By default the parser metadata from this config (and the loaded nomad_plugin.yaml) is used to instantiate a parser interface that is lazy loading the actual parser and performs the mainfile matching. If the parser interface matching based on parser metadata is not sufficient and you implemented your own is_mainfile parser method, this setting can be used to use the given parser class directly for parsing and matching.default: <code>False</code> mainfile_contents_re <code>str</code> A regular expression that is applied the content of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches. mainfile_name_re <code>str</code> A regular expression that is applied the name of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_mime_re <code>str</code> A regular expression that is applied the mime type of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>text/.*</code> mainfile_binary_header <code>bytes</code> Matches a binary file if the given bytes are included in the file. mainfile_binary_header_re <code>bytes</code> Matches a binary file if the given binary regular expression bytes matches the file contents. mainfile_alternative <code>int</code> If True, the parser only matches a file, if no other file in the same directory matches a parser.default: <code>False</code> mainfile_contents_dict <code>dict</code> Is used to match structured data files like JSON or HDF5. supported_compressions <code>List[str]</code> Files compressed with the given formats (e.g. xz, gz) are uncompressed and matched like normal files.default: <code>[]</code> domain <code>str</code> The domain value <code>dft</code> will apply all normalizers for atomistic codes. Deprecated.default: <code>dft</code> level <code>int</code> The order by which the parser is executed with respect to other parsers.default: <code>0</code> code_name <code>str</code> code_homepage <code>str</code> code_category <code>str</code> metadata <code>dict</code> Metadata passed to the UI. Deprecated."},{"location":"reference/config.html#parserentrypoint","title":"ParserEntryPoint","text":"<p>Base model for parser plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Determines the entry point type.default: <code>parser</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> A human readable description of the plugin entry point. plugin_package <code>str</code> The plugin package from which this entry points comes from. level <code>int</code> Integer that determines the execution order of this parser. Parser with lowest level will attempt to match raw files first.default: <code>0</code> mainfile_contents_re <code>str</code> A regular expression that is applied the content of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches. mainfile_name_re <code>str</code> A regular expression that is applied the name of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_mime_re <code>str</code> A regular expression that is applied the mime type of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_binary_header <code>bytes</code> Matches a binary file if the given bytes are included in the file. mainfile_binary_header_re <code>bytes</code> Matches a binary file if the given binary regular expression bytes matches the file contents. mainfile_alternative <code>int</code> If True, the parser only matches a file, if no other file in the same directory matches a parser.default: <code>False</code> mainfile_contents_dict <code>dict</code> Is used to match structured data files like JSON or HDF5. supported_compressions <code>List[str]</code> Files compressed with the given formats (e.g. xz, gz) are uncompressed and matched like normal files.default: <code>[]</code>"},{"location":"reference/config.html#normalizer","title":"Normalizer","text":"<p>A Normalizer describes a NOMAD normalizer that can be loaded as a plugin.</p> name type plugin_type <code>str</code> The type of the plugin. This has to be the string <code>normalizer</code> for normalizer plugins.default: <code>normalizer</code> id <code>str</code> The unique identifier for this plugin. name <code>str</code> A short descriptive human readable name for the plugin. description <code>str</code> A human readable description of the plugin. plugin_documentation_url <code>str</code> The URL to the plugins main documentation page. plugin_source_code_url <code>str</code> The URL of the plugins main source code repository. python_package <code>str</code> Name of the python package that contains the plugin code and a plugin metadata file called <code>nomad_plugin.yaml</code>. normalizer_class_name <code>str</code> The fully qualified name of the Python class that implements the normalizer. This class must have a function <code>def normalize(self, logger)</code>."},{"location":"reference/config.html#normalizerentrypoint","title":"NormalizerEntryPoint","text":"<p>Base model for normalizer plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Determines the entry point type.default: <code>normalizer</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> A human readable description of the plugin entry point. plugin_package <code>str</code> The plugin package from which this entry points comes from. level <code>int</code> Integer that determines the execution order of this normalizer. Normalizers are run in order from lowest level to highest level.default: <code>0</code>"},{"location":"reference/config.html#schema","title":"Schema","text":"<p>A Schema describes a NOMAD Python schema that can be loaded as a plugin.</p> name type plugin_type <code>str</code> The type of the plugin. This has to be the string <code>schema</code> for schema plugins.default: <code>schema</code> id <code>str</code> The unique identifier for this plugin. name <code>str</code> A short descriptive human readable name for the plugin. description <code>str</code> A human readable description of the plugin. plugin_documentation_url <code>str</code> The URL to the plugins main documentation page. plugin_source_code_url <code>str</code> The URL of the plugins main source code repository. python_package <code>str</code> Name of the python package that contains the plugin code and a plugin metadata file called <code>nomad_plugin.yaml</code>. package_path <code>str</code> Path of the plugin package. Will be determined using python_package if not explicitly defined. key <code>str</code> Key used to identify this plugin."},{"location":"reference/glossary.html","title":"Glossary","text":"<p>This is a list of terms that have a specific meaning for NOMAD and are used through out the application and this documentation.</p>"},{"location":"reference/glossary.html#annotation","title":"Annotation","text":"<p>Annotations are part of data schemas and they describe aspects that are not directly defining the type or shape of data. They often allow to alter how certain data is managed, represented, or edited. See annotations in the schema documentation.</p>"},{"location":"reference/glossary.html#app","title":"App","text":"<p>Apps allow you to build customized user interfaces for specific research domains, making it easier to navigate and understand the data. This typically means that certain domain-specific properties are highlighted, different units may be used for physical properties, and specialized dashboards may be presented. This becomes crucial for NOMAD installations to be able to scale with data that contains a mixture of experiments and simulations, different techniques, and physical properties spanning different time and length scales.</p>"},{"location":"reference/glossary.html#archive","title":"Archive","text":"<p>NOMAD processes (parses and normalizes) all data. The entirety of all processed data is referred to as the Archive. Sometimes the term archive is used to refer to the processed data of a particular entry, e.g. \"the archive of that entry\".</p> <p>The term archive is an old synonym for processed data. Since the term has different meaning for different people and is very abstract, we are slowly deprecating its use in favor of processed data (e.g. NOMAD Repository and NOMAD Archive).</p>"},{"location":"reference/glossary.html#author","title":"Author","text":"<p>An author is typically a natural person that has uploaded a piece of data into NOMAD and has authorship over it. Often authors are users, but not always. Therefore, we have to distinguish between authors and users.</p>"},{"location":"reference/glossary.html#eln","title":"ELN","text":"<p>Electronic Lab Notebooks (ELNs) are a specific kind of entry in NOMAD. These entries can be edited in NOMAD, in contrast to entries that are created by uploading and processing data. ELNs offer form fields and other widgets to modify the contents of an entry. As all entries, ELNs are based on a schema; how quantities are edited (e.g. which type of widget) can be controlled through annotations.</p>"},{"location":"reference/glossary.html#entry","title":"Entry","text":"<p>Data in NOMAD is organized in entries (as in \"database entry\"). Entries have an entry id. Entries can be searched for and entries have individual pages on the NOMAD GUI. Entries are always associated with raw files, where one of these files is the mainfile. Raw files are processed to create the processed data (or the archive) for an entry.</p>"},{"location":"reference/glossary.html#dataset","title":"Dataset","text":"<p>Users can organize entries into datasets. Datasets are not created automatically, don't confuse them with uploads. Datasets can be compared to albums, labels, or tags on other platforms. Datasets are used to reference a collection of data and users can get a DOI for their datasets.</p>"},{"location":"reference/glossary.html#mainfile","title":"Mainfile","text":"<p>Each entry has one raw file that defines it. This is called the mainfile of that entry. Typically most, if not all, processed data of an entry is retrieved from that mainfile.</p>"},{"location":"reference/glossary.html#metadata","title":"Metadata","text":"<p>In NOMAD metadata refers to a specific technical sub-set of processed data. The metadata of an entry comprises ids, timestamps, hashes, authors, datasets, references, used schema, and other information.</p>"},{"location":"reference/glossary.html#metainfo","title":"Metainfo","text":"<p>The term metainfo refers to the sum of all schemas. In particular it is associated with all pre-defined schemas that are used to represent all processed data in a standardized way. Similar to an ontology, the metainfo provides additional meaning by associated in each piece of data with name, description, categories, type, shape, units, and more.</p>"},{"location":"reference/glossary.html#normalizer","title":"Normalizer","text":"<p>A normalizer is a small tool that can refine the processed data of an entry. Normalizers can read and modify processed data and thereby either normalize (change) some of the data or add normalized derived data. Normalizers are run after parsers and are often used to do processing steps that need to be applied to the outcome of many parsers and are therefore not part of the parsers themselves.</p> <p>There are normalizer classes and normalize functions. The normalizer classes are run after parsing in a particular order and if certain conditions are fulfilled. Normalize functions are part of schemas (i.e. section definitions). They are run at the end of processing on all the sections that instantiate the respective section definition.</p>"},{"location":"reference/glossary.html#parser","title":"Parser","text":"<p>A parser is a small program that takes a mainfile as input and produces processed data. Parsers transform information from a particular source format into NOMAD's structured schema-based format. Parsers start with a mainfile, but can open and read data from other files (e.g. those referenced in the mainfile). Typically, a parser is associated with a certain file-format and is only applied to files of that format.</p>"},{"location":"reference/glossary.html#plugin","title":"Plugin","text":"<p>NOMAD installations can be customized through plugins, which are Git repositories containing an installable python package that will add new features upon being installed. Plugins can contain one or many plugin entry points, which represent individual customizations.</p>"},{"location":"reference/glossary.html#plugin-entry-point","title":"Plugin entry point","text":"<p>Plugin entry points are used to configure and load different types of NOMAD customizations. There are several entry point types, including entry points for parsers, schema packages and apps. A single plugin may contain multiple entry points.</p>"},{"location":"reference/glossary.html#processed-data","title":"Processed data","text":"<p>NOMAD processes (parses and normalizes) all data. The processed data is the outcome of this process. Therefore, each NOMAD entry is associated with processed data that contains all the parsed and normalized information. Processed data always follows a schema. Processed data can be retrieved (via API) or downloaded as <code>.json</code> data.</p>"},{"location":"reference/glossary.html#processing","title":"Processing","text":"<p>NOMAD processes (parses and normalizes) all data. During processing, all provided files are considered. First, files are matched to parsers. Second, files that match with a parser, become mainfiles and an entry is created. Third, we run the parser to create processed data. Fourth, the processed data is further refined by running normalizers. Last, the processed data is saved and indexed. The exact processing time depends on the size of the uploaded data and users can track the processing state of each entry in the GUI.</p>"},{"location":"reference/glossary.html#quantity","title":"Quantity","text":"<p>All processed data is structured into sections and quantities. Sections provide hierarchy and organization, quantities refer to the actual pieces of data. In NOMAD, a quantity is the smallest referable unit of processed data. Quantities can have many types and shapes; examples are strings, numbers, lists, or matrices.</p> <p>In a schema, quantities are defined by their name, description, type, shape, and unit. Quantities in processed data are associated with a respective quantity definition from the respective schema.</p>"},{"location":"reference/glossary.html#raw-file","title":"Raw file","text":"<p>A raw file is any file that was provided by a NOMAD user. A raw-file might produce an entry, if it is of a supported file-format, but does not have to. Raw files always belong to an upload and might be associated with an entry (in this case, raw-files are also mainfiles).</p> <p>The sum of all raw files is also referred to as the Repository. This is an old term from when NOMAD was implemented as several services (e.g. NOMAD Repository and NOMAD Archive).</p>"},{"location":"reference/glossary.html#results-section-results","title":"Results (section <code>results</code>)","text":"<p>The results are a particular section of processed data. They comprise a summary of the most relevant data for an entry.</p> <p>While all processed data can be downloaded and is accessible via API, an entry's results (combined with its metadata) is also searchable and can be read quicker and in larger amounts.</p>"},{"location":"reference/glossary.html#schema","title":"Schema","text":"<p>Schemas define possible data structures for processed data. Like a book they organize data hierarchically in sections and subsections. Schemas are similar to ontologies as they define possible relationships between data organized within them.</p> <p>A schema is a collection of section and quantity definitions. Schemas are organized in schema packages, i.e. collections of definitions. All schemas combined form the metainfo.</p>"},{"location":"reference/glossary.html#schema-package","title":"Schema package","text":"<p>Schema packages contain a collection of schema definitions. Schema packages may be defined as YAML files or in Python as plugin entry points.</p>"},{"location":"reference/glossary.html#section-and-subsection","title":"Section and Subsection","text":"<p>All processed data is structured into sections and quantities. Sections provide hierarchy and organization, quantities refer to the actual pieces of data.</p> <p>In a schema, a section are defined by their name, description, all possible subsections, and quantities. Section definitions can also inherit all properties (subsections, quantities) from other section definitions using them as base sections.</p>"},{"location":"reference/glossary.html#upload","title":"Upload","text":"<p>NOMAD organizes raw-files (and all entries created from them) in uploads. Uploads consist of a directory structure of raw-files and a list of respective entries.</p> <p>Uploads are created by a single user, the owner. Uploads have two states. Initially, they are mutable and have limited visibility. Owners can invite other to collaborate and those users can add/remove/change data. The owner can publish an upload at some point, where the upload becomes immutable and visible to everyone. Uploads are the smallest unit of data that can be individually shared and published.</p>"},{"location":"reference/glossary.html#user","title":"User","text":"<p>A user is anyone with a NOMAD account. It is different from an author as all users can be authors, but not all authors have to be users. All data in NOMAD is always owned by a single user (others can be collaborators and co-authors).</p>"},{"location":"reference/parsers.html","title":"Supported parsers","text":"<p>Note</p> <p>You might also want to read:</p> <ul> <li>How to run parsers locally</li> <li>How to write a parser</li> </ul> <p>This is a list of all available parsers and supported file formats:</p> <p>ABACUS, ABINIT, AFLOW, Amber, AMS, ASAP, ASR, QuantumATK, Atomate, BigDFT, BOPfox, CASTEP, CHARMM, Chemotion, CP2K, CPMD, CRYSTAL, DFTB+, DL_POLY, DMol3, eDMFT, EELSDB, eLabFTW, ElaStic, Elk, exciting, FHI-aims, FHI-vibes, FLEUR, FPLO, GAMESS, Gaussian, GPAW, GROMACS, GROMOS, GULP, H5MD, LAMMPS, libAtoms, LOBSTER, magres, Molcas, MOPAC, NAMD, NWChem, OCEAN, Octopus, ONETEP, OpenKIM, OpenMX, ORCA, phonopy, Psi4, Qball, Qbox, QuantumEspressoEPW, QuantumEspressPhonon, QuantumESPRESSOXSpectra, QuantumESPRESSO, SIESTA, solid_dmft, TBStudio, Tinker, TURBOMOLE, VASP, w2dynamics, Wannier90, WIEN2k, xTB, YAMBO</p>"},{"location":"reference/parsers.html#atomistic-codes","title":"Atomistic codes","text":""},{"location":"reference/parsers.html#abacus","title":"ABACUS","text":"parser ABACUS format homepage http://abacus.ustc.edu.cn/ plugin name parsers/abacus package electronicparsers.abacus parser class electronicparsers.abacus.parser.ABACUSParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/abacus Input Filename Description <code>&lt;text_file&gt;</code> Mainfile, plain text file w/arbitrary name, e.g.,  <code>running_&lt;scf, nscf, relax, ...md&gt;.log</code> <code>INPUT</code> Runtime information AUX FILES Description <code>STRU</code> Material's atomic-structure information <code>KPT</code> K-points information <code>&lt;text_file&gt;</code> pseudopotental files <code>&lt;text_file&gt;</code> optimized atomic basis sets <code>TDOS</code> Kohn-Sham total DOS <code>PDOS</code> Projected DOS <code>BANDS_&lt;nspin&gt;.dat</code> bandstructure file"},{"location":"reference/parsers.html#abinit","title":"ABINIT","text":"parser ABINIT format homepage https://www.abinit.org/ plugin name parsers/abinit package electronicparsers.abinit parser class electronicparsers.abinit.parser.AbinitParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/abinit Input Filename Description <code>*.*o*</code> Mainfile: a plain text file w/ user-defined name <code>*.files</code> plain text; user-defined filenames <code>*.*i*</code> plain text, input parameters <code>*_o_DDB</code> netcdf binary file, Derivative DataBases of total energy <code>*_o_DEN</code> netcdf binary file, charge density <code>*_o_EIG</code> text file, eigenvalues <code>*_o_WFK</code> netcdf binary file, wavefunction <code>*o_SCR</code> netcdf binary file, RPA inverse dielectric screening <code>*o_SIGRES</code> netcdf binary file, GW self-energy correction <code>log</code> plain text, redirection of screen output (<code>stdout</code>)"},{"location":"reference/parsers.html#amber","title":"Amber","text":"parser Amber format homepage http://ambermd.org/ plugin name parsers/amber package atomisticparsers.amber parser class atomisticparsers.amber.parser.AmberParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/amber"},{"location":"reference/parsers.html#ams","title":"AMS","text":"parser AMS format homepage https://www.scm.com plugin name parsers/ams package electronicparsers.ams parser class electronicparsers.ams.parser.AMSParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/ams"},{"location":"reference/parsers.html#asap","title":"ASAP","text":"parser ASAP format homepage https://wiki.fysik.dtu.dk/asap plugin name parsers/asap package atomisticparsers.asap parser class atomisticparsers.asap.parser.AsapParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/asap"},{"location":"reference/parsers.html#quantumatk","title":"QuantumATK","text":"parser QuantumATK format homepage https://www.synopsys.com/silicon/quantumatk.html plugin name parsers/atk package electronicparsers.atk parser class electronicparsers.atk.parser.ATKParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/atk Input Filename Description <code>*.nc</code> The NetCDF output is used as the mainfile (HDF5 output is currently not yet supported) <code>*</code> Other ATK input and output files act as auxiliary files that can be downloaded, put are not parsed"},{"location":"reference/parsers.html#bigdft","title":"BigDFT","text":"parser BigDFT format homepage http://bigdft.org/ plugin name parsers/bigdft package electronicparsers.bigdft parser class electronicparsers.bigdft.parser.BigDFTParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/bigdft"},{"location":"reference/parsers.html#bopfox","title":"BOPfox","text":"parser BOPfox format homepage http://bopfox.de/ plugin name parsers/bopfox package atomisticparsers.bopfox parser class atomisticparsers.bopfox.parser.BOPfoxParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/bobfox"},{"location":"reference/parsers.html#castep","title":"CASTEP","text":"parser CASTEP format homepage http://www.castep.org/ plugin name parsers/castep package electronicparsers.castep parser class electronicparsers.castep.parser.CastepParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/castep"},{"location":"reference/parsers.html#charmm","title":"CHARMM","text":"parser CHARMM format homepage https://www.charmm.org plugin name parsers/charmm package electronicparsers.charmm parser class electronicparsers.charmm.parser.CharmmParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/charmm"},{"location":"reference/parsers.html#cp2k","title":"CP2K","text":"parser CP2K format homepage https://www.cp2k.org/ plugin name parsers/cp2k package electronicparsers.cp2k parser class electronicparsers.cp2k.parser.CP2KParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/cp2k Input Filename Description <code>*.out</code> Mainfile: output text file w/ arbitrary name <code>*.in</code> or <code>*.restart</code> input text file; defined in the first lines of <code>*.out</code> <code>*.pdos</code> (projected) dos output file <code>*.xyz</code> trajectories output file <code>*.ener</code> MD energies output file"},{"location":"reference/parsers.html#cpmd","title":"CPMD","text":"parser CPMD format homepage https://www.cpmd.org/ plugin name parsers/cpmd package electronicparsers.cpmd parser class electronicparsers.cpmd.parser.CPMDParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/cpmd"},{"location":"reference/parsers.html#crystal","title":"CRYSTAL","text":"parser CRYSTAL format homepage https://www.crystal.unito.it/ plugin name parsers/crystal package electronicparsers.crystal parser class electronicparsers.crystal.parser.CrystalParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/crystal Input Filename Description <code>&lt;text_file&gt;</code> Mainfile, plain text file w/arbitrary name. E.g.,  <code>simulation.out</code> <code>&lt;text_file&gt;.d12</code> Program input. Plain text file with the same name (different extension) as the mainfile. E.g. <code>simulation.d12</code> AUX FILES Description <code>&lt;text_file&gt;.f25</code> Output of various electronic and electrical properties. Plain text file with the same name (different extension) as the mainfile. NOTE: required in order to parse band structures and density of states. E.g. <code>simulation.f25</code>"},{"location":"reference/parsers.html#dftb","title":"DFTB+","text":"parser DFTB+ format homepage http://www.dftbplus.org/ plugin name parsers/dftbplus package atomisticparsers.dftbplus parser class atomisticparsers.dftbplus.parser.DFTBPlusParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/dftplus"},{"location":"reference/parsers.html#dl_poly","title":"DL_POLY","text":"parser DL_POLY format homepage https://www.scd.stfc.ac.uk/Pages/DL_POLY.aspx plugin name parsers/dl-poly package atomisticparsers.dlpoly parser class atomisticparsers.dlpoly.parser.DLPolyParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/dlpoly"},{"location":"reference/parsers.html#dmol3","title":"DMol3","text":"parser DMol3 format homepage http://dmol3.web.psi.ch/ plugin name parsers/dmol package electronicparsers.dmol3 parser class electronicparsers.dmol3.parser.Dmol3Parser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/dmol3"},{"location":"reference/parsers.html#edmft","title":"eDMFT","text":"parser eDMFT format homepage http://hauleweb.rutgers.edu/tutorials/ plugin name parsers/edmft package electronicparsers.edmft parser class electronicparsers.edmft.parser.EDMFTParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/edmft Input Filename Description <code>dmft_info.out</code> Mainfile: output full DMFT loop text file <code>dmft1_info.out</code> output DMFT1 loop text file <code>dmft2_info.out</code> output DMFT2 loop text file <code>*.indmfl</code> input basis parameters text file <code>*params.dat</code> input DMFT parameters text file <code>*.struct</code> output text file with data for the structure (specific to WIEN2k) <code>*projectorw.dat</code> output data file with projectors <code>*.dayfile</code> output sfc charge information for DMFT2-&gt;DFT <code>info.iterate</code> output sfc information; use second Ftot+T*Simp column for the free energy <code>*.gcJ</code> output Greens function lattice data per DMFT loop J <code>imp.X/Gf.out.I.J</code> output Greens function data for impurity X per DFT+DMFT loop I and DMFT loop J <code>sig.inpJ</code> output self-energy lattice data per DMFT loop J <code>imp.X/Sig.out.I.J</code> output self-energy data for impurity X data per DFT+DMFT loop I and DMFT loop J <code>*.dltJ</code> output hybridization function data per DMFT loop J <code>imp.X/Delta.inp.I.J</code> output hybridization function data for impurity X data DFT+DMFT loop I and DMFT loop J"},{"location":"reference/parsers.html#elk","title":"Elk","text":"parser Elk format homepage http://elk.sourceforge.net/ plugin name parsers/elk package electronicparsers.elk parser class electronicparsers.elk.parser.ElkParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/elk"},{"location":"reference/parsers.html#exciting","title":"exciting","text":"parser exciting format homepage http://exciting-code.org/ plugin name parsers/exciting package electronicparsers.exciting parser class electronicparsers.exciting.parser.ExcitingParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/exciting Input Filename Description <code>INFO.OUT</code> mainfile <code>BAND-QP.OUT</code> <code>BANDLINES.OUT</code> <code>DIELTENS0*.OUT</code> <code>DIELTENS0_NOSYM*.OUT</code> <code>EIGVAL.OUT</code> <code>EPSILON_*FXC*_OC*.OUT</code> <code>EPSILON_*NLF_FXC*_OC*.OUT</code> <code>EPSILON_BSE*_SCR*_OC*.OUT</code> <code>EVALQP.DAT or EVALQP.TXT</code> <code>EXCITON_BSE*_SCR*_OC*.OUT</code> <code>FERMISURF.bxsf</code> <code>GQPOINTS*.OUT</code> <code>GW_INFO.OUT</code> <code>INFO_VOL</code> <code>LOSS_*FXC*_OC*.OUT</code> <code>LOSS_*NLF_*FXC*_OC*.OUT</code> <code>QPOINTS.OUT</code> <code>SIGMA_*FXC*_OC*.OUT</code> <code>SIGMA_*NLF_FXC*_OC*.OUT</code> <code>SIGMA_BSE*_SCR*_OC*.OUT</code> <code>TDOS-QP.OUT</code> time dependent DOS <code>bandstructure-qp.dat</code> <code>bandstructure.xml</code> (vertexLabGWFile) <code>bandstructure.xml</code> <code>dos.xml</code> <code>input-gw.xml</code> <code>input.xml</code> (GSFile) <code>input.xml</code> (XSFile) <code>str.out</code>"},{"location":"reference/parsers.html#fhi-aims","title":"FHI-aims","text":"parser FHI-aims format homepage https://aimsclub.fhi-berlin.mpg.de/ plugin name parsers/fhi-aims package electronicparsers.fhiaims parser class electronicparsers.fhiaims.parser.FHIAimsParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/fhiaims Input Filename Description <code>&lt;text_file&gt;</code> Mainfile, plain text file w/arbitrary name, e.g.,  <code>&lt;output,control, aims,...&gt;.out</code> <code>control.in</code> Runtime information <code>geometry.in</code> Material's atomic-structure information, AUX FILES Description <code>&lt;atoml_label&gt;_l_proj_dos.out</code> Angular-momentum-resolved DOS @ Fermi Energy <code>&lt;atoml_label&gt;_l_proj_dos_raw.out</code> Angular-momentum-resolved DOS @ vacuum <code>KS_DOS_total.dat</code> Kohn-Sham total DOS @ Fermi Energy <code>KS_DOS_total_raw.dat</code> Kohn-Sham total DOS @ vacuum <code>Mulliken.out</code> WARNING--&gt; Mulliken charge analysis on all atoms. WARNING not yet read by NOMAD's parser <code>atom_proj_dos_&lt;atom_name&gt;&lt;index&gt;_raw.dat</code> Atom-projected DOS @ vacuum <code>atom_projected_dos_&lt;atom_name&gt;&lt;index&gt;.dat</code> Atom-projected DOS @ Fermi Energy <code>band&lt;spin&gt;&lt;segment&gt;.out</code> bandstructure file <code>GW_band&lt;spin&gt;&lt;segment&gt;</code> GW bandstructure file"},{"location":"reference/parsers.html#fleur","title":"FLEUR","text":"parser FLEUR format homepage https://www.flapw.de/ plugin name parsers/fleur package electronicparsers.fleur parser class electronicparsers.fleur.parser.FleurParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/fleur"},{"location":"reference/parsers.html#fplo","title":"FPLO","text":"parser FPLO format homepage https://www.fplo.de/ plugin name parsers/fplo package electronicparsers.fplo parser class electronicparsers.fplo.parser.FploParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/fplo"},{"location":"reference/parsers.html#gamess","title":"GAMESS","text":"parser GAMESS format homepage https://www.msg.chem.iastate.edu/ plugin name parsers/gamess package electronicparsers.gamess parser class electronicparsers.gamess.parser.GamessParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/gamess"},{"location":"reference/parsers.html#gaussian","title":"Gaussian","text":"parser Gaussian format homepage http://gaussian.com plugin name parsers/gaussian package electronicparsers.gaussian parser class electronicparsers.gaussian.parser.GaussianParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/gaussian"},{"location":"reference/parsers.html#gpaw","title":"GPAW","text":"parser GPAW format homepage https://wiki.fysik.dtu.dk/gpaw/ plugin name parsers/gpaw package electronicparsers.gpaw parser class electronicparsers.gpaw.parser.GPAWParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/gpaw"},{"location":"reference/parsers.html#gromacs","title":"GROMACS","text":"parser GROMACS format homepage http://www.gromacs.org/ plugin name parsers/gromacs package atomisticparsers.gromacs parser class atomisticparsers.gromacs.parser.GromacsParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/gromacs"},{"location":"reference/parsers.html#gromos","title":"GROMOS","text":"parser GROMOS format homepage http://www.gromos.net/ plugin name parsers/gromos package atomisticparsers.gromos parser class atomisticparsers.gromos.parser.GromosParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/gromos"},{"location":"reference/parsers.html#gulp","title":"GULP","text":"parser GULP format homepage http://gulp.curtin.edu.au/gulp/ plugin name parsers/gulp package atomisticparsers.gulp parser class atomisticparsers.gulp.parser.GulpParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/gulp"},{"location":"reference/parsers.html#h5md","title":"H5MD","text":"parser H5MD format homepage None plugin name parsers/h5md package atomisticparsers.h5md parser class atomisticparsers.h5md.parser.H5MDParser parser code None"},{"location":"reference/parsers.html#lammps","title":"LAMMPS","text":"parser LAMMPS format homepage https://lammps.sandia.gov/ plugin name parsers/lammps package atomisticparsers.lammps parser class atomisticparsers.lammps.parser.LammpsParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/lammps"},{"location":"reference/parsers.html#libatoms","title":"libAtoms","text":"parser libAtoms format homepage http://libatoms.github.io/ plugin name parsers/lib-atoms package atomisticparsers.libatoms parser class atomisticparsers.libatoms.parser.LibAtomsParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/libatoms"},{"location":"reference/parsers.html#magres","title":"magres","text":"parser magres format homepage https://www.ccpnc.ac.uk/docs/magres plugin name parsers/magres package electronicparsers.magres parser class electronicparsers.magres.parser.MagresParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/magres"},{"location":"reference/parsers.html#molcas","title":"Molcas","text":"parser Molcas format homepage http://molcas.org/ plugin name parsers/molcas package electronicparsers.molcas parser class electronicparsers.molcas.parser.MolcasParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/molcas"},{"location":"reference/parsers.html#mopac","title":"MOPAC","text":"parser MOPAC format homepage http://openmopac.net/ plugin name parsers/mopac package electronicparsers.mopac parser class electronicparsers.mopac.parser.MopacParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/mopac"},{"location":"reference/parsers.html#namd","title":"NAMD","text":"parser NAMD format homepage http://www.ks.uiuc.edu/Research/namd/ plugin name parsers/namd package atomisticparsers.namd parser class atomisticparsers.namd.parser.NAMDParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/namd"},{"location":"reference/parsers.html#nwchem","title":"NWChem","text":"parser NWChem format homepage https://nwchemgit.github.io/ plugin name parsers/nwchem package electronicparsers.nwchem parser class electronicparsers.nwchem.parser.NWChemParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/nwchem"},{"location":"reference/parsers.html#ocean","title":"OCEAN","text":"parser OCEAN format homepage https://feff.phys.washington.edu/OCEAN/index.html plugin name parsers/ocean package electronicparsers.ocean parser class electronicparsers.ocean.parser.OceanParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/ocean Input Filename Description <code>*</code> Mainfile: text output file (in JSON format) <code>*.in</code> input file with all parameters <code>absspct*</code> output data file with the Absorption Spectra <code>abslanc*</code> output data file with (Lanzcos algorithm) Absorption spectra <code>xesspct*</code> output data file with the Emission Spectra <code>rxsspct*</code> output data file with the RIXS <code>photon*</code> electron-photon operator"},{"location":"reference/parsers.html#octopus","title":"Octopus","text":"parser Octopus format homepage https://octopus-code.org/ plugin name parsers/octopus package electronicparsers.octopus parser class electronicparsers.octopus.parser.OctopusParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/octopus Input Filename Description <code>&lt;text_file&gt;</code> Mainfile: a plain text file w/arbitrary name <code>exec/</code> Subdir for runtime information <code>exec/parser.log</code> Input variables (user-defined &amp; default values) <code>inp</code> input file <code>parse.log</code> Warining : probably obsolete <code>restart/</code> Data to restart a calculation, e.g., <code>restart/gs/</code> is for ground-state <code>static/</code> Subdir to report static part of a calculation <code>static/eigenvalues</code> <code>static/info</code> General info on static part"},{"location":"reference/parsers.html#onetep","title":"ONETEP","text":"parser ONETEP format homepage https://www.onetep.org/ plugin name parsers/onetep package electronicparsers.onetep parser class electronicparsers.onetep.parser.OnetepParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/onetep"},{"location":"reference/parsers.html#openmx","title":"OpenMX","text":"parser OpenMX format homepage http://www.openmx-square.org/ plugin name parsers/openmx package electronicparsers.openmx parser class electronicparsers.openmx.parser.OpenmxParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/openmx Input Filename Description <code>&lt;systemname&gt;.out</code> Mainfile in OpenMX specific plain-text"},{"location":"reference/parsers.html#orca","title":"ORCA","text":"parser ORCA format homepage https://www.faccts.de/orca/ plugin name parsers/orca package electronicparsers.orca parser class electronicparsers.orca.parser.OrcaParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/orca"},{"location":"reference/parsers.html#psi4","title":"Psi4","text":"parser Psi4 format homepage https://psicode.org/ plugin name parsers/psi4 package electronicparsers.psi4 parser class electronicparsers.psi4.parser.Psi4Parser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/psi4 Input Filename Description <code>*.out</code> Mainfile: a plain text file w/ user-defined name <code>*.dat</code> plain text input file"},{"location":"reference/parsers.html#qball","title":"Qball","text":"parser Qball format homepage https://github.com/LLNL/qball plugin name parsers/qball package electronicparsers.qball parser class electronicparsers.qball.parser.QBallParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/qball Input Filename Description <code>*.out</code> Mainfile: a plain text file w/ user-defined name"},{"location":"reference/parsers.html#qbox","title":"Qbox","text":"parser Qbox format homepage http://qboxcode.org/ plugin name parsers/qbox package electronicparsers.qbox parser class electronicparsers.qbox.parser.QboxParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/qbox"},{"location":"reference/parsers.html#quantumespresso","title":"QuantumESPRESSO","text":"parser QuantumESPRESSO format homepage http://www.quantum-espresso.org/ plugin name parsers/quantumespresso package electronicparsers.quantumespresso parser class electronicparsers.quantumespresso.parser.QuantumEspressoParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/quantumespresso Filename Description <code>&lt;text_file&gt;</code> Mainfile: a plain text file w/arbitrary name. \\ One of the top lines must contain '<code>Program PWSCF.*starts</code>', \\ where '<code>.*</code>' means an arbitrary number '<code>*</code>' of arbitrary \\ characters '<code>.</code>' \""},{"location":"reference/parsers.html#siesta","title":"SIESTA","text":"parser SIESTA format homepage https://siesta-project.org/siesta plugin name parsers/siesta package electronicparsers.siesta parser class electronicparsers.siesta.parser.SiestaParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/siesta"},{"location":"reference/parsers.html#solid_dmft","title":"solid_dmft","text":"parser solid_dmft format homepage https://github.com/TRIQS/solid_dmft plugin name parsers/soliddmft package electronicparsers.soliddmft parser class electronicparsers.soliddmft.parser.SolidDMFTParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/soliddmft Input Filename Description <code>*.h5</code> Mainfile: h5 file containing all i/o parameters w/ arbitrary name"},{"location":"reference/parsers.html#tbstudio","title":"TBStudio","text":"parser TBStudio format homepage https://tight-binding.com/ plugin name parsers/tbstudio package electronicparsers.tbstudio parser class electronicparsers.tbstudio.parser.TBStudioParser parser code None Input Filename Description <code>*.tbm</code> Mainfile: output binary file"},{"location":"reference/parsers.html#tinker","title":"Tinker","text":"parser Tinker format homepage https://dasher.wustl.edu/tinker/ plugin name parsers/tinker package atomisticparsers.tinker parser class atomisticparsers.tinker.parser.TinkerParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/tinker"},{"location":"reference/parsers.html#turbomole","title":"TURBOMOLE","text":"parser TURBOMOLE format homepage https://www.turbomole.org/ plugin name parsers/turbomole package electronicparsers.turbomole parser class electronicparsers.turbomole.parser.TurbomoleParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/turbomole"},{"location":"reference/parsers.html#vasp","title":"VASP","text":"parser VASP format homepage https://www.vasp.at/ plugin name parsers/vasp package electronicparsers.vasp parser class electronicparsers.vasp.parser.VASPParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/vasp Input Filename Description <code>vasprun.xml</code> Mainfile in plain-text (structured) XML format <code>OUTCAR</code> plain-text (semi-structured) file, VAPS's detailed output. Read by NOMAD only as fallback to parse <code>outcar</code> data"},{"location":"reference/parsers.html#w2dynamics","title":"w2dynamics","text":"parser w2dynamics format homepage https://github.com/w2dynamics/w2dynamics plugin name parsers/w2dynamics package electronicparsers.w2dynamics parser class electronicparsers.w2dynamics.parser.W2DynamicsParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/w2dynamics Input Filename Description <code>*.hdf5</code> Mainfile: hdf5 file containing all i/o parameters w/ arbitrary name <code>*.in</code> input text file containing [general], [atoms], and [QMC] input parameters <code>epsk</code> plain text, discrete bath levels <code>Vk</code> plain text, hybridizations <code>w2d.log</code> output log error file"},{"location":"reference/parsers.html#wannier90","title":"Wannier90","text":"parser Wannier90 format homepage http://www.wannier.org/ plugin name parsers/wannier90 package electronicparsers.wannier90 parser class electronicparsers.wannier90.parser.Wannier90Parser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/wannier90 Input Filename Description <code>*.wout</code> Mainfile: output text file w/ arbitrary name <code>*.win</code> input text file <code>*band.dat</code> band structure output file <code>*dos.dat</code> dos output file <code>*hr.dat</code> hopping matrices (written if write_hr *.win is true)"},{"location":"reference/parsers.html#wien2k","title":"WIEN2k","text":"parser WIEN2k format homepage http://www.wien2k.at/ plugin name parsers/wien2k package electronicparsers.wien2k parser class electronicparsers.wien2k.parser.Wien2kParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/wien2k"},{"location":"reference/parsers.html#xtb","title":"xTB","text":"parser xTB format homepage https://www.chemie.uni-bonn.de/pctc/mulliken-center/software/xtb/ plugin name parsers/xtb package atomisticparsers.xtb parser class atomisticparsers.xtb.parser.XTBParser parser code https://github.com/nomad-coe/atomistic-parsers/tree/develop/atomisticparsers/xtb Input Filename Description <code>*.out</code> Mainfile: a plain text file w/ user-defined name <code>*.coord</code> plain text; structure file <code>*.xyz</code> plain text, structure file <code>*xtbopt.log</code> plain text, trajectory file of geometry optimization <code>*xtb.trj</code> plain text, trajectory of molecular dynamics <code>*xtbtopo.mol</code> plain text, topology file <code>*xtbrestart</code> binary file, restart file <code>charges</code> plain text, output charges"},{"location":"reference/parsers.html#yambo","title":"YAMBO","text":"parser YAMBO format homepage https://www.yambo-code.org/ plugin name parsers/yambo package electronicparsers.yambo parser class electronicparsers.yambo.parser.YamboParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/electronicparsers/yambo Input Filename Description <code>r-*</code> Mainfile: a plain text file w/ user-defined name <code>o-*</code> plain text auxiliary output files w/ user-defined filenames <code>*.in</code> plain text input file w/ user-defined name <code>n.*</code> netcdf file with user-defined name"},{"location":"reference/parsers.html#workflow-managers","title":"Workflow managers","text":""},{"location":"reference/parsers.html#aflow","title":"AFLOW","text":"parser AFLOW format homepage http://www.aflowlib.org/ plugin name parsers/aflow package workflowparsers.aflow parser class workflowparsers.aflow.parser.AFLOWParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/aflow Input Filename Description <code>aflowlib.json</code> Mainfile: a json file containing the aflow output <code>aflow.ael.out</code> plain text, elastic outputs <code>aflow.agl.out</code> plain text, Debye model output"},{"location":"reference/parsers.html#asr","title":"ASR","text":"parser ASR format homepage https://asr.readthedocs.io/en/latest/index.html plugin name parsers/asr package workflowparsers.asr parser class workflowparsers.asr.parser.ASRParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/asr Input Filename Description <code>archive*.json</code> Mainfile: a json file w/ user-defined name"},{"location":"reference/parsers.html#atomate","title":"Atomate","text":"parser Atomate format homepage https://www.atomate.org/ plugin name parsers/atomate package workflowparsers.atomate parser class workflowparsers.atomate.parser.AtomateParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/automate Input Filename Description <code>*materials.json</code> Mainfile: a json file containing system info <code>*.json</code> json files containing workflow results"},{"location":"reference/parsers.html#elastic","title":"ElaStic","text":"parser ElaStic format homepage http://exciting.wikidot.com/elastic plugin name parsers/elastic package workflowparsers.elastic parser class workflowparsers.elastic.parser.ElasticParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/elastic"},{"location":"reference/parsers.html#fhi-vibes","title":"FHI-vibes","text":"parser FHI-vibes format homepage https://vibes.fhi-berlin.mpg.de/ plugin name parsers/fhi-vibes package workflowparsers.fhivibes parser class workflowparsers.fhivibes.parser.FHIVibesParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/fhivibes Input Filename Description <code>&lt;hdf_file&gt;</code> Mainfile, binary hdf file w/ ext .nc`"},{"location":"reference/parsers.html#lobster","title":"LOBSTER","text":"parser LOBSTER format homepage http://schmeling.ac.rwth-aachen.de/cohp/ plugin name parsers/lobster package workflowparsers.lobster parser class workflowparsers.lobster.parser.LobsterParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/lobster Input Filename Description <code>lobsterout</code> Mainfile in LOBSTER specific plain-text"},{"location":"reference/parsers.html#phonopy","title":"phonopy","text":"parser phonopy format homepage https://phonopy.github.io/phonopy/ plugin name parsers/phonopy package workflowparsers.phonopy parser class workflowparsers.phonopy.parser.PhonopyParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/phonopy"},{"location":"reference/parsers.html#quantumespressoepw","title":"QuantumEspressoEPW","text":"parser QuantumEspressoEPW format homepage https://www.quantum-espresso.org plugin name parsers/quantumespressoepw package workflowparsers.quantum_espresso_epw parser class workflowparsers.quantum_espresso_epw.parser.QuantumEspressoEPWParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/quantum_espresso_epw"},{"location":"reference/parsers.html#quantumespressphonon","title":"QuantumEspressPhonon","text":"parser QuantumEspressPhonon format homepage https://www.quantum-espresso.org plugin name parsers/quantumespressophonon package workflowparsers.quantum_espresso_phonon parser class workflowparsers.quantum_espresso_phonon.parser.QuantumEspressoPhononParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/quantum_espresso_phonon"},{"location":"reference/parsers.html#quantumespressoxspectra","title":"QuantumESPRESSOXSpectra","text":"parser QuantumESPRESSOXSpectra format homepage https://www.quantum-espresso.org/Doc/INPUT_XSpectra.txt plugin name parsers/quantumespressoxspectra package workflowparsers.quantum_espresso_xspectra parser class workflowparsers.quantum_espresso_xspectra.parser.QuantumEspressoXSpectraParser parser code https://github.com/nomad-coe/workflow-parsers/tree/master/workflowparsers/quantum_espresso_xpectra Input Filename Description <code>*.out</code> Mainfile: text output file <code>*.dat</code> output data file with the Absorption Spectra"},{"location":"reference/parsers.html#elns","title":"ELNs","text":""},{"location":"reference/parsers.html#chemotion","title":"Chemotion","text":"parser Chemotion format homepage https://chemotion.net/ plugin name parsers/chemotion package nomad.parsing.chemotion parser class nomad.parsing.chemotion.ChemotionParser parser code None Input Filename Description <code>*.export.json</code> Mainfile as part of the .eln chemotion export <code>*</code> Files in .eln chemotion export that are references by the mainfile and description.txt"},{"location":"reference/parsers.html#elabftw","title":"eLabFTW","text":"parser eLabFTW format homepage https://www.elabftw.net/ plugin name parsers/elabftw package nomad.parsing.elabftw parser class nomad.parsing.elabftw.ELabFTWParser parser code https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-FAIR/-/tree/develop/nomad/parsing/elabftw Input Filename Description <code>*.ro-crate-metadata.json</code> Mainfile as part of the .eln eLabFTW export <code>*</code> Files in .eln eLabFTW export that are potentially references by the mainfile"},{"location":"reference/parsers.html#database-managers","title":"Database managers","text":""},{"location":"reference/parsers.html#eelsdb","title":"EELSDB","text":"parser EELSDB format homepage https://eelsdb.eu/ plugin name parsers/eels package eelsdbparser parser class eelsdbparser.eelsdb_parser.EELSDBParser parser code https://github.com/nomad-coe/nomad-parser-eelsdb"},{"location":"reference/parsers.html#openkim","title":"OpenKIM","text":"parser OpenKIM format homepage https://openkim.org/ plugin name parsers/openkim package databaseparsers.openkim parser class databaseparsers.openkim.parser.OpenKIMParser parser code https://github.com/nomad-coe/electronic-parsers/tree/develop/databaseparsers/openkim"},{"location":"reference/plugins.html","title":"Plugins","text":"<p>Plugins allow one to add Python-based functionality to NOMAD without a custom NOMAD image or release. Plugins can be installed at NOMAD start-up time. Therefore, a NOMAD installation or Oasis can be configured with a different custom set of plugins or disable unnecessary plugins.</p> <p>Note</p> <p>You might also want to read the how-to guide on plugins</p>"},{"location":"reference/plugins.html#plugin-entry-point-reference","title":"Plugin entry point reference","text":"<p>This is a list of the available plugin entry point configuration models.</p>"},{"location":"reference/plugins.html#appentrypoint","title":"AppEntryPoint","text":"<p>Base model for app plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Determines the entry point type.default: <code>app</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> A human readable description of the plugin entry point. plugin_package <code>str</code> The plugin package from which this entry points comes from. app <code>App</code> The app configuration."},{"location":"reference/plugins.html#exampleuploadentrypoint","title":"ExampleUploadEntryPoint","text":"<p>Base model for example upload plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Determines the entry point type.default: <code>example_upload</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> Longer description of the example upload. plugin_package <code>str</code> The plugin package from which this entry points comes from. category <code>str</code> Category for the example upload. title <code>str</code> Title of the example upload. path <code>str</code> Path to the example upload contents folder. Should be a path that starts from the package root folder, e.g. 'example_uploads/my_upload'. url <code>str</code> URL that points to an online file. If you use this instead of 'path', the file will be downloaded once upon app startup. local_path <code>str</code> The final path to use when creating the upload. This field will be automatically generated by the 'load' function and is typically not set manually."},{"location":"reference/plugins.html#normalizerentrypoint","title":"NormalizerEntryPoint","text":"<p>Base model for normalizer plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Determines the entry point type.default: <code>normalizer</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> A human readable description of the plugin entry point. plugin_package <code>str</code> The plugin package from which this entry points comes from. level <code>int</code> Integer that determines the execution order of this normalizer. Normalizers are run in order from lowest level to highest level.default: <code>0</code>"},{"location":"reference/plugins.html#parserentrypoint","title":"ParserEntryPoint","text":"<p>Base model for parser plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Determines the entry point type.default: <code>parser</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> A human readable description of the plugin entry point. plugin_package <code>str</code> The plugin package from which this entry points comes from. level <code>int</code> Integer that determines the execution order of this parser. Parser with lowest level will attempt to match raw files first.default: <code>0</code> mainfile_contents_re <code>str</code> A regular expression that is applied the content of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches. mainfile_name_re <code>str</code> A regular expression that is applied the name of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_mime_re <code>str</code> A regular expression that is applied the mime type of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_binary_header <code>bytes</code> Matches a binary file if the given bytes are included in the file. mainfile_binary_header_re <code>bytes</code> Matches a binary file if the given binary regular expression bytes matches the file contents. mainfile_alternative <code>int</code> If True, the parser only matches a file, if no other file in the same directory matches a parser.default: <code>False</code> mainfile_contents_dict <code>dict</code> Is used to match structured data files like JSON or HDF5. supported_compressions <code>List[str]</code> Files compressed with the given formats (e.g. xz, gz) are uncompressed and matched like normal files.default: <code>[]</code>"},{"location":"reference/plugins.html#schemapackageentrypoint","title":"SchemaPackageEntryPoint","text":"<p>Base model for schema package plugin entry points.</p> name type id <code>str</code> Unique identifier corresponding to the entry point name. Automatically set to the plugin entry point name in pyproject.toml. entry_point_type <code>str</code> Specifies the entry point type.default: <code>schema_package</code> name <code>str</code> Name of the plugin entry point. description <code>str</code> A human readable description of the plugin entry point. plugin_package <code>str</code> The plugin package from which this entry points comes from."},{"location":"reference/plugins.html#default-plugin-entry-points","title":"Default plugin entry points","text":"<p>This is a list of the plugin entry points that are activated by default:</p> <p>example_upload: None (This upload demonstrate the basic use of NOMAD's parsers. For many electronic structure codes (VASP, etc.), NOMAD provides parsers. You simply upload the input and output files of your simulations and NOMAD parsers are extracting all necessary metadata to produce a FAIR dataset. ), None (This example contains a custom NOMAD schema to create an Electronic Lab Notebook (ELN) and a few example data entries that use this schema. The schema demonstrates the basic concepts behind a NOMAD ELN and can be a good starting point to create you own schemas that model FAIR data acquired in your lab. ), None (This upload demonstrates the used of tabular data. In this example we use an xlsx file in combination with a custom schema. The schema describes what the columns in the excel file mean and NOMAD can parse everything accordingly to produce a FAIR dataset. ), None (This notebook will teach you how you can build tailored research data management (RDM) solutions using NOMAD. It uses existing thermally activated delayed fluorescent (TADF) molecule data to teach you how we can use the NOMAD to turn research data into datasets that are FAIR: Findable, Accessible, Interoperable and Reusable. The end-result can be distributed as a NOMAD plugin: a self-contained Python package that can be installed on a NOMAD Oasis. ), None (This upload provides a notebook as a tutorial that demonstrates how to use NOMAD for managing custom data and file types. Based on a simple Countries of the World dataset, it shows how to model the data in a schema, do parsing and normalization, process data, access existing data with NOMAD's API for analysis, and how to add visualization to your data. ), None (This example presents the capabilities of the NOMAD platform to store and standardize ellipsometry data. It shows the generation of a NeXus file according to the NXellipsometry application definition and a successive analysis of a SiO2 on Si Psi/Delta measurement. ), None (This example presents the capabilities of the NOMAD platform to store and standardize multi photoemission spectroscopy (MPES) experimental data. It contains three major examples:</p> <ul> <li>Taking a pre-binned file, here stored in a h5 file, and converting it into the standardized MPES NeXus format.     There exists a NeXus application definition for MPES which details the internal structure of such a file.</li> <li>Binning of raw data (see here for additional resources) into a h5 file and consecutively generating a NeXus file from it.</li> <li>An analysis example using data in the NeXus format and employing the pyARPES analysis tool to reproduce the main findings of this paper. ), None (This example presents the capabilities of the NOMAD platform to store and standardize XPS data. It shows the generation of a NeXus file according to the NXmpes application definition and a successive analysis of an example data set. ), None (This example is for two types of experiments: Scanning Tunneling Microscopy (STM) and Scanning Tunneling Spectroscopy (STS) from Scanning Probe Microscopy. It can transform the data from files generated by a the nanonis software into the NeXus application definition NXsts. The example contains data files from the two specific nanonis software versions generic 5e and generic 4.5. ), None (This example is for two types of experiments: Scanning Tunneling Microscopy (STM) and Scanning Tunneling Spectroscopy (STS) from Scanning Probe Microscopy. It can transform the data from files generated by a the nanonis software into the NeXus application definition NXsts. The example contains data files from the two specific nanonis software versions generic 5e and generic 4.5. ), None (This example presents the capabilities of the NOMAD platform to store and standardize atom probe data. It shows the generation of a NeXus file according to the NXapm application definition and a successive analysis of an example data set. The example contains a small atom probe dataset from an experiment with a LEAP instrument to get you started and keep the size of your NOMAD installation small. Ones started, we recommend to change the respective input file in the NOMAD Oasis ELN to run the example with your own datasets. ), None (This example presents the capabilities of the NOMAD platform to store and standardize electron microscopy. It shows the generation of a NeXus file according to the NXem application definition. The example contains a small set of electron microscopy datasets to get started and keep the size of your NOMAD installation small. Ones started, we recommend to change the respective input file in the NOMAD Oasis ELN to run the example with your own datasets. ), None (This example shows users how to take data from a Python framework and map it out to a Nexus application definition for IV Temperature measurements, NXiv_temp. We use the Nexus ELN features of Nomad to generate a Nexus file. )</li> </ul> <p>parser: parsers/abacus, parsers/abinit, parsers/aflow, parsers/amber, parsers/ams, parsers/asap, parsers/asr, parsers/atk, parsers/atomate, parsers/bigdft, parsers/bopfox, parsers/castep, parsers/charmm, parsers/chemotion, parsers/cp2k, parsers/cpmd, parsers/crystal, parsers/dftbplus, parsers/dl-poly, parsers/dmol, parsers/edmft, parsers/eels, parsers/elabftw, parsers/elastic, parsers/elk, parsers/exciting, parsers/fhi-aims, parsers/fhi-vibes, parsers/fleur, parsers/fplo, parsers/gamess, parsers/gaussian, parsers/gpaw, parsers/gromacs, parsers/gromos, parsers/gulp, parsers/h5md, parsers/lammps, parsers/lib-atoms, parsers/lobster, parsers/magres, parsers/molcas, parsers/mopac, parsers/namd, parsers/nwchem, parsers/ocean, parsers/octopus, parsers/onetep, parsers/openkim, parsers/openmx, parsers/orca, parsers/phonopy, parsers/psi4, parsers/qball, parsers/qbox, parsers/quantumespressoepw, parsers/quantumespressophonon, parsers/quantumespressoxspectra, parsers/quantumespresso, parsers/siesta, parsers/soliddmft, parsers/tbstudio, parsers/tinker, parsers/turbomole, parsers/vasp, parsers/w2dynamics, parsers/wannier90, parsers/wien2k, parsers/xtb, parsers/yambo</p>"},{"location":"reference/tutorials.html","title":"List of NOMAD tutorials","text":"<p>Attention</p> <p>This page is still being updated with older NOMAD tutorials.</p> <ul> <li> <p>14.02.2024 FAIRmat Tutorial 12: Getting started with NOMAD and NOMAD Oasis for research data management (RDM)</p> <ul> <li>A practical session to go through a Jupyter notebook that demonstrates how to use NOMAD for managing custom data and file types. Based on a simple given dataset, we show how to model the data in a schema, do parsing and normalization, process data, access existing data with NOMAD's API for analysis, and how to add visualization to your data.</li> </ul> </li> <li> <p>25.10.2023 FAIRmat Tutorial 11: Research data management, from fundamentals to implementation</p> <ul> <li>Introduction to the FAIR data principles, Research data management practices, and data management plans.</li> </ul> </li> <li> <p>29.09.2023 CECAM workshop: An Introduction to the NOMAD repository for soft matter simulators</p> <ul> <li>Basic NOMAD usage with molecular dynamics simulations, custom workflows, python API</li> </ul> </li> <li> <p>14.06.2023 FAIRmat Tutorial 10: FAIR electronic-structure data in NOMAD</p> <ul> <li>Basic NOMAD usage for computational data, numerical precision filtering, custom workflows, knowledge-based XC functionals exploration</li> </ul> </li> <li> <p>26.04.2023 FAIRmat Tutorial 9: Plugins: Python schemas and parsers</p> <ul> <li>Introduction the new plugin mechanism in NOMAD: how to develop and integrate their own Python schemas and parsers to a NOMAD Oasis.</li> </ul> </li> <li> <p>15.03.2023 FAIRmat Tutorial 8: Using NOMAD as an Electronic lab notebook (ELN) for FAIR data</p> <ul> <li>Using NOMAD as an ELN which enables the users to generate data following the FAIR principles.</li> </ul> </li> <li> <p>15.02.2023 FAIRmat Tutorial 7: Molecular Dynamics Trajectories and Workflows in NOMAD</p> <ul> <li>Uploading MD data, examining metadata, overview page, workflow visualizer, extracting MD data for trajectory analysis</li> </ul> </li> <li> <p>06.11.2022 FAIRmat Tutorial 6: Experimental data management in NOMAD</p> <ul> <li>Follow research data lifecycle from planning and running an experiment to collecting and annotating data according to international community standards for searchability, and reuse.</li> </ul> </li> <li> <p>05.10.2022 FAIRmat Tutorial 5: NOMAD Encyclopedia</p> <ul> <li>Navigate the materials space using the Encyclopedia GUI, and using the Encyclopedia API.</li> </ul> </li> <li> <p>11.05.2022 FAIRmat Tutorial 4: NOMAD Oasis and FAIR Data Collaboration and Sharing</p> <ul> <li>How to get started with NOMAD Oasis and adapt it to your research.</li> </ul> </li> <li> <p>06.04.2022 FAIRmat Tutorial 3: Introduction to the Artificial Intelligence Toolkit</p> <ul> <li>NOMAD AI-toolkit: query over the NOMAD Archive via the NOMAD API and basic notebooks for learning AI methods.</li> </ul> </li> <li> <p>09.03.2022 FAIRmat Tutorial 2: Electronic Lab Notebooks and FAIR Data Management</p> <ul> <li>ELN in FAIR data management, NOMAD ELN, and integrating other ELN tools with NOMAD.</li> </ul> </li> <li> <p>09.02.2022 FAIRmat Tutorial 1: Publish and Explore Data with NOMAD</p> <ul> <li>Basic NOMAD guide to prepare, upload, publish data, and reference them with a DOI.</li> </ul> </li> </ul>"},{"location":"tutorial/access_api.html","title":"Access data via API","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>This video tutorial explains the basics of API and shows how to do simple requests against the NOMAD API.</p> <p>Note</p> <p>The NOMAD seen in the tutorials is an older version with a different color theme, but all the demonstrated functionality is still available on the current version. You'll find the NOMAD test installation mentioned in the first video here.</p>"},{"location":"tutorial/custom.html","title":"Schemas and plugins","text":""},{"location":"tutorial/custom.html#what-is-a-custom-schema","title":"What is a custom schema","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p> <p>An example of custom schema written in YAML language.</p> <pre><code>definitions:\n  name: 'My test ELN'\n  sections:\n    MySection:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n      m_annotations:\n        eln:\n      quantities:\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n</code></pre>"},{"location":"tutorial/custom.html#the-base-sections","title":"The base sections","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"tutorial/custom.html#use-of-yaml-files","title":"Use of YAML files","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"tutorial/custom.html#the-built-in-tabular-parser","title":"The built-in tabular parser","text":"<p>NOMAD provides a standard parser to import your data from a spreadsheet file (<code>Excel</code> file with .xlsx extension) or from a CSV file (a Comma-Separated Values file with .csv extension). There are several ways to parse a tabular data file into a structured data file, depending on which structure we want to give our data. Therefore, the tabular parser can be set very flexibly, directly from the schema file through annotations. In this tutorial we will focus on most common modes of the tabular parser. A complete description of all modes is given in the Reference section. You can also follow the dedicated How To to see practical examples of the NOMAD tabular parser, in each section you can find a commented sample schema with a step-by-step guide on how to set it to obtain the desired final structure of your parsed data. We will make use of the tabular parser in a custom yaml schema. To obtain some structured data in NOMAD with this parser:</p> <p>1) the schema files should follow the NOMAD archive files naming convention (i.e. <code>.archive.json</code> or <code>.archive.yaml</code> extension) 2) a data file must be instantiated from the schema file</p> <p>3) a tabular data file must be dragged in the annotated quantity in order for NOMAD to parse it (the quantity is called <code>data_file</code> in the following examples)</p>"},{"location":"tutorial/custom.html#to-be-an-entry-or-not-to-be-an-entry","title":"To be an Entry or not to be an Entry","text":"<p>To use this parser, three kinds of annotation must be included in the schema: <code>tabular</code>, <code>tabular_parser</code>, <code>label_quantity</code>. Refer to the dedicated Reference section for the full list of options.</p> <p>important</p> <p>The ranges of the three <code>mapping_options</code>, namely <code>file_mode</code>, <code>mapping_mode</code>, and <code>sections</code> can give rise to twelve different combinations (see table in Reference). It is worth to analyze each of them to understand which is the best choice to pursue from case to case. Some of them give rise to \"not possible\" data structures but are still listed for completeness, a brief explanation of why it is not possible to implement them is also provided. The main bring-home message is that a tabular data file can be parsed in one or more entries in NOMAD, giving rise to diverse and arbitrarily complex structures.</p> <p>In the following sections, two examples will be illustrated. A tabular data file is parsed into one or more data archive files, their structure is based on a schema archive file. NOMAD archive files are denoted as Entries.</p> <p>Note</p> <p>From the NOMAD point of view, a schema file and a data file are the same kind of file where different sections have been filled (see archive files description). Specifically, a schema file has its <code>definitions</code> section filled while a data file will have its <code>data</code> section filled. See How to write a schema for a more complete description of an archive file.</p>"},{"location":"tutorial/custom.html#example-1","title":"Example 1","text":"<p>We want instantiate an object created from the schema already shown in the first Tutorial section and populate it with the data contained in the following excel file.</p> <p> </p> <p>The two columns in the file will be stored in a NOMAD Entry archive within two array quantities, as shown in the image below. In the case where the section to be filled is not in the root level of our schema but nested inside, it is useful to check the dedicated How-to.</p> <p> </p> <p>The schema will be decorated by the annotations mentioned at the beginning of this section  and will look like this:</p> <pre><code>definitions:\n  name: 'My test ELN'\n  sections:\n    MySection:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: column\n                  file_mode: current_entry\n                  sections:\n                    - '#root'\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre> <p>Here the tabular data file is parsed by columns, directly within the Entry where the <code>TableData</code> is inherited and filling the quantities in the root level of the schema (see dedicated how-to to learn how to inherit tabular parser in your schema).</p> <p>Note</p> <p>In yaml files a dash character indicates a list element. <code>mapping_options</code> is a list because it is possible to parse multiple tabular sheets from the same schema with different parsing options. <code>sections</code> in turn is a list because multiple sections of the schema can be parsed with same parsing options.</p>"},{"location":"tutorial/custom.html#example-2","title":"Example 2","text":"<p>In this example, each row of the tabular data file will be placed in a new Entry that is an instance of a class defined in the schema. This would make sense for, say, an inventory spreadsheet where each row can be a separate entity such as a sample, a substrate, etc. In this case, a manyfold of Entries will be generated based on the only class available in the schema. These Entries will not be bundled together by a parent Entry but just live in our NOMAD Upload as a spare list, to bundle them together it is useful to check the dedicated How-to. They might still be referenced manually inside an overarching Entry, such as an experiment Entry, from the ELN with <code>ReferenceEditQuantity</code>.</p> <pre><code>definitions:\n  name: 'My test ELN'\n  sections:\n    MySection:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      more:\n        label_quantity: my_quantity_1\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: multiple_new_entries\n                  sections:\n                    - '#root'\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_quantity_2:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre> <p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"tutorial/custom.html#custom-normalizers","title":"Custom normalizers","text":"<p>For custom schemas, you might want to add custom normalizers. All files are parsed and normalized when they are uploaded or changed. The NOMAD metainfo Python interface allows you to add functions that are called when your data is normalized.</p> <p>Here is an example:</p> <pre><code>from nomad.datamodel import Schema, ArchiveSection\nfrom nomad.metainfo.metainfo import Quantity, Datetime, SubSection\n\n\nclass Sample(ArchiveSection):\n    added_date = Quantity(type=Datetime)\n    formula = Quantity(type=str)\n\n    sample_id = Quantity(type=str)\n\n    def normalize(self, archive, logger):\n        super(Sample, self).normalize(archive, logger)\n\n        if self.sample_id is None:\n            self.sample_id = f'{self.added_date}--{self.formula}'\n\n\nclass SampleDatabase(Schema):\n    samples = SubSection(section=Sample, repeats=True)\n</code></pre> <p>To add a <code>normalize</code> function, your section has to inherit from <code>ArchiveSection</code> which provides the base for this functionality. Now you can overwrite the <code>normalize</code> function and add you own behavior. Make sure to call the <code>super</code> implementation properly to support schemas with multiple inheritance.</p> <p>If we parse an archive like this:</p> <pre><code>data:\n  m_def: 'examples.archive.custom_schema.SampleDatabase'\n  samples:\n    - formula: NaCl\n      added_date: '2022-06-18'\n</code></pre> <p>we will get a final normalized archive that contains our data like this:</p> <pre><code>{\n  \"data\": {\n    \"m_def\": \"examples.archive.custom_schema.SampleDatabase\",\n    \"samples\": [\n      {\n        \"added_date\": \"2022-06-18T00:00:00+00:00\",\n        \"formula\": \"NaCl\",\n        \"sample_id\": \"2022-06-18 00:00:00+00:00--NaCl\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"tutorial/develop_plugin.html","title":"Developing a NOMAD Plugin","text":"<p>In this tutorial you will learn how to create and develop a NOMAD plugin. As an example we will create a plugin to log data for a simple sintering process.</p>"},{"location":"tutorial/develop_plugin.html#prerequisites","title":"Prerequisites","text":"<ul> <li>A GitHub account. This can be created for free on github.com.</li> <li>Basic understanding of Python.</li> <li>Basic understanding of NOMAD metainfo, see for example tutorial 8.</li> </ul> <p>Note</p> <p>Several software development concepts are being used during this tutorial. Here is a list with some further information on each of them:</p> <ul> <li>what is Git</li> <li>what is VSCode, i. e., an Integrated Development Environment (IDE)</li> <li>what is Pip</li> <li>what is a Python virtual environment</li> <li>creating a Python package</li> <li>uploading a package to PyPI</li> <li>what is cruft</li> </ul>"},{"location":"tutorial/develop_plugin.html#create-a-github-repository","title":"Create a Git(Hub) repository","text":"<p>Firstly, we recommend to use git to version control your NOMAD plugin. There is a GitHub template repository that can be used for this at github.com/FAIRmat-NFDI/nomad-plugin-template.</p> <p>To use the template you should choose the \"Create an new repository\" option after pressing the green \"Use this template\" button in the upper right corner. Please note that you have to be logged into to GitHub to see this option.</p> <p> </p> <p>Enter a name (I will use \"nomad-sintering\" for mine) for your repository and click \"Create Repository\".</p>"},{"location":"tutorial/develop_plugin.html#generate-the-plugin-structure","title":"Generate the plugin structure","text":"<p>Next, we will use a cookiecutter template to create the basic structure of our NOMAD plugin.</p> <p>There are now two options for how to proceed.</p> <ol> <li>You can use the GitHub codespaces environment to develop your plugin, or</li> <li>If you have access to a Linux computer you can also run the same steps locally.</li> </ol>"},{"location":"tutorial/develop_plugin.html#1-using-github-codespaces","title":"1. Using GitHub codespaces","text":"<p>To use a GitHub codespace for the plugin development you should choose the \"Create codespace on main\" option after pressing the green \"&lt;&gt; Code\" button in the upper right corner.</p> <p> </p>"},{"location":"tutorial/develop_plugin.html#2-developing-locally","title":"2. Developing locally","text":"<p>If you have a Linux machine and prefer to develop locally you should instead click the \"Local\" tab after pressing the green \"&lt;&gt; Code\" button, copy the path, and clone your repository by running:</p> <p><pre><code>git clone PATH/COPIED/FROM/REPOSITORY\n</code></pre> and move inside the top directory <pre><code>cd REPOSITORY_NAME\n</code></pre> You will also need to install cruft, preferably using <code>pipx</code>: <pre><code># pipx is strongly recommended.\npipx install cruft\n\n# If pipx is not an option,\n# you can install cruft in your Python user directory.\npython -m pip install --user cruft\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#run-cruft","title":"Run cruft","text":"<p>The next step is to run cruft to use our cookiecutter template: <pre><code>cruft create https://github.com/FAIRmat-NFDI/cookiecutter-nomad-plugin\n</code></pre> Cookiecutter prompts you for information regarding your plugin and I will enter the following for my example:</p> <pre><code>  [1/12] full_name (John Doe): Hampus N\u00e4sstr\u00f6m\n  [2/12] email (john.doe@physik.hu-berlin.de): hampus.naesstroem@physik.hu-berlin.de\n  [3/12] github_username (foo): hampusnasstrom\n  [4/12] plugin_name (foobar): sintering\n  [5/12] module_name (sintering):\n  [6/12] short_description (Nomad example template): A schema package plugin for sintering.\n  [7/12] version (0.1.0):\n  [8/12] Select license\n    1 - MIT\n    2 - BSD-3\n    3 - GNU GPL v3.0+\n    4 - Apache Software License 2.0\n    Choose from [1/2/3/4] (1):\n  [9/12] include_schema_package [y/n] (y): y\n  [10/12] include_normalizer [y/n] (y): n\n  [11/12] include_parser [y/n] (y): n\n  [12/12] include_app [y/n] (y): n\n</code></pre> <p>There you go - you just created a minimal NOMAD plugin:</p> <p>Note</p> <p>In the above prompt, we pressed <code>y</code> for schema_package, this creates a python package</p> <p>with a plugin entry point for a schema package.</p> <pre><code>nomad-sintering/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 move_template_files.sh\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 nomad_sintering\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 schema_packages\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u2514\u2500\u2500 mypackage.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 conftest.py\n    \u251c\u2500\u2500 data\n    \u2502   \u2514\u2500\u2500 test.archive.yaml\n    \u2514\u2500\u2500 schema_packages\n        \u2514\u2500\u2500 test_schema.py\n</code></pre> <p>Note</p> <p>The project <code>nomad-sintering</code> is created in a new directory, we have included a helper script to move all the files to the parent level of the repository.</p> <pre><code>sh CHANGE_TO_PLUGIN_NAME/move_template_files.sh\n</code></pre> <p>Attention</p> <p>The <code>CHANGE_TO_PLUGIN_NAME</code> should be substituted by the name of the plugin you've created. In the above case it'll be <code>sh nomad-sintering/move_template_files.sh</code>.</p> <p>Finally, we should add the files we created to git and commit the changes we have made: <pre><code>git add -A\ngit commit -m \"Generated plugin from cookiecutter template\"\ngit push\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#enable-cruft-updates","title":"Enable Cruft updates","text":"<p>In order to receive updates from our cookiecutter template we have included a GitHub action that automatically checks for updates once a week (or by triggering it manually). In order for this action to run we need to give the action permission to write and create pull requests. To do this we should go back to the plugin repo and head to the settings tab and navigate to the Actions/General options on the left:</p> <p> </p> <p>At the very bottom of this place you should mark the \"Read and write permissions\" and the \"Allow GitHub Actions to create and approve pull requests\" options and click save.</p> <p> </p>"},{"location":"tutorial/develop_plugin.html#setting-up-the-python-environment","title":"Setting up the python environment","text":""},{"location":"tutorial/develop_plugin.html#creating-a-virtual-environment","title":"Creating a virtual environment","text":"<p>Before we can start developing we recommend to create a virtual environment using Python 3.11</p> <pre><code>python3.11 -m venv .pyenv\nsource .pyenv/bin/activate\n</code></pre>"},{"location":"tutorial/develop_plugin.html#installing-the-plugin","title":"Installing the plugin","text":"<p>Next we should install our plugin package in editable mode and using the nomad package index</p> <pre><code>pip install --upgrade pip\npip install -e '.[dev]' --index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre> <p>Note</p> <p>Until we have an official PyPI NOMAD release with the latest NOMAD version, make sure to include NOMAD's internal package registry (e.g. via --index-url). The latest PyPI package available today is version 1.2.2 and it misses some updates functional to this tutorial. In the future, when a newer release of <code>nomad-lab</code> will be available (    1.2.2) you can omit the <code>--index-url</code>.</p>"},{"location":"tutorial/develop_plugin.html#importing-a-yaml-schema","title":"Importing a yaml schema","text":""},{"location":"tutorial/develop_plugin.html#the-schema","title":"The schema","text":"<p>We will now convert the yaml schema package from part 2 where we described a sintering step:</p> <pre><code>definitions:\n  name: 'Tutorial 13 sintering schema'\n  sections:\n    TemperatureRamp:\n      m_annotations:\n        eln:\n          properties:\n            order:\n              - \"name\"\n              - \"start_time\"\n              - \"initial_temperature\"\n              - \"final_temperature\"\n              - \"duration\"\n              - \"comment\"\n      base_sections:\n        - nomad.datamodel.metainfo.basesections.ProcessStep\n      quantities:\n        initial_temperature:\n          type: np.float64\n          unit: celsius\n          description: \"initial temperature set for ramp\"\n          m_annotations:\n            eln:\n              component: NumberEditQuantity\n              defaultDisplayUnit: celsius\n        final_temperature:\n          type: np.float64\n          unit: celsius\n          description: \"final temperature set for ramp\"\n          m_annotations:\n            eln:\n              component: NumberEditQuantity\n              defaultDisplayUnit: celsius\n    Sintering:\n      base_sections:\n        - nomad.datamodel.metainfo.basesections.Process\n        - nomad.datamodel.data.EntryData\n      sub_sections:\n        steps:\n          repeats: True\n          section: '#/TemperatureRamp'\n</code></pre> <p>We can grab this file from the tutorial repository using curl <pre><code>curl -L -o sintering.archive.yaml \"https://raw.githubusercontent.com/FAIRmat-NFDI/AreaA-Examples/main/tutorial13/part3/files/sintering.archive.yaml\"\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#metainfo-yaml2py","title":"<code>metainfo-yaml2py</code>","text":"<p>We will now use an external package <code>metainfo-yaml2py</code> to convert the yaml schema package into python class definitions. First we install the package with <code>pip</code>: <pre><code>pip install metainfoyaml2py\n</code></pre></p> <p>Then we can run the <code>metainfo-yaml2py</code> command on the <code>sintering.archive.yaml</code> file with the <code>-n</code> flag for adding <code>normalize()</code> functions (will be explained later) and specify the output directory, with the <code>-o</code> flag, to be our <code>schema_packages</code> directory: <pre><code>metainfo-yaml2py sintering.archive.yaml -o src/nomad_sintering/schema_packages -n\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#updating-__init__py-and-pyprojecttoml","title":"Updating <code>__init__.py</code> and <code>pyproject.toml</code>","text":"<p>The metadata of our package is defined in the <code>__init__.py</code> file and here we now need to add the sintering package that we just created. If we take a look in that file we can see an example created by the cookiecutter template. We can go ahead and copy the <code>MySchemaPackageEntryPoint</code> class and the <code>mypackage</code> instance and paste them below. We then need to change: 1. the name of the class, 2. the import in the load function to import our sintering schema package, 3. the name of the instance and the class it uses, 4. ideally we should also update the description and the name.</p> <p>The changes could look something like this:</p> <pre><code>class SinteringEntryPoint(SchemaPackageEntryPoint):\n\n    def load(self):\n        from nomad_sintering.schema_packages.sintering import m_package\n\n        return m_package\n\n\nsintering = SinteringEntryPoint(\n    name='Sintering',\n    description='Schema package for describing a sintering process.',\n)\n</code></pre> <p>Finally, we also need to add our new entry point to the <code>pyproject.toml</code>. At the bottom of the toml you will see how this was done for the example and we just need to replicate that with whatever we called our instance: <pre><code>sintering = \"nomad_sintering.schema_packages:sintering\"\n</code></pre></p> <p>Before we continue, we should commit our changes to git: <pre><code>git add -A\ngit commit -m \"Added sintering classes from yaml schema\"\ngit push\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#ruff-autoformatting","title":"Ruff autoformatting","text":"<p>If we check the actions tab of the GitHub repository we might see that the last commit caused an error in the Ruff format checking. We can either disable this workflow (not recommended) or we can check and format our code with Ruff.</p> <p>To check what Ruff thinks about our code we run:</p> <pre><code>ruff check .\n</code></pre> <p>To fix any issues we can run:</p> <pre><code>ruff check . --fix\n</code></pre> <p>And commit the changes: <pre><code>git add -A\ngit commit -m \"Ruff linting\"\ngit push\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#adding-a-normalize-function","title":"Adding a normalize function","text":"<p>Next we will add some functionality to our use case through a so called \"normalize\" function. This allows us to add functionality to our schemas via Python code.</p>"},{"location":"tutorial/develop_plugin.html#the-use-case","title":"The use case","text":"<p>For this tutorial we will assume that we have a recipe file for our hot plate that we will parse: <pre><code>step name,duration [min],initial temperature [C],final temperature [C]\nheating, 30, 25, 300\nhold, 60, 300, 300\ncooling, 30, 300, 25\n</code></pre></p> <p>We can grab this file from the tutorial repository and place it in the tests/data directory using curl <pre><code>curl -L -o tests/data/sintering_example.csv \"https://raw.githubusercontent.com/FAIRmat-NFDI/AreaA-Examples/main/tutorial13/part3/files/sintering_example.csv\"\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#adding-the-code","title":"Adding the code","text":"<p>The first thing we need to add is a new <code>Quantity</code> in our <code>Sintering</code> class to hold the recipe file: <pre><code>data_file = Quantity(\n    type=str,\n    description='The recipe file for the sintering process.',\n    a_eln={\n        \"component\": \"FileEditQuantity\",\n    },\n)\n</code></pre> Here we have used the <code>a_eln</code> component annotation to add a <code>FileEditQuantity</code>. You will see in part 4 how this looks in the GUI.</p> <p>Secondly we need to update the normalize method to read the data file and update the corresponding data.</p> <p>First we will check if the <code>self.data_file</code> is present and, if so, use the <code>archive.m_context.raw_file()</code> method to open the file and read it with the pandas function <code>read_csv()</code>:</p> <pre><code>if self.data_file:\n  with archive.m_context.raw_file(self.data_file) as file:\n    df = pd.read_csv(file)\n</code></pre> <p>We will then create a list to hold the steps, iterate over our data frame, create an instance of a <code>TemperatureRamp</code>, and fill them. <pre><code>    steps = []\n    for i, row in df.iterrows():\n      step = TemperatureRamp()\n      step.name = row['step name']\n      step.duration = ureg.Quantity(float(row['duration [min]']), 'min')\n      step.initial_temperature = ureg.Quantity(row['initial temperature [C]'], 'celsius')\n      step.final_temperature = ureg.Quantity(row['final temperature [C]'], 'celsius')\n      steps.append(step)\n</code></pre> Here we have used the NOMAD unit registry to handle all the units.</p> <p>Finally, we will assign the <code>self.steps</code> with our new list of steps. <pre><code>  self.steps = steps\n</code></pre></p> <p>We also need to add the import of pandas and the NOMAD unit registry to the top of our <code>sintering.py</code> file: <pre><code>from nomad.units import ureg\nimport pandas as pd\n</code></pre></p> <p>Here are all the changes combined: <pre><code>from nomad.units import ureg\nimport pandas as pd\n\n\nclass Sintering(Process, EntryData, ArchiveSection):\n    '''\n    Class autogenerated from yaml schema.\n    '''\n    m_def = Section()\n    steps = SubSection(\n        section_def=TemperatureRamp,\n        repeats=True,\n    )\n    data_file = Quantity(\n        type=str,\n        description='The recipe file for the sintering process.',\n        a_eln={\n            \"component\": \"FileEditQuantity\",\n        },\n    )\n\n    def normalize(self, archive, logger: BoundLogger) -    None:\n        '''\n        The normalizer for the `Sintering` class.\n\n        Args:\n            archive (EntryArchive): The archive containing the section that is being\n            normalized.\n            logger (BoundLogger): A structlog logger.\n        '''\n        super(Sintering, self).normalize(archive, logger)\n        if self.data_file:\n          with archive.m_context.raw_file(self.data_file) as file:\n            df = pd.read_csv(file)\n          steps = []\n          for i, row in df.iterrows():\n            step = TemperatureRamp()\n            step.name = row['step name']\n            step.duration = ureg.Quantity(float(row['duration [min]']), 'min')\n            step.initial_temperature = ureg.Quantity(row['initial temperature [C]'], 'celsius')\n            step.final_temperature = ureg.Quantity(row['final temperature [C]'], 'celsius')\n            steps.append(step)\n        self.steps = steps\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#running-the-normalize-function","title":"Running the normalize function","text":"<p>We will now run the NOMAD processing on a test file to see the normalize function in action.</p>"},{"location":"tutorial/develop_plugin.html#create-an-archivejson-file","title":"Create an archive.json file","text":"<p>The first step is to create the test file. We should add a file with the ending <code>.archive.yaml</code> or <code>archive.json</code> and which contains a <code>data</code> section and an <code>m_def</code> key with the value being our sintering section. Finally, we should also add the <code>data_file</code> key with the value being our <code>.csv</code> file from before. <pre><code>data:\n  m_def: nomad_sintering.schema_packages.sintering.Sintering\n  data_file: sintering_example.csv\n</code></pre></p> <p>We can once again grab this file from the tutorial repository and place it in the tests/data directory using curl <pre><code>curl -L -o tests/data/test_sintering.archive.yaml \"https://raw.githubusercontent.com/FAIRmat-NFDI/AreaA-Examples/main/tutorial13/part3/files/test_sintering.archive.yaml\"\n</code></pre></p> <p>Attention</p> <p>You might need to modify the package name for the <code>m_def</code> if you called your python module something other than <code>nomad_sintering</code></p>"},{"location":"tutorial/develop_plugin.html#run-the-nomad-cli","title":"Run the NOMAD CLI","text":"<p>To run the processing we us the NOMAD CLI method <code>parse</code> with the flag <code>--show-archive</code> and save the output in a json file</p> <pre><code>nomad parse tests/data/test_sintering.archive.yaml --show-archive &gt; normalized.archive.json\n</code></pre> <p>However, when we run this we will get an error from NOMAD!</p> <pre><code>could not normalize section (normalizer=MetainfoNormalizer, section=Sintering, exc_info=Cannot convert from 'milliinch' ([length]) to 'second' ([time]))\n</code></pre> <p>What is happening here is that it has treated our <code>'min'</code> unit for duration as <code>'milliinch'</code> and not the intended minutes. To fix this we can directly edit the normalize function of the <code>Sintering</code> class in the <code>sintering.py</code> file by replacing <code>'min'</code> with <code>'minutes'</code>.</p> <pre><code>def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n    \"\"\"\n    The normalizer for the `Sintering` class.\n\n    Args:\n        archive (EntryArchive): The archive containing the section that is being\n        normalized.\n        logger (BoundLogger): A structlog logger.\n    \"\"\"\n    super().normalize(archive, logger)\n    if self.data_file:\n        with archive.m_context.raw_file(self.data_file) as file:\n            df = pd.read_csv(file)\n        steps = []\n        for i, row in df.iterrows():\n            step = TemperatureRamp()\n            step.name = row['step name']\n            # Changed 'min' to 'minutes' here:\n            step.duration = ureg.Quantity(float(row['duration [min]']), 'minutes')\n</code></pre> <p>Since we installed our package in editable mode the changes will take effect as soon as we save and rerunning the nomad parse command above should now work.</p> <p>To view the output you can open and inspect the <code>normalized.archive.json</code> file. The beginning of that file should look something like:</p> <pre><code>{\n  \"data\": {\n    \"m_def\": \"nomad_sintering.schema_packages.sintering.Sintering\",\n    \"name\": \"test sintering\",\n    \"datetime\": \"2024-06-04T16:52:23.998519+00:00\",\n    \"data_file\": \"sintering_example.csv\",\n    \"steps\": [\n      {\n        \"name\": \"heating\",\n        \"duration\": 1800.0,\n        \"initial_temperature\": 25.0,\n        \"final_temperature\": 300.0\n      },\n      {\n        \"name\": \"hold\",\n        \"duration\": 3600.0,\n        \"initial_temperature\": 300.0,\n        \"final_temperature\": 300.0\n      },\n      {\n        \"name\": \"cooling\",\n        \"duration\": 1800.0,\n        \"initial_temperature\": 300.0,\n        \"final_temperature\": 25.0\n      }\n    ]\n  },\n...\n</code></pre>"},{"location":"tutorial/develop_plugin.html#next-steps","title":"Next steps","text":"<p>The next step is to include your new schema in a custom NOMAD Oasis. For more information on how to setup a NOMAD Oasis you can have a look at How-to guides/NOMAD Oasis/Install and Oasis.</p> <p>Before we move one we should make sure that we have committed our changes to git: <pre><code>git add -A\ngit commit -m \"Added a normalize function to the Sintering schema\"\ngit push\n</code></pre></p>"},{"location":"tutorial/explore.html","title":"Exploring data","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>This tutorial shows how to use NOMAD's search interface and structured data browsing to explore available data.</p> <p>Note</p> <p>The NOMAD seen in the tutorials is an older version with a different color theme, but all the demonstrated functionality is still available on the current version. You'll find the NOMAD test installation mentioned in the first video here.</p>"},{"location":"tutorial/nomad_repo.html","title":"Navigating to the NOMAD repository","text":"<p>There are several access points to the NOMAD repository. The general landing page will give you a quick rundown of NOMAD's usage and features, and provides several links to documentation, tutorials, and the history behind the project.</p> <p>From this page, we can navigate to the NOMAD repository, where we can upload, manage, and explore data. There are 2 public versions available:</p> <ol> <li>stable, which is accessed by clicking the \"Open NOMAD\" button at the top of the landing page (highlighted orange in images below).</li> <li>beta /staging, which has the latest release and updates much more frequently, but may also harbor unstable or untested features. You can navigate to this version via two distinct links: 1. at the bottom-right corner of the landing page and 2. under \"SOLUTIONS\" &gt; \"NOMAD\" &gt; \"Try and Test\" in the top navigation menu (highlighted red in images below).</li> </ol> <p> </p>"},{"location":"tutorial/upload_publish.html","title":"Uploading and publishing data","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>This tutorial guides you through the basics of going from files on your computer to a published dataset with DOI.</p> <p>It covers the whole data-life cycle: starting with data on your hard drive, you will learn how to prepare, upload, publish data, and reference them with a DOI. Furthermore, you will learn how to explore, download, and use data that were published on NOMAD before. We will perform these steps with NOMAD's graphical user interface and its APIs.</p> <p>Note</p> <p>The NOMAD seen in the tutorials is an older version with a different color theme, but all the demonstrated functionality is still available on the current version. You'll find the NOMAD test installation mentioned in the first video here.</p>"}]}